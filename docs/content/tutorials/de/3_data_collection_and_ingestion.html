<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.47">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Ravi Shankar">

<title>data_collection_and_ingestion – Data Down To Earth</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../">
<link href="../../../logo.png" rel="icon" type="image/png">
<script src="../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


<link rel="stylesheet" href="../../../styles.css">
<link rel="stylesheet" href="../../content.css">
<meta property="og:title" content="– Data Down To Earth">
<meta property="og:site_name" content="Data Down To Earth">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="../../../index.html" class="navbar-brand navbar-brand-logo">
    <img src="../../../logo.png" alt="" class="navbar-logo">
    </a>
    <a class="navbar-brand" href="../../../index.html">
    <span class="navbar-title">Data Down To Earth</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-projects" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Projects</span>
    </a>
    <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="nav-menu-projects">    
        <li>
    <a class="dropdown-item" href="../../../projects/dsml_projects.html">
 <span class="dropdown-text">Data Science and ML Projects</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../projects/genai_projects.html">
 <span class="dropdown-text">Generative AI Projects</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-tutorials" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Tutorials</span>
    </a>
    <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="nav-menu-tutorials">    
        <li>
    <a class="dropdown-item" href="../../../topics/statistics.html">
 <span class="dropdown-text">Statistics &amp; Probability</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../topics/machine_learning.html">
 <span class="dropdown-text">Machine Learning</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../topics/de.html">
 <span class="dropdown-text">Data Engineering</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../topics/generative_ai.html">
 <span class="dropdown-text">Generative AI</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../topics/public_health.html">
 <span class="dropdown-text">Public Health</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../topics/product_sense.html">
 <span class="dropdown-text">Product Sense</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item">
    <a class="nav-link" href="../../../notes/notes.html"> 
<span class="menu-text">Notes</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/iamrsps"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/iamravishankar/"> <i class="bi bi-linkedin" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content column-page" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
</div>



<div class="quarto-title-meta column-page">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Ravi Shankar </p>
          </div>
  </div>
    
  
    
  </div>
  


</header>


<div class="text-content">
<section id="chapter-3-data-collection-and-ingestion" class="level1">
<h1>Chapter 3: Data Collection and Ingestion</h1>
<section id="data-sources-structured-semi-structured-and-unstructured-data" class="level3">
<h3 class="anchored" data-anchor-id="data-sources-structured-semi-structured-and-unstructured-data">Data Sources: Structured, Semi-Structured, and Unstructured Data</h3>
<section id="structured-data" class="level4">
<h4 class="anchored" data-anchor-id="structured-data">Structured Data</h4>
<p><strong>Definition</strong></p>
<p>Structured data is highly organized and easily searchable. It resides in fixed fields within a record or file, often in tabular form. Examples include relational databases and spreadsheets. Structured data is typically managed using SQL (Structured Query Language), which provides powerful querying capabilities.</p>
<p><strong>Examples</strong></p>
<ul>
<li><p><strong>Databases</strong>: SQL databases like MySQL, PostgreSQL. These databases store data in tables with defined columns and data types.</p></li>
<li><p><strong>Spreadsheets</strong>: Excel files, which organize data into rows and columns for easy manipulation and analysis.</p></li>
<li><p><strong>CRM Systems</strong>: Salesforce data, which is organized into fields like customer names, contact details, and sales records.</p></li>
</ul>
</section>
<section id="semi-structured-data" class="level4">
<h4 class="anchored" data-anchor-id="semi-structured-data">Semi-Structured Data</h4>
<p><strong>Definition</strong></p>
<p>Semi-structured data does not conform to a rigid schema but contains tags or markers to separate elements. It combines some aspects of structured and unstructured data. Semi-structured data formats are flexible and can evolve over time without requiring significant changes to the schema.</p>
<p><strong>Examples</strong></p>
<ul>
<li><p><strong>JSON</strong>: JavaScript Object Notation files, which store data as key-value pairs in a nested format.</p></li>
<li><p><strong>XML</strong>: eXtensible Markup Language files, which use tags to define elements and attributes for data.</p></li>
<li><p><strong>NoSQL Databases</strong>: MongoDB, which stores data in BSON (Binary JSON) format, allowing for flexible schema design.</p></li>
</ul>
</section>
<section id="unstructured-data" class="level4">
<h4 class="anchored" data-anchor-id="unstructured-data">Unstructured Data</h4>
<p><strong>Definition</strong></p>
<p>Unstructured data lacks a predefined format or organization, making it difficult to process and analyze using traditional methods. This type of data includes text, multimedia, and social media content, which require advanced techniques like natural language processing (NLP) and machine learning to derive insights.</p>
<p><strong>Examples</strong></p>
<ul>
<li><p><strong>Text Files</strong>: Logs, emails, documents. These contain raw text without a consistent structure.</p></li>
<li><p><strong>Multimedia</strong>: Images, videos, audio files. These require specialized processing techniques to extract useful information.</p></li>
<li><p><strong>Social Media</strong>: Tweets, Facebook posts. These contain unstructured text, images, and videos that need to be analyzed for sentiment and trends.</p></li>
</ul>
<hr>
</section>
</section>
<section id="api-integration-and-web-scraping" class="level3">
<h3 class="anchored" data-anchor-id="api-integration-and-web-scraping">API Integration and Web Scraping</h3>
<section id="api-integration" class="level4">
<h4 class="anchored" data-anchor-id="api-integration">API Integration</h4>
<p><strong>Definition</strong></p>
<p>APIs (Application Programming Interfaces) allow different software systems to communicate and exchange data. API integration involves connecting applications via APIs to automate data collection and sharing. This method provides structured and reliable access to data from external sources.</p>
<p><strong>Examples</strong></p>
<ul>
<li><p><strong>REST APIs</strong>: Used for web services and integration with platforms like Twitter, Google Maps. REST (Representational State Transfer) APIs use standard HTTP methods (GET, POST, PUT, DELETE) and are stateless, making them scalable and easy to use.</p></li>
<li><p><strong>SOAP APIs</strong>: Used in enterprise environments for secure transactions. SOAP (Simple Object Access Protocol) APIs use XML messaging and are known for their robustness and security features.</p></li>
</ul>
<p><strong>Tools</strong></p>
<ul>
<li><p><strong>Postman</strong>: API development and testing tool that allows developers to create, test, and document APIs.</p></li>
<li><p><strong>Swagger</strong>: API design and documentation tool that helps in creating interactive API documentation and client SDK generation.</p></li>
</ul>
</section>
<section id="web-scraping" class="level4">
<h4 class="anchored" data-anchor-id="web-scraping">Web Scraping</h4>
<p><strong>Definition</strong></p>
<p>Web scraping involves extracting data from websites using automated scripts. It is used when APIs are not available or to collect data from static web pages. Scraping tools navigate web pages, extract data, and save it for analysis.</p>
<p><strong>Techniques</strong></p>
<ul>
<li><p><strong>HTML Parsing</strong>: Extracting data from HTML code using libraries like BeautifulSoup (Python). HTML parsing involves identifying the structure of a web page and selecting specific elements to extract data.</p></li>
<li><p><strong>Headless Browsers</strong>: Using tools like Selenium to interact with dynamic web pages. Headless browsers can execute JavaScript and simulate user interactions, making them useful for scraping dynamic content.</p></li>
</ul>
<p><strong>Considerations</strong></p>
<ul>
<li><p><strong>Legal Issues</strong>: Compliance with website terms of service and copyright laws. It is important to respect the website’s terms and avoid scraping data without permission.</p></li>
<li><p><strong>Technical Challenges</strong>: Handling CAPTCHA, JavaScript-rendered content. These challenges require advanced techniques like automated CAPTCHA solving and JavaScript execution.</p></li>
</ul>
<hr>
</section>
</section>
<section id="log-ingestion-and-processing" class="level3">
<h3 class="anchored" data-anchor-id="log-ingestion-and-processing">Log Ingestion and Processing</h3>
<section id="definition" class="level4">
<h4 class="anchored" data-anchor-id="definition">Definition</h4>
<p>Log ingestion involves collecting and processing log data generated by applications, servers, and devices. This data is used for monitoring, debugging, and analytics. Log data provides valuable insights into system performance, security, and user behavior.</p>
<p><strong>Components</strong></p>
<ul>
<li><p><strong>Log Collection Agents</strong></p>
<p>Tools like Fluentd, Logstash, or Beats collect logs from various sources. These agents can be deployed on servers and applications to gather log data and forward it to a central processing system.</p></li>
<li><p><strong>Log Processing and Storage</strong></p>
<p>Logs are processed for parsing, filtering, and enrichment before being stored in systems like Elasticsearch or Splunk. Processing involves structuring the log data, removing irrelevant information, and adding context for better analysis.</p></li>
</ul>
<p><strong>Use Cases</strong></p>
<ul>
<li><p><strong>Monitoring and Alerting</strong></p>
<p>Real-time monitoring of system health and performance metrics. Alerts can be configured to notify administrators of any anomalies or issues detected in the logs.</p></li>
<li><p><strong>Security and Compliance</strong></p>
<p>Tracking security incidents and ensuring regulatory compliance through log analysis. Logs can help detect unauthorized access, data breaches, and other security threats.</p></li>
<li><p><strong>Debugging and Troubleshooting</strong></p>
<p>Identifying and diagnosing issues in applications and infrastructure. Logs provide detailed information about errors, performance bottlenecks, and system failures.</p></li>
</ul>
<hr>
</section>
</section>
<section id="batch-ingestion-techniques" class="level3">
<h3 class="anchored" data-anchor-id="batch-ingestion-techniques">Batch Ingestion Techniques</h3>
<section id="definition-1" class="level4">
<h4 class="anchored" data-anchor-id="definition-1">Definition</h4>
<p>Batch ingestion involves collecting and processing large volumes of data at scheduled intervals. This approach is suitable for scenarios where real-time data processing is not required. Batch processing can handle complex transformations and aggregations on large datasets.</p>
<p><strong>Techniques</strong></p>
<ul>
<li><p><strong>ETL (Extract, Transform, Load)</strong></p>
<p>A traditional method where data is extracted from source systems, transformed to fit the target schema, and loaded into the target system. ETL processes can be scheduled to run during off-peak hours to minimize impact on system performance.</p></li>
<li><p><strong>Batch Processing Frameworks</strong></p>
<p>Tools like Apache Hadoop and Apache Spark (batch mode) are used to process large datasets efficiently. These frameworks provide distributed processing capabilities, allowing large-scale data processing across multiple nodes.</p></li>
</ul>
<p><strong>Advantages</strong></p>
<ul>
<li><p><strong>Efficiency</strong></p>
<p>Can process large volumes of data in a single run, optimizing resource usage. Batch processing can handle complex transformations and aggregations more efficiently than real-time processing.</p></li>
<li><p><strong>Cost-Effective</strong></p>
<p>Lower operational costs compared to continuous processing for certain workloads. Batch processing can be scheduled during off-peak hours, reducing the need for constant resource allocation.</p></li>
</ul>
<p><strong>Challenges</strong></p>
<ul>
<li><p><strong>Latency</strong></p>
<p>Inherent delay in processing data, making it unsuitable for real-time applications. Data is only updated at the end of each batch cycle, which can be hours or even days apart.</p></li>
<li><p><strong>Complexity</strong></p>
<p>Handling large datasets and ensuring data quality can be complex. Batch processes need to be carefully designed and managed to ensure data consistency and reliability.</p></li>
</ul>
<hr>
</section>
</section>
<section id="real-time-data-streaming" class="level3">
<h3 class="anchored" data-anchor-id="real-time-data-streaming">Real-Time Data Streaming</h3>
<section id="definition-2" class="level4">
<h4 class="anchored" data-anchor-id="definition-2">Definition</h4>
<p>Real-time data streaming involves continuously collecting and processing data as it is generated. This approach is suitable for applications that require immediate insights and actions. Real-time streaming allows organizations to respond quickly to changing conditions and make data-driven decisions.</p>
<p><strong>Techniques</strong></p>
<ul>
<li><p><strong>Event Streaming</strong></p>
<p>Using platforms like Apache Kafka to stream data from producers to consumers in real-time. Event streaming captures events as they occur and makes them available for immediate processing and analysis.</p></li>
<li><p><strong>Stream Processing Frameworks</strong></p>
<p>Tools like Apache Flink, Apache Spark (streaming mode), and Storm for processing and analyzing streaming data. These frameworks provide capabilities for real-time data transformation, aggregation, and enrichment.</p></li>
</ul>
<p><strong>Advantages</strong></p>
<ul>
<li><p><strong>Low Latency</strong></p>
<p>Provides near-instantaneous data processing and insights. Real-time streaming ensures that data is processed and made available for analysis as soon as it is generated.</p></li>
<li><p><strong>Scalability</strong></p>
<p>Can handle large volumes of data in real-time. Streaming platforms can scale horizontally to accommodate increasing data loads.</p></li>
</ul>
<p><strong>Challenges</strong></p>
<ul>
<li><p><strong>Complexity</strong></p>
<p>Designing and maintaining real-time systems can be complex and resource-intensive. Real-time processing requires robust infrastructure and sophisticated data management techniques.</p></li>
<li><p><strong>Consistency</strong></p>
<p>Ensuring data consistency and fault tolerance in a distributed environment can be challenging. Real-time systems need to handle data duplication, ordering, and failure recovery.</p></li>
</ul>
<hr>
</section>
</section>
<section id="tools-deep-dive" class="level3">
<h3 class="anchored" data-anchor-id="tools-deep-dive">Tools Deep Dive</h3>
<section id="apache-kafka" class="level4">
<h4 class="anchored" data-anchor-id="apache-kafka">Apache Kafka</h4>
<p><strong>Description</strong></p>
<p>Apache Kafka is a distributed event streaming platform capable of handling trillions of events a day. It is used for building real-time data pipelines and streaming applications. Kafka is designed to provide high throughput, scalability, and durability.</p>
<p><strong>Features</strong></p>
<ul>
<li><p><strong>High Throughput and Scalability</strong></p>
<p>Handles large volumes of data with low latency. Kafka can scale horizontally by adding more brokers to the cluster.</p></li>
<li><p><strong>Durability and Fault Tolerance</strong></p>
<p>Ensures data persistence and reliability in case of failures. Kafka replicates data across multiple brokers to provide fault tolerance.</p></li>
<li><p><strong>Stream Processing</strong></p>
<p>Supports real-time stream processing through Kafka Streams and ksqlDB. Kafka Streams is a library for building stream processing applications, while ksqlDB provides a SQL interface for querying streaming data.</p></li>
</ul>
<p><strong>Use Cases</strong></p>
<ul>
<li><p><strong>Real-time Analytics</strong></p>
<p>Processing and analyzing data streams in real-time for immediate insights. Kafka can be used to collect and analyze log data, user activity, and other real-time events.</p></li>
<li><p><strong>Event Sourcing</strong></p>
<p>Capturing changes in the state of applications as a series of events. Event sourcing allows for reconstructing the state of the system by replaying events.</p></li>
<li><p><strong>Log Aggregation</strong></p>
<p>Collecting and centralizing logs from various sources for monitoring and analysis. Kafka can be used to aggregate log data from different applications and systems.</p></li>
</ul>
</section>
<section id="apache-flume" class="level4">
<h4 class="anchored" data-anchor-id="apache-flume">Apache Flume</h4>
<p><strong>Description</strong></p>
<p>Apache Flume is a distributed, reliable, and available service for efficiently collecting, aggregating, and moving large amounts of log data. It is designed for high-volume log ingestion and can integrate with various data sources and sinks.</p>
<p><strong>Features</strong></p>
<ul>
<li><p><strong>Reliability</strong></p>
<p>Ensures delivery of data from source to destination. Flume uses a reliable, fault-tolerant mechanism to guarantee data delivery.</p></li>
<li><p><strong>Flexibility</strong></p>
<p>Supports a wide range of data sources and sinks. Flume can collect data from various sources, such as log files, network events, and system metrics, and deliver it to multiple destinations, including HDFS, HBase, and Kafka.</p></li>
<li><p><strong>Scalability</strong></p>
<p>Can handle large volumes of log data across distributed systems. Flume can be scaled horizontally by adding more agents and channels.</p></li>
</ul>
<p><strong>Use Cases</strong></p>
<ul>
<li><p><strong>Log Aggregation</strong></p>
<p>Collecting and moving log data from multiple sources to centralized storage. Flume can be used to gather logs from different servers and applications for centralized analysis.</p></li>
<li><p><strong>Real-time Data Loading</strong></p>
<p>Ingesting data into Hadoop HDFS for real-time processing and analysis. Flume can stream data into HDFS, enabling real-time analytics on big data platforms.</p></li>
<li><p><strong>Monitoring and Alerting</strong></p>
<p>Real-time monitoring of system and application logs for anomalies. Flume can collect log data and forward it to monitoring systems for real-time alerting.</p></li>
</ul>
</section>
<section id="aws-kinesis" class="level4">
<h4 class="anchored" data-anchor-id="aws-kinesis">AWS Kinesis</h4>
<p><strong>Description</strong></p>
<p>AWS Kinesis is a platform on AWS to collect, process, and analyze real-time, streaming data. It offers capabilities to build applications that react in real-time. Kinesis provides several services, including Kinesis Streams, Kinesis Firehose, and Kinesis Analytics.</p>
<p><strong>Features</strong></p>
<ul>
<li><p><strong>Kinesis Streams</strong></p>
<p>Real-time data streaming and processing. Kinesis Streams can capture and store data streams from various sources, enabling real-time processing and analytics.</p></li>
<li><p><strong>Kinesis Firehose</strong></p>
<p>Fully managed service for loading streaming data into AWS data stores. Kinesis Firehose can deliver data to destinations like S3, Redshift, Elasticsearch, and Splunk.</p></li>
<li><p><strong>Kinesis Analytics</strong></p>
<p>Enables real-time SQL querying on streaming data. Kinesis Analytics allows users to run SQL queries on data streams, providing real-time insights and triggering actions based on query results.</p></li>
</ul>
<p><strong>Use Cases</strong></p>
<ul>
<li><p><strong>Real-time Analytics</strong></p>
<p>Building dashboards and applications that analyze streaming data in real-time. Kinesis can be used to monitor and analyze data from IoT devices, logs, and social media streams.</p></li>
<li><p><strong>Log and Event Data Collection</strong></p>
<p>Aggregating and processing log and event data for monitoring and insights. Kinesis can collect log data from various sources and deliver it to AWS services for analysis.</p></li>
<li><p><strong>Machine Learning</strong></p>
<p>Processing and analyzing data streams for real-time machine learning applications. Kinesis can feed data streams into machine learning models for real-time predictions and decision-making.</p></li>
</ul>
</section>
<section id="google-cloud-pubsub" class="level4">
<h4 class="anchored" data-anchor-id="google-cloud-pubsub">Google Cloud Pub/Sub</h4>
<p><strong>Description</strong></p>
<p>Google Cloud Pub/Sub is a messaging service for exchanging event data among applications and services. It enables real-time messaging and event-driven architectures, supporting asynchronous communication between decoupled systems.</p>
<p><strong>Features</strong></p>
<ul>
<li><p><strong>Real-time Messaging</strong></p>
<p>Enables asynchronous messaging between decoupled systems. Pub/Sub can deliver messages to subscribers in real-time, ensuring timely processing of events.</p></li>
<li><p><strong>Scalability</strong></p>
<p>Handles high throughput and large volumes of messages. Pub/Sub can scale horizontally to accommodate increasing message loads.</p></li>
<li><p><strong>Integration</strong></p>
<p>Integrates with other Google Cloud services for end-to-end data processing. Pub/Sub can be used with Dataflow, BigQuery, and other Google Cloud services for comprehensive data solutions.</p></li>
</ul>
<p><strong>Use Cases</strong></p>
<ul>
<li><p><strong>Event-driven Microservices</strong></p>
<p>Facilitating communication and coordination between microservices. Pub/Sub can be used to trigger actions in microservices based on incoming events.</p></li>
<li><p><strong>Real-time Analytics</strong></p>
<p>Streaming event data for real-time processing and analytics. Pub/Sub can deliver data to real-time analytics platforms for immediate insights.</p></li>
<li><p><strong>Data Ingestion</strong></p>
<p>Ingesting data from various sources for further processing and storage. Pub/Sub can collect data from IoT devices, logs, and applications and deliver it to data processing pipelines.</p></li>
</ul>
</section>
<section id="azure-event-hubs" class="level4">
<h4 class="anchored" data-anchor-id="azure-event-hubs">Azure Event Hubs</h4>
<p><strong>Description</strong></p>
<p>Azure Event Hubs is a big data streaming platform and event ingestion service capable of receiving and processing millions of events per second. It is designed for real-time data ingestion and processing at scale.</p>
<p><strong>Features</strong></p>
<ul>
<li><p><strong>High Throughput</strong></p>
<p>Handles large volumes of data streams. Event Hubs can process millions of events per second, making it suitable for high-velocity data streams.</p></li>
<li><p><strong>Real-time Processing</strong></p>
<p>Supports real-time data processing and analytics. Event Hubs can deliver data to real-time processing systems like Azure Stream Analytics and Apache Storm.</p></li>
<li><p><strong>Integration</strong></p>
<p>Seamlessly integrates with other Azure services for comprehensive data solutions. Event Hubs can be used with Azure Data Lake, Azure Synapse Analytics, and other Azure services.</p></li>
</ul>
<p><strong>Use Cases</strong></p>
<ul>
<li><p><strong>Telemetry Ingestion</strong></p>
<p>Collecting and analyzing telemetry data from IoT devices and applications. Event Hubs can ingest large volumes of telemetry data for real-time monitoring and analysis.</p></li>
<li><p><strong>Log and Event Data Streaming</strong></p>
<p>Streaming log and event data for real-time monitoring and alerting. Event Hubs can collect log data from various sources and deliver it to monitoring systems.</p></li>
<li><p><strong>Real-time Analytics</strong></p>
<p>Enabling real-time insights and actions based on streaming data. Event Hubs can feed data into real-time analytics platforms for immediate processing and decision-making.</p></li>
</ul>
</section>
</section>
</section>
<section id="questions" class="level1">
<h1>Questions</h1>
<section id="question-1-explain-what-structured-data-is-and-provide-examples-of-where-it-is-commonly-found." class="level4">
<h4 class="anchored" data-anchor-id="question-1-explain-what-structured-data-is-and-provide-examples-of-where-it-is-commonly-found.">Question 1: Explain what structured data is and provide examples of where it is commonly found.</h4>
<p><strong>Answer:</strong> Structured data is highly organized and easily searchable data that resides in fixed fields within a record or file, often in tabular form. Examples include relational databases like MySQL and PostgreSQL, spreadsheets like Excel files, and CRM systems like Salesforce, where data is organized into fields like customer names, contact details, and sales records.</p>
</section>
<section id="question-2-define-semi-structured-data-and-give-examples-of-its-typical-formats." class="level4">
<h4 class="anchored" data-anchor-id="question-2-define-semi-structured-data-and-give-examples-of-its-typical-formats.">Question 2: Define semi-structured data and give examples of its typical formats.</h4>
<p><strong>Answer:</strong> Semi-structured data does not conform to a rigid schema but contains tags or markers to separate elements. Examples include JSON (JavaScript Object Notation) files, XML (eXtensible Markup Language) files, and NoSQL databases like MongoDB, which store data in a flexible, nested format.</p>
</section>
<section id="question-3-what-is-unstructured-data-and-what-are-some-common-examples" class="level4">
<h4 class="anchored" data-anchor-id="question-3-what-is-unstructured-data-and-what-are-some-common-examples">Question 3: What is unstructured data, and what are some common examples?</h4>
<p><strong>Answer:</strong> Unstructured data lacks a predefined format or organization, making it difficult to process and analyze using traditional methods. Examples include text files (logs, emails, documents), multimedia files (images, videos, audio), and social media content (tweets, Facebook posts).</p>
</section>
<section id="question-4-describe-what-api-integration-is-and-provide-examples-of-commonly-used-apis." class="level4">
<h4 class="anchored" data-anchor-id="question-4-describe-what-api-integration-is-and-provide-examples-of-commonly-used-apis.">Question 4: Describe what API integration is and provide examples of commonly used APIs.</h4>
<p><strong>Answer:</strong> API integration involves connecting applications via APIs (Application Programming Interfaces) to automate data collection and sharing. Examples include REST APIs used for web services and integration with platforms like Twitter and Google Maps, and SOAP APIs used in enterprise environments for secure transactions.</p>
</section>
<section id="question-5-explain-the-concept-of-web-scraping-and-the-techniques-used." class="level4">
<h4 class="anchored" data-anchor-id="question-5-explain-the-concept-of-web-scraping-and-the-techniques-used.">Question 5: Explain the concept of web scraping and the techniques used.</h4>
<p><strong>Answer:</strong> Web scraping involves extracting data from websites using automated scripts. Techniques include HTML parsing using libraries like BeautifulSoup (Python) and using headless browsers like Selenium to interact with dynamic web pages.</p>
</section>
<section id="question-6-what-is-log-ingestion-and-why-is-it-important" class="level4">
<h4 class="anchored" data-anchor-id="question-6-what-is-log-ingestion-and-why-is-it-important">Question 6: What is log ingestion, and why is it important?</h4>
<p><strong>Answer:</strong> Log ingestion involves collecting and processing log data generated by applications, servers, and devices. It is important for monitoring, debugging, and analytics, providing valuable insights into system performance, security, and user behavior.</p>
</section>
<section id="question-7-what-are-some-common-tools-used-for-log-collection-and-processing" class="level4">
<h4 class="anchored" data-anchor-id="question-7-what-are-some-common-tools-used-for-log-collection-and-processing">Question 7: What are some common tools used for log collection and processing?</h4>
<p><strong>Answer:</strong> Common tools for log collection and processing include Fluentd, Logstash, and Beats for log collection, and Elasticsearch and Splunk for log processing and storage.</p>
</section>
<section id="question-8-what-is-the-etl-process-and-what-are-its-key-components" class="level4">
<h4 class="anchored" data-anchor-id="question-8-what-is-the-etl-process-and-what-are-its-key-components">Question 8: What is the ETL process, and what are its key components?</h4>
<p><strong>Answer:</strong> ETL stands for Extract, Transform, Load. It involves extracting data from source systems, transforming it to fit the target schema, and loading it into the target system. Key components include data extraction, data transformation, and data loading.</p>
</section>
<section id="question-9-name-some-batch-processing-frameworks-and-their-advantages." class="level4">
<h4 class="anchored" data-anchor-id="question-9-name-some-batch-processing-frameworks-and-their-advantages.">Question 9: Name some batch processing frameworks and their advantages.</h4>
<p><strong>Answer:</strong> Batch processing frameworks include Apache Hadoop and Apache Spark (batch mode). Advantages include the ability to process large volumes of data efficiently, handle complex transformations and aggregations, and optimize resource usage.</p>
</section>
<section id="question-10-what-is-event-streaming-and-which-platform-is-commonly-used-for-it" class="level4">
<h4 class="anchored" data-anchor-id="question-10-what-is-event-streaming-and-which-platform-is-commonly-used-for-it">Question 10: What is event streaming, and which platform is commonly used for it?</h4>
<p><strong>Answer:</strong> Event streaming involves continuously collecting and processing data as it is generated. Apache Kafka is a commonly used platform for event streaming, capable of handling large volumes of real-time data.</p>
</section>
<section id="question-11-provide-examples-of-stream-processing-frameworks-and-their-benefits." class="level4">
<h4 class="anchored" data-anchor-id="question-11-provide-examples-of-stream-processing-frameworks-and-their-benefits.">Question 11: Provide examples of stream processing frameworks and their benefits.</h4>
<p><strong>Answer:</strong> Examples of stream processing frameworks include Apache Flink, Apache Spark (streaming mode), and Storm. Benefits include low latency, real-time data processing, scalability, and the ability to handle large volumes of data in real-time.</p>
</section>
<section id="question-12-what-is-apache-kafka-and-what-are-its-primary-features" class="level4">
<h4 class="anchored" data-anchor-id="question-12-what-is-apache-kafka-and-what-are-its-primary-features">Question 12: What is Apache Kafka, and what are its primary features?</h4>
<p><strong>Answer:</strong> Apache Kafka is a distributed event streaming platform used for building real-time data pipelines and streaming applications. Primary features include high throughput and scalability, durability and fault tolerance, and support for real-time stream processing through Kafka Streams and ksqlDB.</p>
</section>
<section id="question-13-describe-apache-flume-and-its-main-use-cases." class="level4">
<h4 class="anchored" data-anchor-id="question-13-describe-apache-flume-and-its-main-use-cases.">Question 13: Describe Apache Flume and its main use cases.</h4>
<p><strong>Answer:</strong> Apache Flume is a distributed service for efficiently collecting, aggregating, and moving large amounts of log data. Main use cases include log aggregation, real-time data loading into Hadoop HDFS, and monitoring and alerting for system and application logs.</p>
</section>
<section id="question-14-what-are-the-components-of-aws-kinesis-and-what-are-their-functions" class="level4">
<h4 class="anchored" data-anchor-id="question-14-what-are-the-components-of-aws-kinesis-and-what-are-their-functions">Question 14: What are the components of AWS Kinesis, and what are their functions?</h4>
<p><strong>Answer:</strong> AWS Kinesis includes Kinesis Streams for real-time data streaming, Kinesis Firehose for fully managed data delivery to AWS data stores, and Kinesis Analytics for real-time SQL querying on streaming data.</p>
</section>
<section id="question-15-explain-the-functionality-of-google-cloud-pubsub-and-its-typical-use-cases." class="level4">
<h4 class="anchored" data-anchor-id="question-15-explain-the-functionality-of-google-cloud-pubsub-and-its-typical-use-cases.">Question 15: Explain the functionality of Google Cloud Pub/Sub and its typical use cases.</h4>
<p><strong>Answer:</strong> Google Cloud Pub/Sub is a messaging service for exchanging event data among applications and services. It enables real-time messaging and event-driven architectures, with use cases including event-driven microservices, real-time analytics, and data ingestion.</p>
</section>
<section id="question-16-what-is-azure-event-hubs-and-what-are-its-primary-features" class="level4">
<h4 class="anchored" data-anchor-id="question-16-what-is-azure-event-hubs-and-what-are-its-primary-features">Question 16: What is Azure Event Hubs, and what are its primary features?</h4>
<p><strong>Answer:</strong> Azure Event Hubs is a big data streaming platform and event ingestion service capable of receiving and processing millions of events per second. Primary features include high throughput, real-time processing, and seamless integration with other Azure services.</p>
</section>
<section id="question-17-what-challenges-might-you-encounter-during-the-etl-process-and-how-can-you-address-them" class="level4">
<h4 class="anchored" data-anchor-id="question-17-what-challenges-might-you-encounter-during-the-etl-process-and-how-can-you-address-them">Question 17: What challenges might you encounter during the ETL process, and how can you address them?</h4>
<p><strong>Answer:</strong> Challenges during the ETL process include handling data inconsistencies and missing values. These can be addressed by implementing data validation rules during transformation, setting default values for missing data, and using data profiling tools to identify and correct inconsistencies before loading the data into the warehouse.</p>
</section>
<section id="question-18-what-measures-can-be-taken-to-ensure-data-quality-in-a-data-warehouse" class="level4">
<h4 class="anchored" data-anchor-id="question-18-what-measures-can-be-taken-to-ensure-data-quality-in-a-data-warehouse">Question 18: What measures can be taken to ensure data quality in a data warehouse?</h4>
<p><strong>Answer:</strong> Measures to ensure data quality include data profiling, implementing data validation rules during ETL processes, regular audits and data cleaning, maintaining metadata for data traceability, and establishing data governance policies to manage data access and quality control.</p>
</section>
<section id="question-19-explain-the-importance-of-data-governance-in-large-organizations." class="level4">
<h4 class="anchored" data-anchor-id="question-19-explain-the-importance-of-data-governance-in-large-organizations.">Question 19: Explain the importance of data governance in large organizations.</h4>
<p><strong>Answer:</strong> Data governance is crucial in large organizations to ensure data consistency, integrity, and security. It involves defining data policies, standards, and procedures for data management, helping in compliance with regulations, improving data quality, facilitating data sharing, and supporting informed decision-making by ensuring that data is accurate, available, and secure.</p>
</section>
<section id="question-20-how-can-organizations-protect-sensitive-data-in-their-data-warehouses" class="level4">
<h4 class="anchored" data-anchor-id="question-20-how-can-organizations-protect-sensitive-data-in-their-data-warehouses">Question 20: How can organizations protect sensitive data in their data warehouses?</h4>
<p><strong>Answer:</strong> Organizations can protect sensitive data through measures such as data encryption (both at rest and in transit), implementing access controls (role-based access and least privilege principle), regular security audits, data masking for sensitive information, and compliance with data protection regulations like GDPR and HIPAA.</p>
</section>
<section id="question-21-what-is-data-masking-and-when-would-it-be-used" class="level4">
<h4 class="anchored" data-anchor-id="question-21-what-is-data-masking-and-when-would-it-be-used">Question 21: What is data masking, and when would it be used?</h4>
<p><strong>Answer:</strong> Data masking is the process of obfuscating sensitive data to protect it from unauthorized access while maintaining its usability for testing or analysis. It is used in scenarios where data needs to be shared with developers or analysts who do not need access to the actual sensitive information, such as during software testing or data analysis projects.</p>
</section>
<section id="question-22-what-techniques-can-be-used-to-optimize-query-performance-in-a-data-warehouse" class="level4">
<h4 class="anchored" data-anchor-id="question-22-what-techniques-can-be-used-to-optimize-query-performance-in-a-data-warehouse">Question 22: What techniques can be used to optimize query performance in a data warehouse?</h4>
<p><strong>Answer:</strong> Techniques to optimize query performance include indexing to speed up data retrieval, partitioning tables to manage large datasets more efficiently, using materialized views to store precomputed results of complex queries, query optimization through SQL tuning, and ensuring the proper configuration of data warehouse hardware and software resources.</p>
</section>
<section id="question-23-describe-the-concept-of-indexing-and-its-impact-on-query-performance." class="level4">
<h4 class="anchored" data-anchor-id="question-23-describe-the-concept-of-indexing-and-its-impact-on-query-performance.">Question 23: Describe the concept of indexing and its impact on query performance.</h4>
<p><strong>Answer:</strong> Indexing involves creating data structures that improve the speed of data retrieval operations on a database table. Indexes allow the database system to find and access data quickly without scanning the entire table, significantly improving query performance, especially for large tables and frequently queried columns. For example, adding an index on a customer ID column in an orders table can expedite the retrieval of all orders for a specific customer.</p>
</section>
<section id="question-24-describe-the-challenges-associated-with-processing-unstructured-data." class="level4">
<h4 class="anchored" data-anchor-id="question-24-describe-the-challenges-associated-with-processing-unstructured-data.">Question 24: Describe the challenges associated with processing unstructured data.</h4>
<p><strong>Answer:</strong> Processing unstructured data involves several challenges, such as its inherent lack of predefined format, which makes it difficult to analyze using traditional methods. Additionally, unstructured data often requires advanced techniques like natural language processing (NLP) for text analysis and computer vision for multimedia content. The vast variety of formats and sources, including text files, images, videos, and social media, further complicates processing and necessitates robust storage and computing resources.</p>
</section>
<section id="question-25-what-are-the-main-advantages-of-using-a-data-lake-for-storing-data" class="level4">
<h4 class="anchored" data-anchor-id="question-25-what-are-the-main-advantages-of-using-a-data-lake-for-storing-data">Question 25: What are the main advantages of using a data lake for storing data?</h4>
<p><strong>Answer:</strong> The main advantages of using a data lake include its ability to store raw, unstructured, and structured data at any scale, providing flexibility in data processing. Data lakes support schema-on-read, allowing data to be stored in its raw format and structured when read. This flexibility accommodates various data types and evolving schemas without requiring significant changes. Additionally, data lakes can handle large volumes of data, making them suitable for big data analytics and machine learning applications.</p>
</section>
<section id="question-26-explain-the-importance-of-real-time-data-streaming-in-modern-applications." class="level4">
<h4 class="anchored" data-anchor-id="question-26-explain-the-importance-of-real-time-data-streaming-in-modern-applications.">Question 26: Explain the importance of real-time data streaming in modern applications.</h4>
<p><strong>Answer:</strong> Real-time data streaming is crucial in modern applications for providing immediate insights and actions. It enables organizations to respond quickly to changing conditions, such as detecting fraud in financial transactions or providing real-time recommendations in e-commerce. Real-time streaming supports low-latency processing, ensuring that data is analyzed as soon as it is generated, which is essential for applications requiring timely decision-making and fast response times.</p>
</section>
<section id="question-27-how-do-batch-processing-and-stream-processing-differ-in-terms-of-data-handling" class="level4">
<h4 class="anchored" data-anchor-id="question-27-how-do-batch-processing-and-stream-processing-differ-in-terms-of-data-handling">Question 27: How do batch processing and stream processing differ in terms of data handling?</h4>
<p><strong>Answer:</strong> Batch processing involves collecting and processing large volumes of data at scheduled intervals, suitable for tasks that do not require real-time data processing. It is efficient for handling complex transformations and aggregations on large datasets. In contrast, stream processing continuously collects and processes data as it is generated, providing immediate insights and actions. Stream processing is ideal for applications that require low-latency data processing and real-time decision-making, such as monitoring and analytics for live events or IoT devices.</p>
</section>
<section id="question-28-describe-a-scenario-where-a-data-lakehouse-architecture-would-be-beneficial." class="level4">
<h4 class="anchored" data-anchor-id="question-28-describe-a-scenario-where-a-data-lakehouse-architecture-would-be-beneficial.">Question 28: Describe a scenario where a data lakehouse architecture would be beneficial.</h4>
<p><strong>Answer:</strong> A data lakehouse architecture would be beneficial for an e-commerce company needing a versatile data platform to support both operational and analytical workloads. The company can manage real-time transaction data alongside historical sales and customer behavior data, facilitating both operational efficiency and strategic analysis. The unified storage and processing capabilities of a data lakehouse allow for seamless data management across different data types, supporting both real-time analytics and long-term data warehousing needs.</p>
</section>
<section id="question-29-what-are-the-primary-features-of-google-cloud-pubsub-and-how-do-they-support-event-driven-architectures" class="level4">
<h4 class="anchored" data-anchor-id="question-29-what-are-the-primary-features-of-google-cloud-pubsub-and-how-do-they-support-event-driven-architectures">Question 29: What are the primary features of Google Cloud Pub/Sub, and how do they support event-driven architectures?</h4>
<p><strong>Answer:</strong> Google Cloud Pub/Sub is a messaging service for exchanging event data among applications and services, supporting asynchronous communication between decoupled systems. Primary features include real-time messaging, enabling events to be delivered to subscribers immediately, and scalability, handling high throughput and large volumes of messages. Pub/Sub’s integration with other Google Cloud services, like Dataflow and BigQuery, supports comprehensive data processing and analytics solutions, making it ideal for event-driven architectures where timely event handling is crucial.</p>
</section>
<section id="question-30-explain-the-role-of-data-profiling-in-ensuring-data-quality." class="level4">
<h4 class="anchored" data-anchor-id="question-30-explain-the-role-of-data-profiling-in-ensuring-data-quality.">Question 30: Explain the role of data profiling in ensuring data quality.</h4>
<p><strong>Answer:</strong> Data profiling involves analyzing data to understand its structure, content, and quality. It helps identify data anomalies, inconsistencies, and patterns that may indicate quality issues. By providing insights into data distributions, types, and relationships, data profiling helps in designing effective data validation rules and transformation processes during ETL. Ensuring data quality through profiling is crucial for maintaining reliable and accurate data in data warehouses, supporting better decision-making and analytics.</p>
</section>
</section>
</div>



</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>