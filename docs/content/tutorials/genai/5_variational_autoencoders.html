<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.47">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Ravi Shankar">

<title>variational_autoencoders – Data Down To Earth</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../">
<link href="../../../logo.png" rel="icon" type="image/png">
<script src="../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../../styles.css">
<link rel="stylesheet" href="../../content.css">
<meta property="og:title" content="– Data Down To Earth">
<meta property="og:site_name" content="Data Down To Earth">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="../../../index.html" class="navbar-brand navbar-brand-logo">
    <img src="../../../logo.png" alt="" class="navbar-logo">
    </a>
    <a class="navbar-brand" href="../../../index.html">
    <span class="navbar-title">Data Down To Earth</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-projects" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Projects</span>
    </a>
    <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="nav-menu-projects">    
        <li>
    <a class="dropdown-item" href="../../../projects/dsml_projects.html">
 <span class="dropdown-text">Data Science and ML Projects</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../projects/genai_projects.html">
 <span class="dropdown-text">Generative AI Projects</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-tutorials" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Tutorials</span>
    </a>
    <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="nav-menu-tutorials">    
        <li>
    <a class="dropdown-item" href="../../../topics/statistics.html">
 <span class="dropdown-text">Statistics &amp; Probability</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../topics/machine_learning.html">
 <span class="dropdown-text">Machine Learning</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../topics/de.html">
 <span class="dropdown-text">Data Engineering</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../topics/generative_ai.html">
 <span class="dropdown-text">Generative AI</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../topics/public_health.html">
 <span class="dropdown-text">Public Health</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../topics/product_sense.html">
 <span class="dropdown-text">Product Sense</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item">
    <a class="nav-link" href="../../../blogs/blogs.html"> 
<span class="menu-text">Blog</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/iamrsps"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/iamravishankar/"> <i class="bi bi-linkedin" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content column-page" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
</div>



<div class="quarto-title-meta column-page">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Ravi Shankar </p>
          </div>
  </div>
    
  
    
  </div>
  


</header>


<section id="chapter-5-variational-autoencoders" class="level1 text-content">
<h1>Chapter 5: Variational Autoencoders</h1>
<section id="encoder-decoder-architecture" class="level2">
<h2 class="anchored" data-anchor-id="encoder-decoder-architecture">Encoder-Decoder Architecture</h2>
<p>Variational Autoencoders (VAEs) are a class of generative models that combine ideas from autoencoders and probabilistic graphical models. The VAE architecture consists of two main components:</p>
<ol type="1">
<li><strong>Encoder</strong>: Also known as the recognition model or inference network, denoted as <span class="math inline">\(q_\phi(z|x)\)</span>.</li>
<li><strong>Decoder</strong>: Also known as the generative model, denoted as <span class="math inline">\(p_\theta(x|z)\)</span>.</li>
</ol>
<p>Where: - <span class="math inline">\(x\)</span> is the input data. - <span class="math inline">\(z\)</span> is the latent variable. - <span class="math inline">\(\phi\)</span> and <span class="math inline">\(\theta\)</span> are the parameters of the encoder and decoder networks, respectively.</p>
<p>The encoder maps the input data to a probability distribution in the latent space, while the decoder maps points from the latent space back to the data space.</p>
<section id="encoder" class="level3">
<h3 class="anchored" data-anchor-id="encoder">Encoder</h3>
<p>The encoder network takes an input <span class="math inline">\(x\)</span> and outputs parameters of a probability distribution in the latent space. Typically, this is a multivariate Gaussian distribution with diagonal covariance:</p>
<p><span class="math display">\[
q_\phi(z|x) = \mathcal{N}(z; \mu_\phi(x), \sigma_\phi^2(x)I)
\]</span></p>
<p>Where <span class="math inline">\(\mu_\phi(x)\)</span> and <span class="math inline">\(\sigma_\phi^2(x)\)</span> are neural networks that output the mean and variance of the distribution, respectively.</p>
<section id="neural-network-architecture" class="level4">
<h4 class="anchored" data-anchor-id="neural-network-architecture">Neural Network Architecture</h4>
<p>The encoder network typically consists of several layers of neural networks, such as convolutional layers for image data or fully connected layers for other types of data. The final layer outputs the mean and log-variance of the latent distribution. Common choices include:</p>
<ul>
<li><strong>Convolutional Layers</strong>: For handling image data, these layers extract hierarchical features by applying filters that detect patterns such as edges, textures, and more complex structures at deeper layers.</li>
<li><strong>Fully Connected Layers</strong>: For other types of data, these layers capture global features by connecting each neuron to every neuron in the previous layer.</li>
<li><strong>Activation Functions</strong>: ReLU (Rectified Linear Unit) or LeakyReLU for non-linear transformations that introduce non-linearity into the model, enabling it to learn complex patterns.</li>
<li><strong>Batch Normalization</strong>: To stabilize and accelerate training by normalizing the outputs of each layer, reducing internal covariate shift.</li>
</ul>
<p>The output of the encoder network is the mean <span class="math inline">\(\mu_\phi(x)\)</span> and log-variance <span class="math inline">\(\log \sigma_\phi^2(x)\)</span> of the latent distribution. These parameters define the Gaussian distribution from which we sample the latent variable <span class="math inline">\(z\)</span>.</p>
</section>
</section>
<section id="decoder" class="level3">
<h3 class="anchored" data-anchor-id="decoder">Decoder</h3>
<p>The decoder network takes a point <span class="math inline">\(z\)</span> from the latent space and reconstructs the input data. It defines a probability distribution over the possible values of <span class="math inline">\(x\)</span>:</p>
<p><span class="math display">\[
p_\theta(x|z) = f(x; g_\theta(z))
\]</span></p>
<p>Where <span class="math inline">\(g_\theta(z)\)</span> is a neural network that maps <span class="math inline">\(z\)</span> to the parameters of the distribution <span class="math inline">\(f\)</span>.</p>
<section id="neural-network-architecture-1" class="level4">
<h4 class="anchored" data-anchor-id="neural-network-architecture-1">Neural Network Architecture</h4>
<p>The decoder network typically mirrors the architecture of the encoder, with layers corresponding to the inverse operations of the encoder layers. Common choices include:</p>
<ul>
<li><strong>Transposed Convolutional Layers</strong>: Also known as deconvolutional layers, these layers upsample the input by applying filters that reconstruct the image from the latent space representation.</li>
<li><strong>Fully Connected Layers</strong>: For non-image data, these layers transform the latent variables back to the data space.</li>
<li><strong>Activation Functions</strong>: ReLU for hidden layers and tanh or sigmoid for output layers, depending on the data type (e.g., sigmoid for binary data, tanh for continuous data).</li>
</ul>
<p>The output of the decoder network is a reconstructed sample <span class="math inline">\(\hat{x}\)</span>, generated from the latent variable <span class="math inline">\(z\)</span>.</p>
</section>
</section>
</section>
<section id="variational-inference-and-reparameterization-trick" class="level2">
<h2 class="anchored" data-anchor-id="variational-inference-and-reparameterization-trick">Variational Inference and Reparameterization Trick</h2>
<p>VAEs use variational inference to approximate the true posterior distribution <span class="math inline">\(p(z|x)\)</span> with the encoder distribution <span class="math inline">\(q_\phi(z|x)\)</span>. The goal is to maximize the evidence lower bound (ELBO), which is equivalent to minimizing the Kullback-Leibler divergence between the approximate and true posteriors.</p>
<section id="variational-inference" class="level3">
<h3 class="anchored" data-anchor-id="variational-inference">Variational Inference</h3>
<p>Variational inference involves approximating the true posterior distribution <span class="math inline">\(p(z|x)\)</span> with a simpler distribution <span class="math inline">\(q_\phi(z|x)\)</span>. The objective is to find the parameters <span class="math inline">\(\phi\)</span> that minimize the Kullback-Leibler divergence between <span class="math inline">\(q_\phi(z|x)\)</span> and <span class="math inline">\(p(z|x)\)</span>:</p>
<p><span class="math display">\[
D_{KL}(q_\phi(z|x) \parallel p(z|x)) = \mathbb{E}_{q_\phi(z|x)}\left[\log \frac{q_\phi(z|x)}{p(z|x)}\right]
\]</span></p>
<p>Since the true posterior <span class="math inline">\(p(z|x)\)</span> is intractable, we maximize the evidence lower bound (ELBO) instead:</p>
<p><span class="math display">\[
\log p(x) = D_{KL}(q_\phi(z|x) \parallel p_\theta(z|x)) + \mathbb{E}_{q_\phi(z|x)}\left[\log \frac{p_\theta(x, z)}{q_\phi(z|x)} \right] \geq \mathbb{E}_{q_\phi(z|x)}[\log p_\theta(x|z)] - D_{KL}(q_\phi(z|x) \parallel p(z))
\]</span></p>
<p>Where: - The first term <span class="math inline">\(\mathbb{E}_{q_\phi(z|x)}[\log p_\theta(x|z)]\)</span> is the expected log-likelihood of the data under the approximate posterior. - The second term <span class="math inline">\(D_{KL}(q_\phi(z|x) \parallel p(z))\)</span> is the Kullback-Leibler divergence between the approximate posterior and the prior distribution.</p>
</section>
<section id="reparameterization-trick" class="level3">
<h3 class="anchored" data-anchor-id="reparameterization-trick">Reparameterization Trick</h3>
<p>The reparameterization trick is used to allow backpropagation through the sampling process. Instead of directly sampling from <span class="math inline">\(q_\phi(z|x)\)</span>, we sample from a standard normal distribution and then transform the samples:</p>
<p><span class="math display">\[
z = \mu_\phi(x) + \sigma_\phi(x) \odot \epsilon, \quad \epsilon \sim \mathcal{N}(0, I)
\]</span></p>
<p>Where <span class="math inline">\(\odot\)</span> denotes element-wise multiplication.</p>
<p>By reparameterizing the sampling process, we can compute gradients with respect to the parameters <span class="math inline">\(\phi\)</span> and <span class="math inline">\(\theta\)</span> using standard backpropagation.</p>
</section>
<section id="detailed-explanation" class="level3">
<h3 class="anchored" data-anchor-id="detailed-explanation">Detailed Explanation</h3>
<section id="evidence-lower-bound-elbo" class="level4">
<h4 class="anchored" data-anchor-id="evidence-lower-bound-elbo">Evidence Lower Bound (ELBO)</h4>
<p>The Evidence Lower Bound (ELBO) is a key concept in variational inference. It provides a lower bound on the marginal likelihood of the data, <span class="math inline">\(\log p(x)\)</span>. To understand the ELBO, consider the following derivation:</p>
<p>Starting from the marginal likelihood:</p>
<p><span class="math display">\[
\log p(x) = \log \int p_\theta(x, z) \, dz
\]</span></p>
<p>We introduce the approximate posterior <span class="math inline">\(q_\phi(z|x)\)</span> and use Jensen’s inequality to derive the ELBO:</p>
<p><span class="math display">\[
\log p(x) = \log \int q_\phi(z|x) \frac{p_\theta(x, z)}{q_\phi(z|x)} \, dz \geq \int q_\phi(z|x) \log \frac{p_\theta(x, z)}{q_\phi(z|x)} \, dz = \mathbb{E}_{q_\phi(z|x)} \left[ \log \frac{p_\theta(x, z)}{q_\phi(z|x)} \right]
\]</span></p>
<p>We can rewrite this as:</p>
<p><span class="math display">\[
\log p(x) = \mathbb{E}_{q_\phi(z|x)} \left[ \log \frac{p_\theta(x|z) p(z)}{q_\phi(z|x)} \right] + D_{KL}(q_\phi(z|x) \parallel p_\theta(z|x))
\]</span></p>
<p>Since the Kullback-Leibler divergence <span class="math inline">\(D_{KL}(q_\phi(z|x) \parallel p_\theta(z|x)) \geq 0\)</span>, we get:</p>
<p><span class="math display">\[
\log p(x) \geq \mathbb{E}_{q_\phi(z|x)}[\log p_\theta(x|z)] - D_{KL}(q_\phi(z|x) \parallel p(z))
\]</span></p>
<p>The ELBO is thus given by:</p>
<p><span class="math display">\[
\text{ELBO} = \mathbb{E}_{q_\phi(z|x)}[\log p_\theta(x|z)] - D_{KL}(q_\phi(z|x) \parallel p(z))
\]</span></p>
<p>Maximizing the ELBO ensures that the approximate posterior <span class="math inline">\(q_\phi(z|x)\)</span> is close to the true posterior <span class="math inline">\(p(z|x)\)</span>, and that the reconstruction from the latent variable <span class="math inline">\(z\)</span> is accurate.</p>
</section>
<section id="kullback-leibler-divergence" class="level4">
<h4 class="anchored" data-anchor-id="kullback-leibler-divergence">Kullback-Leibler Divergence</h4>
<p>The Kullback-Leibler divergence between two probability distributions <span class="math inline">\(P\)</span> and <span class="math inline">\(Q\)</span> is defined as:</p>
<p><span class="math display">\[
D_{KL}(P \parallel Q) = \mathbb{E}_{P}\left[\log \frac{P}{Q}\right]
\]</span></p>
<p>In the context of VAEs, the KL divergence term <span class="math inline">\(D_{KL}(q_\phi(z|x) \parallel p(z))\)</span> measures how much the approximate posterior <span class="math inline">\(q_\phi(z|x)\)</span> diverges from the prior <span class="math inline">\(p(z)\)</span>. This term acts as a regularizer, ensuring that the latent space learned by the VAE does not deviate too much from the prior distribution.</p>
<p>The KL divergence between two Gaussian distributions <span class="math inline">\(q(z) = \mathcal{N}(\mu_q, \sigma_q^2)\)</span> and <span class="math inline">\(p(z) = \mathcal{N}(\mu_p, \sigma_p^2)\)</span> has a closed-form expression:</p>
<p><span class="math display">\[
D_{KL}(\mathcal{N}(\mu_q, \sigma_q^2) \parallel \mathcal{N}(\mu_p, \sigma_p^2)) = \frac{1}{2} \left[ \log \frac{\sigma_p^2}{\sigma_q^2} + \frac{\sigma_q^2 + (\mu_q - \mu_p)^2}{\sigma_p^2} - 1 \right]
\]</span></p>
<p>For the VAE, the prior <span class="math inline">\(p(z)\)</span> is typically a standard normal distribution <span class="math inline">\(\mathcal{N}(0, I)\)</span>, and the approximate posterior <span class="math inline">\(q_\phi(z|x) = \mathcal{N}(\mu_\phi(x), \sigma_\phi^2(x))\)</span>. Thus, the KL divergence becomes:</p>
<p><span class="math display">\[
D_{KL}(\mathcal{N}(\mu_\phi(x), \sigma_\phi^2(x)) \parallel \mathcal{N}(0, I)) = \frac{1}{2} \sum_{j=1}^J \left( \sigma_\phi^2(x)_j + \mu_\phi(x)_j^2 - 1 - \log \sigma_\phi^2(x)_j \right)
\]</span></p>
</section>
<section id="reparameterization-in-detail" class="level4">
<h4 class="anchored" data-anchor-id="reparameterization-in-detail">Reparameterization in Detail</h4>
<p>The reparameterization trick involves expressing the latent variable <span class="math inline">\(z\)</span> in terms of deterministic and stochastic components:</p>
<p><span class="math display">\[
z = \mu_\phi(x) + \sigma_\phi(x) \odot \epsilon, \quad \epsilon \sim \mathcal{N}(0, I)
\]</span></p>
<p>This allows gradients to be propagated through the sampling process, making it possible to optimize the VAE using gradient-based methods. The stochastic part <span class="math inline">\(\epsilon\)</span> is sampled from a standard normal distribution, while the deterministic part <span class="math inline">\(\mu_\phi(x) + \sigma_\phi(x) \odot \epsilon\)</span> depends on the learned parameters of the encoder.</p>
</section>
</section>
</section>
<section id="kullback-leibler-divergence-and-evidence-lower-bound-elbo" class="level2">
<h2 class="anchored" data-anchor-id="kullback-leibler-divergence-and-evidence-lower-bound-elbo">Kullback-Leibler Divergence and Evidence Lower Bound (ELBO)</h2>
<p>The VAE objective function is derived from the variational lower bound on the marginal likelihood of the data:</p>
<p><span class="math display">\[
\log p(x) \geq \mathbb{E}_{q_\phi(z|x)}[\log p_\theta(x|z)] - D_{KL}(q_\phi(z|x) \parallel p(z))
\]</span></p>
<p>This lower bound is called the Evidence Lower Bound (ELBO). The VAE training objective is to maximize the ELBO, which is equivalent to minimizing the following loss function:</p>
<p><span class="math display">\[
\mathcal{L}(\theta, \phi; x) = -\mathbb{E}_{q_\phi(z|x)}[\log p_\theta(x|z)] + D_{KL}(q_\phi(z|x) \parallel p(z))
\]</span></p>
<section id="reconstruction-loss" class="level3">
<h3 class="anchored" data-anchor-id="reconstruction-loss">Reconstruction Loss</h3>
<p>The first term in the ELBO is the reconstruction loss, which measures how well the decoder can reconstruct the input data from the latent representation:</p>
<p><span class="math display">\[
\mathbb{E}_{q_\phi(z|x)}[\log p_\theta(x|z)]
\]</span></p>
<p>This term encourages the decoder to accurately reconstruct the input data. It is often implemented as the negative log-likelihood of the data given the latent variables, which can take different forms depending on the data type:</p>
<ul>
<li><strong>Mean Squared Error (MSE)</strong>: For continuous data.</li>
<li><strong>Binary Cross-Entropy (BCE)</strong>: For binary data.</li>
<li><strong>Categorical Cross-Entropy</strong>: For categorical data.</li>
</ul>
<section id="mathematical-forms-of-reconstruction-loss" class="level4">
<h4 class="anchored" data-anchor-id="mathematical-forms-of-reconstruction-loss">Mathematical Forms of Reconstruction Loss</h4>
<ul>
<li>For continuous data, the reconstruction loss can be expressed as:</li>
</ul>
<p><span class="math display">\[
\mathbb{E}_{q_\phi(z|x)}[\log p_\theta(x|z)] = - \frac{1}{2\sigma^2} \|x - \hat{x}\|^2
\]</span></p>
<ul>
<li>For binary data, the reconstruction loss is typically:</li>
</ul>
<p><span class="math display">\[
\mathbb{E}_{q_\phi(z|x)}[\log p_\theta(x|z)] = -\sum_{i=1}^N [x_i \log \hat{x}_i + (1 - x_i) \log (1 - \hat{x}_i)]
\]</span></p>
<ul>
<li>For categorical data, the reconstruction loss can be expressed using categorical cross-entropy:</li>
</ul>
<p><span class="math display">\[
\mathbb{E}_{q_\phi(z|x)}[\log p_\theta(x|z)] = -\sum_{i=1}^N \sum_{c=1}^C x_{i,c} \log \hat{x}_{i,c}
\]</span></p>
</section>
</section>
<section id="kullback-leibler-divergence-1" class="level3">
<h3 class="anchored" data-anchor-id="kullback-leibler-divergence-1">Kullback-Leibler Divergence</h3>
<p>The second term in the ELBO is the Kullback-Leibler divergence, which measures the difference between the approximate posterior <span class="math inline">\(q_\phi(z|x)\)</span> and the prior distribution <span class="math inline">\(p(z)\)</span>:</p>
<p><span class="math display">\[
D_{KL}(q_\phi(z|x) \parallel p(z))
\]</span></p>
<p>This term acts as a regularizer, encouraging the approximate posterior to be close to the prior distribution (typically a standard normal distribution). The KL divergence for two Gaussian distributions has a closed-form expression:</p>
<p><span class="math display">\[
D_{KL}(\mathcal{N}(\mu_\phi(x), \sigma_\phi^2(x)) \parallel \mathcal{N}(0, I)) = \frac{1}{2} \sum_{j=1}^J \left( \sigma_\phi^2(x)_j + \mu_\phi(x)_j^2 - 1 - \log \sigma_\phi^2(x)_j \right)
\]</span></p>
</section>
</section>
<section id="generating-samples-from-learned-latent-space" class="level2">
<h2 class="anchored" data-anchor-id="generating-samples-from-learned-latent-space">Generating Samples from Learned Latent Space</h2>
<p>Once a VAE is trained, generating new samples involves two steps:</p>
<ol type="1">
<li>Sample a point <span class="math inline">\(z\)</span> from the prior distribution <span class="math inline">\(p(z)\)</span> (usually <span class="math inline">\(\mathcal{N}(0, I)\)</span>).</li>
<li>Pass <span class="math inline">\(z\)</span> through the decoder network to obtain <span class="math inline">\(p_\theta(x|z)\)</span>, and sample from this distribution to get a generated sample.</li>
</ol>
<section id="sampling-from-the-prior" class="level3">
<h3 class="anchored" data-anchor-id="sampling-from-the-prior">Sampling from the Prior</h3>
<p>The prior distribution <span class="math inline">\(p(z)\)</span> is typically chosen to be a standard normal distribution:</p>
<p><span class="math display">\[
p(z) = \mathcal{N}(z; 0, I)
\]</span></p>
<p>This choice simplifies the Kullback-Leibler divergence term and ensures that the latent space is well-behaved.</p>
</section>
<section id="decoding-the-latent-variable" class="level3">
<h3 class="anchored" data-anchor-id="decoding-the-latent-variable">Decoding the Latent Variable</h3>
<p>The decoder network maps the latent variable <span class="math inline">\(z\)</span> to the parameters of the distribution <span class="math inline">\(p_\theta(x|z)\)</span>, from which we can sample to obtain a generated data point:</p>
<p><span class="math display">\[
x \sim p_\theta(x|z)
\]</span></p>
<p>This process allows VAEs to generate diverse and realistic samples that resemble the training data.</p>
</section>
</section>
<section id="techniques-for-improving-vae-performance" class="level2">
<h2 class="anchored" data-anchor-id="techniques-for-improving-vae-performance">Techniques for Improving VAE Performance</h2>
<section id="β-vae" class="level3">
<h3 class="anchored" data-anchor-id="β-vae">β-VAE</h3>
<p>β-VAE introduces a hyperparameter <span class="math inline">\(\beta\)</span> to control the trade-off between reconstruction quality and disentanglement of the latent space:</p>
<p><span class="math display">\[
\mathcal{L}_{\beta}(\theta, \phi; x, \beta) = -\mathbb{E}_{q_\phi(z|x)}[\log p_\theta(x|z)] + \beta \cdot D_{KL}(q_\phi(z|x) \parallel p(z))
\]</span></p>
<p>By increasing <span class="math inline">\(\beta\)</span>, the model is encouraged to learn more disentangled representations, where each dimension of the latent space corresponds to a single, interpretable factor of variation in the data.</p>
</section>
<section id="factorvae" class="level3">
<h3 class="anchored" data-anchor-id="factorvae">FactorVAE</h3>
<p>FactorVAE aims to achieve disentanglement by encouraging the marginal distribution of the latent variables to factorize into independent components. It adds an additional term to the VAE objective:</p>
<p><span class="math display">\[
\mathcal{L}_{FactorVAE} = \mathcal{L}_{VAE} + \gamma \cdot D_{KL}(q(z) \parallel \prod_j q(z_j))
\]</span></p>
<p>Where <span class="math inline">\(q(z)\)</span> is the aggregate posterior and <span class="math inline">\(\prod_j q(z_j)\)</span> is its factorized version. The hyperparameter <span class="math inline">\(\gamma\)</span> controls the strength of this additional constraint.</p>
</section>
<section id="infovae" class="level3">
<h3 class="anchored" data-anchor-id="infovae">InfoVAE</h3>
<p>InfoVAE addresses the issue of uninformative latent codes by maximizing the mutual information between the input and the latent representation. It modifies the VAE objective as follows:</p>
<p><span class="math display">\[
\mathcal{L}_{InfoVAE} = \mathbb{E}_{p_d(x)}[-\mathbb{E}_{q_\phi(z|x)}[\log p_\theta(x|z)]] + (1-\alpha)D_{KL}(q_\phi(z|x)p_d(x) \parallel p(z)) + (\alpha + \lambda - 1)D_{KL}(q_\phi(z) \parallel p(z))
\]</span></p>
<p>Where <span class="math inline">\(p_d(x)\)</span> is the data distribution, <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\lambda\)</span> are hyperparameters controlling the weight of different terms, and <span class="math inline">\(q_\phi(z)\)</span> is the aggregate posterior.</p>
</section>
<section id="other-techniques" class="level3">
<h3 class="anchored" data-anchor-id="other-techniques">Other Techniques</h3>
<section id="importance-weighted-autoencoders-iwae" class="level4">
<h4 class="anchored" data-anchor-id="importance-weighted-autoencoders-iwae">Importance Weighted Autoencoders (IWAE)</h4>
<p>IWAE improves the ELBO by using importance sampling to provide a tighter bound on the marginal likelihood. The IWAE objective is:</p>
<p><span class="math display">\[
\mathcal{L}_{IWAE} = \mathbb{E}_{q_\phi(z_{1:K}|x)}\left[\log \frac{1}{K} \sum_{k=1}^K \frac{p_\theta(x, z_k)}{q_\phi(z_k|x)}\right]
\]</span></p>
<p>Where <span class="math inline">\(K\)</span> is the number of importance samples. The idea is to use multiple samples from the latent distribution to approximate the likelihood more accurately, resulting in a better approximation of the true posterior.</p>
</section>
<section id="hierarchical-vaes-hvaes" class="level4">
<h4 class="anchored" data-anchor-id="hierarchical-vaes-hvaes">Hierarchical VAEs (HVAEs)</h4>
<p>HVAEs introduce multiple layers of latent variables to capture more complex data distributions. Each layer of latent variables is conditioned on the previous layer, allowing the model to learn hierarchical representations. This is particularly useful for data with complex structures, such as images or text.</p>
<section id="architecture-of-hvaes" class="level5">
<h5 class="anchored" data-anchor-id="architecture-of-hvaes"><strong>Architecture of HVAEs</strong></h5>
<ol type="1">
<li><strong>First Layer</strong>: Encodes the input data into a set of latent variables.</li>
<li><strong>Subsequent Layers</strong>: Each subsequent layer encodes the latent variables from the previous layer into a higher-level latent representation.</li>
<li><strong>Decoder</strong>: Reconstructs the data by decoding from the highest layer of latent variables back to the data space.</li>
</ol>
</section>
</section>
<section id="disentangled-vaes" class="level4">
<h4 class="anchored" data-anchor-id="disentangled-vaes">Disentangled VAEs</h4>
<p>Disentangled VAEs aim to learn latent representations where each dimension corresponds to a distinct factor of variation in the data. Techniques like β-VAE, FactorVAE, and InfoVAE are designed to achieve disentanglement. Disentangled representations are useful for understanding and manipulating the underlying factors that generate the data.</p>
<section id="benefits-of-disentangled-representations" class="level5">
<h5 class="anchored" data-anchor-id="benefits-of-disentangled-representations"><strong>Benefits of Disentangled Representations</strong></h5>
<ul>
<li><strong>Interpretability</strong>: Each dimension of the latent space can be interpreted as a specific factor of variation.</li>
<li><strong>Control</strong>: Allows for controlled generation and modification of samples by manipulating individual latent dimensions.</li>
<li><strong>Improved Generalization</strong>: Better disentanglement can lead to improved generalization to new, unseen data.</li>
</ul>
</section>
</section>
<section id="semi-supervised-vaes" class="level4">
<h4 class="anchored" data-anchor-id="semi-supervised-vaes">Semi-Supervised VAEs</h4>
<p>Semi-supervised VAEs incorporate labeled data to improve the quality of the learned representations. They extend the VAE framework to handle both labeled and unlabeled data, enabling tasks like classification and regression.</p>
<section id="semi-supervised-learning-with-vaes" class="level5">
<h5 class="anchored" data-anchor-id="semi-supervised-learning-with-vaes"><strong>Semi-Supervised Learning with VAEs</strong></h5>
<ul>
<li><strong>Labeled Data</strong>: Used to train the model with supervised learning, guiding the latent space to be informative for the labels.</li>
<li><strong>Unlabeled Data</strong>: Used to capture the overall data distribution, leveraging the generative capabilities of VAEs.</li>
</ul>
<p>These techniques aim to address various limitations of standard VAEs, such as poor reconstruction quality, lack of disentanglement in the latent space, and uninformative latent codes. By modifying the objective function or introducing additional constraints, these methods can lead to improved performance and more interpretable latent representations.</p>
</section>
</section>
</section>
</section>
</section>



</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>