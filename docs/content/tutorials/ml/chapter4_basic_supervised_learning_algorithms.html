<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.47">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Ravi Shankar">

<title>chapter4_basic_supervised_learning_algorithms – Data Down To Earth</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../">
<link href="../../../logo.png" rel="icon" type="image/png">
<script src="../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../../styles.css">
<link rel="stylesheet" href="../../content.css">
<meta property="og:title" content="– Data Down To Earth">
<meta property="og:site_name" content="Data Down To Earth">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="../../../index.html" class="navbar-brand navbar-brand-logo">
    <img src="../../../logo.png" alt="" class="navbar-logo">
    </a>
    <a class="navbar-brand" href="../../../index.html">
    <span class="navbar-title">Data Down To Earth</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-projects" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Projects</span>
    </a>
    <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="nav-menu-projects">    
        <li>
    <a class="dropdown-item" href="../../../projects/dsml_projects.html">
 <span class="dropdown-text">Data Science and ML Projects</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../projects/genai_projects.html">
 <span class="dropdown-text">Generative AI Projects</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-tutorials" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Tutorials</span>
    </a>
    <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="nav-menu-tutorials">    
        <li>
    <a class="dropdown-item" href="../../../topics/statistics.html">
 <span class="dropdown-text">Statistics &amp; Probability</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../topics/machine_learning.html">
 <span class="dropdown-text">Machine Learning</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../topics/de.html">
 <span class="dropdown-text">Data Engineering</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../topics/generative_ai.html">
 <span class="dropdown-text">Generative AI</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../topics/public_health.html">
 <span class="dropdown-text">Public Health</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../topics/product_sense.html">
 <span class="dropdown-text">Product Sense</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item">
    <a class="nav-link" href="../../../notes/notes.html"> 
<span class="menu-text">Notes &amp; Research</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/iamrsps"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/iamravishankar/"> <i class="bi bi-linkedin" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content column-page" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
</div>



<div class="quarto-title-meta column-page">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Ravi Shankar </p>
          </div>
  </div>
    
  
    
  </div>
  


</header>


<div class="text-content">
<section id="linear-regression" class="level1">
<h1>4.1 Linear Regression</h1>
<p>Linear regression is a statistical method used to model the relationship between a dependent variable and one or more independent variables. The goal is to find the best-fitting line that predicts the dependent variable based on the independent variables.</p>
<section id="simple-linear-regression" class="level2">
<h2 class="anchored" data-anchor-id="simple-linear-regression">4.1.1 Simple Linear Regression</h2>
<p>Simple linear regression models the relationship between a single independent variable and a dependent variable by fitting a straight line to the data.</p>
<ul>
<li><p><strong>Example:</strong> Predicting house prices based on square footage.</p></li>
<li><p><strong>Formula:</strong> <span class="math display">\[
y = \beta_0 + \beta_1 x + \epsilon
\]</span> where <span class="math inline">\(y\)</span> is the dependent variable, <span class="math inline">\(x\)</span> is the independent variable, <span class="math inline">\(\beta_0\)</span> is the intercept, <span class="math inline">\(\beta_1\)</span> is the slope, and <span class="math inline">\(\epsilon\)</span> is the error term.</p></li>
<li><p><strong>Steps:</strong></p>
<ol type="1">
<li><p><strong>Estimate the parameters:</strong> Use the least squares method to estimate <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>.</p>
<p>Formula for the slope (<span class="math inline">\(\beta_1\)</span>): <span class="math display">\[
\beta_1 = \frac{\sum (x_i - \bar{x})(y_i - \bar{y})}{\sum (x_i - \bar{x})^2}
\]</span></p>
<p>Formula for the intercept (<span class="math inline">\(\beta_0\)</span>): <span class="math display">\[
\beta_0 = \bar{y} - \beta_1 \bar{x}
\]</span></p></li>
<li><p><strong>Make predictions:</strong> Use the estimated parameters to predict new values of <span class="math inline">\(y\)</span>.</p></li>
</ol></li>
</ul>
</section>
<section id="multiple-linear-regression" class="level2">
<h2 class="anchored" data-anchor-id="multiple-linear-regression">4.1.2 Multiple Linear Regression</h2>
<p>Multiple linear regression models the relationship between two or more independent variables and a dependent variable by fitting a linear equation to the data.</p>
<ul>
<li><p><strong>Example:</strong> Predicting house prices based on square footage, number of bedrooms, and age of the house.</p></li>
<li><p><strong>Formula:</strong> <span class="math display">\[
y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \cdots + \beta_p x_p + \epsilon
\]</span> where <span class="math inline">\(y\)</span> is the dependent variable, <span class="math inline">\(x_1, x_2, \ldots, x_p\)</span> are the independent variables, <span class="math inline">\(\beta_0\)</span> is the intercept, <span class="math inline">\(\beta_1, \beta_2, \ldots, \beta_p\)</span> are the coefficients, and <span class="math inline">\(\epsilon\)</span> is the error term.</p></li>
<li><p><strong>Steps:</strong></p>
<ol type="1">
<li><p><strong>Estimate the parameters:</strong> Use the least squares method to estimate <span class="math inline">\(\beta_0, \beta_1, \ldots, \beta_p\)</span>.</p>
<p>Matrix notation: <span class="math display">\[
\mathbf{\beta} = (\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}^T \mathbf{y}
\]</span></p></li>
<li><p><strong>Make predictions:</strong> Use the estimated parameters to predict new values of <span class="math inline">\(y\)</span>.</p></li>
</ol></li>
</ul>
</section>
<section id="polynomial-regression" class="level2">
<h2 class="anchored" data-anchor-id="polynomial-regression">4.1.3 Polynomial Regression</h2>
<p>Polynomial regression is a type of multiple linear regression where the relationship between the independent variable and the dependent variable is modeled as an nth degree polynomial.</p>
<ul>
<li><p><strong>Example:</strong> Predicting the growth of a plant based on time, where the growth rate accelerates over time.</p></li>
<li><p><strong>Formula:</strong> <span class="math display">\[
y = \beta_0 + \beta_1 x + \beta_2 x^2 + \cdots + \beta_n x^n + \epsilon
\]</span></p></li>
<li><p><strong>Steps:</strong></p>
<ol type="1">
<li><p><strong>Transform the independent variable:</strong> Create new features by raising the independent variable to the power of 2, 3, …, n.</p>
<p>Example: For <span class="math inline">\(x\)</span>, create <span class="math inline">\(x^2, x^3, \ldots, x^n\)</span>.</p></li>
<li><p><strong>Estimate the parameters:</strong> Use the least squares method to estimate <span class="math inline">\(\beta_0, \beta_1, \ldots, \beta_n\)</span>.</p></li>
<li><p><strong>Make predictions:</strong> Use the estimated parameters to predict new values of <span class="math inline">\(y\)</span>.</p></li>
</ol></li>
</ul>
<div id="84d3d8c3" class="cell" data-execution_count="1">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LinearRegression</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> mean_squared_error, r2_score</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> PolynomialFeatures</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Load example dataset: Predicting house prices based on square footage</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Generating synthetic data for simplicity</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>square_footage <span class="op">=</span> <span class="dv">2</span> <span class="op">*</span> np.random.rand(<span class="dv">100</span>, <span class="dv">1</span>) <span class="op">+</span> <span class="dv">1</span>  <span class="co"># Square footage in thousands</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>price <span class="op">=</span> <span class="dv">4</span> <span class="op">+</span> <span class="dv">3</span> <span class="op">*</span> square_footage <span class="op">+</span> np.random.randn(<span class="dv">100</span>, <span class="dv">1</span>)  <span class="co"># House price in hundred thousands</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert to pandas DataFrame</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> pd.DataFrame({<span class="st">'SquareFootage'</span>: square_footage.flatten(), <span class="st">'Price'</span>: price.flatten()})</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Simple Linear Regression</span></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 1: Split the data</span></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> data[[<span class="st">'SquareFootage'</span>]]</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> data[<span class="st">'Price'</span>]</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, y, test_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 2: Estimate the parameters using least squares method</span></span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>linear_model <span class="op">=</span> LinearRegression()</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>linear_model.fit(X_train, y_train)</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 3: Make predictions</span></span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> linear_model.predict(X_test)</span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a><span class="co"># Evaluate the model</span></span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>mse <span class="op">=</span> mean_squared_error(y_test, y_pred)</span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a>r2 <span class="op">=</span> r2_score(y_test, y_pred)</span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'Simple Linear Regression MSE: </span><span class="sc">{</span>mse<span class="sc">:.2f}</span><span class="ss">'</span>)</span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'Simple Linear Regression R^2: </span><span class="sc">{</span>r2<span class="sc">:.2f}</span><span class="ss">'</span>)</span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the results</span></span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">6</span>))</span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a>plt.scatter(X, y, color<span class="op">=</span><span class="st">'blue'</span>)</span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a>plt.plot(X_test, y_pred, color<span class="op">=</span><span class="st">'red'</span>, linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Square Footage (thousands)'</span>)</span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Price (hundred thousands)'</span>)</span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Simple Linear Regression: House Price vs Square Footage'</span>)</span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a><span class="co"># Multiple Linear Regression</span></span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a><span class="co"># Generating additional features: number of bedrooms and age of the house</span></span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb1-49"><a href="#cb1-49" aria-hidden="true" tabindex="-1"></a>bedrooms <span class="op">=</span> np.random.randint(<span class="dv">1</span>, <span class="dv">5</span>, size<span class="op">=</span>(<span class="dv">100</span>, <span class="dv">1</span>))</span>
<span id="cb1-50"><a href="#cb1-50" aria-hidden="true" tabindex="-1"></a>age <span class="op">=</span> np.random.randint(<span class="dv">1</span>, <span class="dv">30</span>, size<span class="op">=</span>(<span class="dv">100</span>, <span class="dv">1</span>))</span>
<span id="cb1-51"><a href="#cb1-51" aria-hidden="true" tabindex="-1"></a>price <span class="op">=</span> <span class="dv">4</span> <span class="op">+</span> <span class="dv">3</span> <span class="op">*</span> square_footage <span class="op">+</span> <span class="fl">1.5</span> <span class="op">*</span> bedrooms <span class="op">-</span> <span class="fl">0.05</span> <span class="op">*</span> age <span class="op">+</span> np.random.randn(<span class="dv">100</span>, <span class="dv">1</span>)  <span class="co"># House price in hundred thousands</span></span>
<span id="cb1-52"><a href="#cb1-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-53"><a href="#cb1-53" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert to pandas DataFrame</span></span>
<span id="cb1-54"><a href="#cb1-54" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> pd.DataFrame({<span class="st">'SquareFootage'</span>: square_footage.flatten(), <span class="st">'Bedrooms'</span>: bedrooms.flatten(), <span class="st">'Age'</span>: age.flatten(), <span class="st">'Price'</span>: price.flatten()})</span>
<span id="cb1-55"><a href="#cb1-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-56"><a href="#cb1-56" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 1: Split the data</span></span>
<span id="cb1-57"><a href="#cb1-57" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> data[[<span class="st">'SquareFootage'</span>, <span class="st">'Bedrooms'</span>, <span class="st">'Age'</span>]]</span>
<span id="cb1-58"><a href="#cb1-58" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> data[<span class="st">'Price'</span>]</span>
<span id="cb1-59"><a href="#cb1-59" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, y, test_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb1-60"><a href="#cb1-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-61"><a href="#cb1-61" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 2: Estimate the parameters using least squares method</span></span>
<span id="cb1-62"><a href="#cb1-62" aria-hidden="true" tabindex="-1"></a>multiple_linear_model <span class="op">=</span> LinearRegression()</span>
<span id="cb1-63"><a href="#cb1-63" aria-hidden="true" tabindex="-1"></a>multiple_linear_model.fit(X_train, y_train)</span>
<span id="cb1-64"><a href="#cb1-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-65"><a href="#cb1-65" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 3: Make predictions</span></span>
<span id="cb1-66"><a href="#cb1-66" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> multiple_linear_model.predict(X_test)</span>
<span id="cb1-67"><a href="#cb1-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-68"><a href="#cb1-68" aria-hidden="true" tabindex="-1"></a><span class="co"># Evaluate the model</span></span>
<span id="cb1-69"><a href="#cb1-69" aria-hidden="true" tabindex="-1"></a>mse <span class="op">=</span> mean_squared_error(y_test, y_pred)</span>
<span id="cb1-70"><a href="#cb1-70" aria-hidden="true" tabindex="-1"></a>r2 <span class="op">=</span> r2_score(y_test, y_pred)</span>
<span id="cb1-71"><a href="#cb1-71" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'Multiple Linear Regression MSE: </span><span class="sc">{</span>mse<span class="sc">:.2f}</span><span class="ss">'</span>)</span>
<span id="cb1-72"><a href="#cb1-72" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'Multiple Linear Regression R^2: </span><span class="sc">{</span>r2<span class="sc">:.2f}</span><span class="ss">'</span>)</span>
<span id="cb1-73"><a href="#cb1-73" aria-hidden="true" tabindex="-1"></a><span class="co"># Polynomial Regression</span></span>
<span id="cb1-74"><a href="#cb1-74" aria-hidden="true" tabindex="-1"></a><span class="co"># Generating synthetic data for simplicity</span></span>
<span id="cb1-75"><a href="#cb1-75" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb1-76"><a href="#cb1-76" aria-hidden="true" tabindex="-1"></a>time <span class="op">=</span> <span class="dv">2</span> <span class="op">*</span> np.random.rand(<span class="dv">100</span>, <span class="dv">1</span>)  <span class="co"># Time in months</span></span>
<span id="cb1-77"><a href="#cb1-77" aria-hidden="true" tabindex="-1"></a>growth <span class="op">=</span> <span class="dv">1</span> <span class="op">+</span> <span class="dv">2</span> <span class="op">*</span> time <span class="op">+</span> time <span class="op">**</span> <span class="dv">2</span> <span class="op">+</span> np.random.randn(<span class="dv">100</span>, <span class="dv">1</span>)  <span class="co"># Growth in cm</span></span>
<span id="cb1-78"><a href="#cb1-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-79"><a href="#cb1-79" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert to pandas DataFrame</span></span>
<span id="cb1-80"><a href="#cb1-80" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> pd.DataFrame({<span class="st">'Time'</span>: time.flatten(), <span class="st">'Growth'</span>: growth.flatten()})</span>
<span id="cb1-81"><a href="#cb1-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-82"><a href="#cb1-82" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 1: Transform the independent variable</span></span>
<span id="cb1-83"><a href="#cb1-83" aria-hidden="true" tabindex="-1"></a>poly_features <span class="op">=</span> PolynomialFeatures(degree<span class="op">=</span><span class="dv">2</span>, include_bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb1-84"><a href="#cb1-84" aria-hidden="true" tabindex="-1"></a>X_poly <span class="op">=</span> poly_features.fit_transform(data[[<span class="st">'Time'</span>]])</span>
<span id="cb1-85"><a href="#cb1-85" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-86"><a href="#cb1-86" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 2: Split the data</span></span>
<span id="cb1-87"><a href="#cb1-87" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X_poly, data[<span class="st">'Growth'</span>], test_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb1-88"><a href="#cb1-88" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-89"><a href="#cb1-89" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 3: Estimate the parameters using least squares method</span></span>
<span id="cb1-90"><a href="#cb1-90" aria-hidden="true" tabindex="-1"></a>poly_model <span class="op">=</span> LinearRegression()</span>
<span id="cb1-91"><a href="#cb1-91" aria-hidden="true" tabindex="-1"></a>poly_model.fit(X_train, y_train)</span>
<span id="cb1-92"><a href="#cb1-92" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-93"><a href="#cb1-93" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 4: Make predictions</span></span>
<span id="cb1-94"><a href="#cb1-94" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> poly_model.predict(X_test)</span>
<span id="cb1-95"><a href="#cb1-95" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-96"><a href="#cb1-96" aria-hidden="true" tabindex="-1"></a><span class="co"># Evaluate the model</span></span>
<span id="cb1-97"><a href="#cb1-97" aria-hidden="true" tabindex="-1"></a>mse <span class="op">=</span> mean_squared_error(y_test, y_pred)</span>
<span id="cb1-98"><a href="#cb1-98" aria-hidden="true" tabindex="-1"></a>r2 <span class="op">=</span> r2_score(y_test, y_pred)</span>
<span id="cb1-99"><a href="#cb1-99" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'Polynomial Regression MSE: </span><span class="sc">{</span>mse<span class="sc">:.2f}</span><span class="ss">'</span>)</span>
<span id="cb1-100"><a href="#cb1-100" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'Polynomial Regression R^2: </span><span class="sc">{</span>r2<span class="sc">:.2f}</span><span class="ss">'</span>)</span>
<span id="cb1-101"><a href="#cb1-101" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-102"><a href="#cb1-102" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the results</span></span>
<span id="cb1-103"><a href="#cb1-103" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">6</span>))</span>
<span id="cb1-104"><a href="#cb1-104" aria-hidden="true" tabindex="-1"></a>plt.scatter(data[<span class="st">'Time'</span>], data[<span class="st">'Growth'</span>], color<span class="op">=</span><span class="st">'blue'</span>)</span>
<span id="cb1-105"><a href="#cb1-105" aria-hidden="true" tabindex="-1"></a>plt.plot(np.sort(data[<span class="st">'Time'</span>]), poly_model.predict(poly_features.transform(np.sort(data[[<span class="st">'Time'</span>]]))), color<span class="op">=</span><span class="st">'red'</span>, linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb1-106"><a href="#cb1-106" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Time (months)'</span>)</span>
<span id="cb1-107"><a href="#cb1-107" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Growth (cm)'</span>)</span>
<span id="cb1-108"><a href="#cb1-108" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Polynomial Regression: Plant Growth vs Time'</span>)</span>
<span id="cb1-109"><a href="#cb1-109" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Simple Linear Regression MSE: 0.65
Simple Linear Regression R^2: 0.81</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="chapter4_basic_supervised_learning_algorithms_files/figure-html/cell-2-output-2.png" width="808" height="523" class="figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Multiple Linear Regression MSE: 1.30
Multiple Linear Regression R^2: 0.76
Polynomial Regression MSE: 0.64
Polynomial Regression R^2: 0.89</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>/Users/ravishankar/miniforge3/lib/python3.10/site-packages/sklearn/base.py:439: UserWarning:

X does not have valid feature names, but PolynomialFeatures was fitted with feature names
</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="chapter4_basic_supervised_learning_algorithms_files/figure-html/cell-2-output-5.png" width="808" height="523" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<hr>
</section>
<section id="assumptions-of-linear-regression" class="level2">
<h2 class="anchored" data-anchor-id="assumptions-of-linear-regression">4.1.4 Assumptions of Linear Regression</h2>
<p>Linear regression relies on several key assumptions. Violations of these assumptions can lead to biased or inefficient estimates.</p>
<ul>
<li><p><strong>Linearity:</strong> The relationship between the dependent and independent variables is linear.</p>
<ul>
<li><strong>Example:</strong> A scatter plot showing a straight-line relationship between variables.</li>
</ul></li>
<li><p><strong>Independence:</strong> Observations are independent of each other.</p>
<ul>
<li><strong>Example:</strong> No patterns in the residuals when plotted against time.</li>
</ul></li>
<li><p><strong>Homoscedasticity:</strong> The variance of the errors is constant across all levels of the independent variables.</p>
<ul>
<li><strong>Example:</strong> Residuals plot showing no funnel shape.</li>
</ul></li>
<li><p><strong>Normality:</strong> The errors are normally distributed.</p>
<ul>
<li><strong>Example:</strong> A Q-Q plot of residuals showing a straight-line pattern.</li>
</ul></li>
<li><p><strong>No multicollinearity:</strong> Independent variables are not highly correlated.</p>
<ul>
<li><strong>Example:</strong> Variance Inflation Factor (VIF) values less than 10.</li>
</ul></li>
</ul>
</section>
<section id="gradient-descent-for-linear-regression" class="level2">
<h2 class="anchored" data-anchor-id="gradient-descent-for-linear-regression">4.1.5 Gradient Descent for Linear Regression</h2>
<p>Gradient descent is an iterative optimization algorithm used to minimize the cost function in linear regression.</p>
<ul>
<li><p><strong>Example:</strong> Finding the optimal parameters for a regression model predicting sales based on advertising spend.</p></li>
<li><p><strong>Formula:</strong> <span class="math display">\[
\beta_j := \beta_j - \alpha \frac{\partial J(\beta)}{\partial \beta_j}
\]</span> where <span class="math inline">\(\alpha\)</span> is the learning rate, <span class="math inline">\(J(\beta)\)</span> is the cost function, and <span class="math inline">\(\frac{\partial J(\beta)}{\partial \beta_j}\)</span> is the partial derivative of the cost function with respect to <span class="math inline">\(\beta_j\)</span>.</p></li>
<li><p><strong>Steps:</strong></p>
<ol type="1">
<li><p><strong>Initialize parameters:</strong> Start with initial guesses for <span class="math inline">\(\beta_0, \beta_1, \ldots, \beta_p\)</span>.</p></li>
<li><p><strong>Compute the cost function:</strong> Calculate the mean squared error (MSE) for the current parameters.</p>
<p>Formula: <span class="math display">\[
J(\beta) = \frac{1}{2m} \sum_{i=1}^{m} (h_\beta(x_i) - y_i)^2
\]</span></p></li>
<li><p><strong>Update parameters:</strong> Adjust the parameters in the direction that reduces the cost function.</p></li>
<li><p><strong>Repeat:</strong> Iterate the process until convergence.</p></li>
</ol></li>
</ul>
<div id="4ad65c3f" class="cell" data-execution_count="2">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>    <span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>    <span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>    <span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Generate synthetic data: Predicting sales based on advertising spend</span></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>    np.random.seed(<span class="dv">42</span>)</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>    advertising_spend <span class="op">=</span> <span class="dv">2</span> <span class="op">*</span> np.random.rand(<span class="dv">100</span>, <span class="dv">1</span>)</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>    sales <span class="op">=</span> <span class="dv">4</span> <span class="op">+</span> <span class="dv">3</span> <span class="op">*</span> advertising_spend <span class="op">+</span> np.random.randn(<span class="dv">100</span>, <span class="dv">1</span>)</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Convert to pandas DataFrame</span></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>    data <span class="op">=</span> pd.DataFrame({<span class="st">'AdvertisingSpend'</span>: advertising_spend.flatten(), <span class="st">'Sales'</span>: sales.flatten()})</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Feature matrix and target vector</span></span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>    X <span class="op">=</span> data[[<span class="st">'AdvertisingSpend'</span>]].values</span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>    y <span class="op">=</span> data[<span class="st">'Sales'</span>].values</span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Add a column of ones to include the intercept term (bias) in the model</span></span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a>    X_b <span class="op">=</span> np.c_[np.ones((<span class="dv">100</span>, <span class="dv">1</span>)), X]</span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Gradient Descent function for Linear Regression</span></span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> gradient_descent(X, y, learning_rate<span class="op">=</span><span class="fl">0.1</span>, n_iterations<span class="op">=</span><span class="dv">1000</span>):</span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a>        m <span class="op">=</span> <span class="bu">len</span>(y)</span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a>        theta <span class="op">=</span> np.random.randn(<span class="dv">2</span>, <span class="dv">1</span>)  <span class="co"># Random initialization of parameters</span></span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-25"><a href="#cb5-25" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> iteration <span class="kw">in</span> <span class="bu">range</span>(n_iterations):</span>
<span id="cb5-26"><a href="#cb5-26" aria-hidden="true" tabindex="-1"></a>            gradients <span class="op">=</span> (<span class="dv">2</span><span class="op">/</span>m) <span class="op">*</span> X.T.dot(X.dot(theta) <span class="op">-</span> y.reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>))</span>
<span id="cb5-27"><a href="#cb5-27" aria-hidden="true" tabindex="-1"></a>            theta <span class="op">=</span> theta <span class="op">-</span> learning_rate <span class="op">*</span> gradients</span>
<span id="cb5-28"><a href="#cb5-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-29"><a href="#cb5-29" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> theta</span>
<span id="cb5-30"><a href="#cb5-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-31"><a href="#cb5-31" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Step 1: Initialize parameters</span></span>
<span id="cb5-32"><a href="#cb5-32" aria-hidden="true" tabindex="-1"></a>    learning_rate <span class="op">=</span> <span class="fl">0.1</span></span>
<span id="cb5-33"><a href="#cb5-33" aria-hidden="true" tabindex="-1"></a>    n_iterations <span class="op">=</span> <span class="dv">1000</span></span>
<span id="cb5-34"><a href="#cb5-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-35"><a href="#cb5-35" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Step 2: Compute the cost function and Step 3: Update parameters</span></span>
<span id="cb5-36"><a href="#cb5-36" aria-hidden="true" tabindex="-1"></a>    theta_optimal <span class="op">=</span> gradient_descent(X_b, y, learning_rate, n_iterations)</span>
<span id="cb5-37"><a href="#cb5-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-38"><a href="#cb5-38" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Predictions using the optimal parameters</span></span>
<span id="cb5-39"><a href="#cb5-39" aria-hidden="true" tabindex="-1"></a>    X_new <span class="op">=</span> np.array([[<span class="dv">0</span>], [<span class="dv">2</span>]])  <span class="co"># New advertising spend data for predictions</span></span>
<span id="cb5-40"><a href="#cb5-40" aria-hidden="true" tabindex="-1"></a>    X_new_b <span class="op">=</span> np.c_[np.ones((<span class="dv">2</span>, <span class="dv">1</span>)), X_new]</span>
<span id="cb5-41"><a href="#cb5-41" aria-hidden="true" tabindex="-1"></a>    y_predict <span class="op">=</span> X_new_b.dot(theta_optimal)</span>
<span id="cb5-42"><a href="#cb5-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-43"><a href="#cb5-43" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Plot the results</span></span>
<span id="cb5-44"><a href="#cb5-44" aria-hidden="true" tabindex="-1"></a>    plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">6</span>))</span>
<span id="cb5-45"><a href="#cb5-45" aria-hidden="true" tabindex="-1"></a>    plt.scatter(X, y, color<span class="op">=</span><span class="st">'blue'</span>)</span>
<span id="cb5-46"><a href="#cb5-46" aria-hidden="true" tabindex="-1"></a>    plt.plot(X_new, y_predict, color<span class="op">=</span><span class="st">'red'</span>, linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb5-47"><a href="#cb5-47" aria-hidden="true" tabindex="-1"></a>    plt.xlabel(<span class="st">'Advertising Spend (in $1000)'</span>)</span>
<span id="cb5-48"><a href="#cb5-48" aria-hidden="true" tabindex="-1"></a>    plt.ylabel(<span class="st">'Sales (in $1000)'</span>)</span>
<span id="cb5-49"><a href="#cb5-49" aria-hidden="true" tabindex="-1"></a>    plt.title(<span class="st">'Gradient Descent for Linear Regression: Sales vs Advertising Spend'</span>)</span>
<span id="cb5-50"><a href="#cb5-50" aria-hidden="true" tabindex="-1"></a>    plt.show()</span>
<span id="cb5-51"><a href="#cb5-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-52"><a href="#cb5-52" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Print the optimal parameters</span></span>
<span id="cb5-53"><a href="#cb5-53" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f'Optimal parameters (theta): </span><span class="sc">{</span>theta_optimal<span class="sc">.</span>flatten()<span class="sc">}</span><span class="ss">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="chapter4_basic_supervised_learning_algorithms_files/figure-html/cell-3-output-1.png" width="808" height="523" class="figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Optimal parameters (theta): [4.21509616 2.77011339]</code></pre>
</div>
</div>
</section>
</section>
<section id="logistic-regression" class="level1">
<h1>4.2 Logistic Regression</h1>
<p>Logistic regression is used to model the probability of a binary or categorical outcome based on one or more predictor variables. It is a type of generalized linear model.</p>
<section id="binary-logistic-regression" class="level2">
<h2 class="anchored" data-anchor-id="binary-logistic-regression">4.2.1 Binary Logistic Regression</h2>
<p>Binary logistic regression models the probability of a binary outcome (e.g., success/failure) as a function of one or more predictor variables.</p>
<ul>
<li><p><strong>Example:</strong> Predicting whether a customer will purchase a product (yes/no) based on age and income.</p></li>
<li><p><strong>Formula:</strong> <span class="math display">\[
\log \left( \frac{p}{1-p} \right) = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \cdots + \beta_p x_p
\]</span> where <span class="math inline">\(p\)</span> is the probability of the event occurring.</p></li>
<li><p><strong>Steps:</strong></p>
<ol type="1">
<li><p><strong>Estimate the parameters:</strong> Use maximum likelihood estimation to estimate <span class="math inline">\(\beta_0, \beta_1, \ldots, \beta_p\)</span>.</p></li>
<li><p><strong>Make predictions:</strong> Calculate the probability of the outcome using the logistic function.</p>
<p>Formula: <span class="math display">\[
p = \frac{1}{1 + e^{-(\beta_0 + \beta_1 x_1 + \beta_2 x_2 + \cdots + \beta_p x_p)}}
\]</span></p></li>
</ol></li>
</ul>
<div id="136b409f" class="cell" data-execution_count="3">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LogisticRegression</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> accuracy_score, confusion_matrix, classification_report</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate synthetic data: Predicting product purchase based on age and income</span></span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>age <span class="op">=</span> np.random.randint(<span class="dv">18</span>, <span class="dv">70</span>, size<span class="op">=</span>(<span class="dv">100</span>, <span class="dv">1</span>))</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>income <span class="op">=</span> np.random.randint(<span class="dv">20000</span>, <span class="dv">100000</span>, size<span class="op">=</span>(<span class="dv">100</span>, <span class="dv">1</span>))</span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>purchase <span class="op">=</span> (<span class="fl">0.3</span> <span class="op">*</span> age <span class="op">+</span> <span class="fl">0.00002</span> <span class="op">*</span> income <span class="op">+</span> np.random.randn(<span class="dv">100</span>, <span class="dv">1</span>)).flatten()</span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a>purchase <span class="op">=</span> (purchase <span class="op">&gt;</span> purchase.mean()).astype(<span class="bu">int</span>)</span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert to pandas DataFrame</span></span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> pd.DataFrame({<span class="st">'Age'</span>: age.flatten(), <span class="st">'Income'</span>: income.flatten(), <span class="st">'Purchase'</span>: purchase})</span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Feature matrix and target vector</span></span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> data[[<span class="st">'Age'</span>, <span class="st">'Income'</span>]]</span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> data[<span class="st">'Purchase'</span>]</span>
<span id="cb7-22"><a href="#cb7-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-23"><a href="#cb7-23" aria-hidden="true" tabindex="-1"></a><span class="co"># Split the data into training and testing sets</span></span>
<span id="cb7-24"><a href="#cb7-24" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, y, test_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb7-25"><a href="#cb7-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-26"><a href="#cb7-26" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize and train the logistic regression model</span></span>
<span id="cb7-27"><a href="#cb7-27" aria-hidden="true" tabindex="-1"></a>logistic_model <span class="op">=</span> LogisticRegression()</span>
<span id="cb7-28"><a href="#cb7-28" aria-hidden="true" tabindex="-1"></a>logistic_model.fit(X_train, y_train)</span>
<span id="cb7-29"><a href="#cb7-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-30"><a href="#cb7-30" aria-hidden="true" tabindex="-1"></a><span class="co"># Make predictions</span></span>
<span id="cb7-31"><a href="#cb7-31" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> logistic_model.predict(X_test)</span>
<span id="cb7-32"><a href="#cb7-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-33"><a href="#cb7-33" aria-hidden="true" tabindex="-1"></a><span class="co"># Evaluate the model</span></span>
<span id="cb7-34"><a href="#cb7-34" aria-hidden="true" tabindex="-1"></a>accuracy <span class="op">=</span> accuracy_score(y_test, y_pred)</span>
<span id="cb7-35"><a href="#cb7-35" aria-hidden="true" tabindex="-1"></a>conf_matrix <span class="op">=</span> confusion_matrix(y_test, y_pred)</span>
<span id="cb7-36"><a href="#cb7-36" aria-hidden="true" tabindex="-1"></a>class_report <span class="op">=</span> classification_report(y_test, y_pred)</span>
<span id="cb7-37"><a href="#cb7-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-38"><a href="#cb7-38" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'Accuracy: </span><span class="sc">{</span>accuracy<span class="sc">:.2f}</span><span class="ss">'</span>)</span>
<span id="cb7-39"><a href="#cb7-39" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Confusion Matrix:'</span>)</span>
<span id="cb7-40"><a href="#cb7-40" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(conf_matrix)</span>
<span id="cb7-41"><a href="#cb7-41" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Classification Report:'</span>)</span>
<span id="cb7-42"><a href="#cb7-42" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(class_report)</span>
<span id="cb7-43"><a href="#cb7-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-44"><a href="#cb7-44" aria-hidden="true" tabindex="-1"></a><span class="co"># Plotting decision boundary</span></span>
<span id="cb7-45"><a href="#cb7-45" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">6</span>))</span>
<span id="cb7-46"><a href="#cb7-46" aria-hidden="true" tabindex="-1"></a>sns.scatterplot(data<span class="op">=</span>data, x<span class="op">=</span><span class="st">'Age'</span>, y<span class="op">=</span><span class="st">'Income'</span>, hue<span class="op">=</span><span class="st">'Purchase'</span>, palette<span class="op">=</span><span class="st">'coolwarm'</span>)</span>
<span id="cb7-47"><a href="#cb7-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-48"><a href="#cb7-48" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a mesh grid for decision boundary</span></span>
<span id="cb7-49"><a href="#cb7-49" aria-hidden="true" tabindex="-1"></a>age_min, age_max <span class="op">=</span> X[<span class="st">'Age'</span>].<span class="bu">min</span>() <span class="op">-</span> <span class="dv">1</span>, X[<span class="st">'Age'</span>].<span class="bu">max</span>() <span class="op">+</span> <span class="dv">1</span></span>
<span id="cb7-50"><a href="#cb7-50" aria-hidden="true" tabindex="-1"></a>income_min, income_max <span class="op">=</span> X[<span class="st">'Income'</span>].<span class="bu">min</span>() <span class="op">-</span> <span class="dv">1000</span>, X[<span class="st">'Income'</span>].<span class="bu">max</span>() <span class="op">+</span> <span class="dv">1000</span></span>
<span id="cb7-51"><a href="#cb7-51" aria-hidden="true" tabindex="-1"></a>xx, yy <span class="op">=</span> np.meshgrid(np.arange(age_min, age_max, <span class="fl">0.5</span>), np.arange(income_min, income_max, <span class="dv">1000</span>))</span>
<span id="cb7-52"><a href="#cb7-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-53"><a href="#cb7-53" aria-hidden="true" tabindex="-1"></a>Z <span class="op">=</span> logistic_model.predict(np.c_[xx.ravel(), yy.ravel()])</span>
<span id="cb7-54"><a href="#cb7-54" aria-hidden="true" tabindex="-1"></a>Z <span class="op">=</span> Z.reshape(xx.shape)</span>
<span id="cb7-55"><a href="#cb7-55" aria-hidden="true" tabindex="-1"></a>plt.contourf(xx, yy, Z, alpha<span class="op">=</span><span class="fl">0.2</span>, cmap<span class="op">=</span><span class="st">'coolwarm'</span>)</span>
<span id="cb7-56"><a href="#cb7-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-57"><a href="#cb7-57" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Age'</span>)</span>
<span id="cb7-58"><a href="#cb7-58" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Income'</span>)</span>
<span id="cb7-59"><a href="#cb7-59" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Binary Logistic Regression: Purchase Prediction'</span>)</span>
<span id="cb7-60"><a href="#cb7-60" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb7-61"><a href="#cb7-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-62"><a href="#cb7-62" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the model's parameters</span></span>
<span id="cb7-63"><a href="#cb7-63" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'Intercept: </span><span class="sc">{</span>logistic_model<span class="sc">.</span>intercept_[<span class="dv">0</span>]<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb7-64"><a href="#cb7-64" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'Coefficients: </span><span class="sc">{</span>logistic_model<span class="sc">.</span>coef_[<span class="dv">0</span>]<span class="sc">}</span><span class="ss">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Accuracy: 0.75
Confusion Matrix:
[[7 2]
 [3 8]]
Classification Report:
              precision    recall  f1-score   support

           0       0.70      0.78      0.74         9
           1       0.80      0.73      0.76        11

    accuracy                           0.75        20
   macro avg       0.75      0.75      0.75        20
weighted avg       0.76      0.75      0.75        20
</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>/Users/ravishankar/miniforge3/lib/python3.10/site-packages/sklearn/base.py:439: UserWarning:

X does not have valid feature names, but LogisticRegression was fitted with feature names
</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="chapter4_basic_supervised_learning_algorithms_files/figure-html/cell-4-output-3.png" width="842" height="523" class="figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Intercept: -0.00020133720147096868
Coefficients: [ 6.23708757e-02 -4.22361215e-05]</code></pre>
</div>
</div>
</section>
<section id="multinomial-logistic-regression" class="level2">
<h2 class="anchored" data-anchor-id="multinomial-logistic-regression">4.2.2 Multinomial Logistic Regression</h2>
<p>Multinomial logistic regression models the probability of multiple categorical outcomes (more than two) based on one or more predictor variables.</p>
<ul>
<li><p><strong>Example:</strong> Predicting the type of vehicle (car, truck, bike) a person will buy based on their age, income, and city.</p></li>
<li><p><strong>Formula:</strong> <span class="math display">\[
\log \left( \frac{p_k}{p_0} \right) = \beta_{0k} + \beta_{1k} x_1 + \beta_{2k} x_2 + \cdots + \beta_{pk} x_p
\]</span> where <span class="math inline">\(p_k\)</span> is the probability of the k-th category and <span class="math inline">\(p_0\)</span> is the probability of the reference category.</p></li>
<li><p><strong>Steps:</strong></p>
<ol type="1">
<li><p><strong>Estimate the parameters:</strong> Use maximum likelihood estimation to estimate <span class="math inline">\(\beta_{0k}, \beta_{1k}, \ldots, \beta_{pk}\)</span> for each category.</p></li>
<li><p><strong>Make predictions:</strong> Calculate the probability of each category using the logistic function.</p></li>
</ol></li>
</ul>
<div id="91826bc8" class="cell" data-execution_count="4">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LogisticRegression</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> accuracy_score, confusion_matrix, classification_report</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate synthetic data: Predicting vehicle type based on age, income, and city</span></span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a>age <span class="op">=</span> np.random.randint(<span class="dv">18</span>, <span class="dv">70</span>, size<span class="op">=</span>(<span class="dv">200</span>, <span class="dv">1</span>))</span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a>income <span class="op">=</span> np.random.randint(<span class="dv">20000</span>, <span class="dv">100000</span>, size<span class="op">=</span>(<span class="dv">200</span>, <span class="dv">1</span>))</span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a>city <span class="op">=</span> np.random.choice([<span class="st">'CityA'</span>, <span class="st">'CityB'</span>, <span class="st">'CityC'</span>], size<span class="op">=</span>(<span class="dv">200</span>, <span class="dv">1</span>))</span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Vehicle type: 0 - Car, 1 - Truck, 2 - Bike</span></span>
<span id="cb11-16"><a href="#cb11-16" aria-hidden="true" tabindex="-1"></a>vehicle_type <span class="op">=</span> (<span class="fl">0.3</span> <span class="op">*</span> age <span class="op">+</span> <span class="fl">0.00002</span> <span class="op">*</span> income <span class="op">+</span> np.random.randn(<span class="dv">200</span>, <span class="dv">1</span>)).flatten()</span>
<span id="cb11-17"><a href="#cb11-17" aria-hidden="true" tabindex="-1"></a>vehicle_type <span class="op">=</span> np.where(vehicle_type <span class="op">&gt;</span> np.percentile(vehicle_type, <span class="dv">67</span>), <span class="dv">2</span>, np.where(vehicle_type <span class="op">&gt;</span> np.percentile(vehicle_type, <span class="dv">33</span>), <span class="dv">1</span>, <span class="dv">0</span>))</span>
<span id="cb11-18"><a href="#cb11-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-19"><a href="#cb11-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert to pandas DataFrame</span></span>
<span id="cb11-20"><a href="#cb11-20" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> pd.DataFrame({<span class="st">'Age'</span>: age.flatten(), <span class="st">'Income'</span>: income.flatten(), <span class="st">'City'</span>: city.flatten(), <span class="st">'VehicleType'</span>: vehicle_type})</span>
<span id="cb11-21"><a href="#cb11-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-22"><a href="#cb11-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert categorical variable 'City' to dummy variables</span></span>
<span id="cb11-23"><a href="#cb11-23" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> pd.get_dummies(data, columns<span class="op">=</span>[<span class="st">'City'</span>], drop_first<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb11-24"><a href="#cb11-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-25"><a href="#cb11-25" aria-hidden="true" tabindex="-1"></a><span class="co"># Feature matrix and target vector</span></span>
<span id="cb11-26"><a href="#cb11-26" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> data[[<span class="st">'Age'</span>, <span class="st">'Income'</span>, <span class="st">'City_CityB'</span>, <span class="st">'City_CityC'</span>]]</span>
<span id="cb11-27"><a href="#cb11-27" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> data[<span class="st">'VehicleType'</span>]</span>
<span id="cb11-28"><a href="#cb11-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-29"><a href="#cb11-29" aria-hidden="true" tabindex="-1"></a><span class="co"># Split the data into training and testing sets</span></span>
<span id="cb11-30"><a href="#cb11-30" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, y, test_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb11-31"><a href="#cb11-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-32"><a href="#cb11-32" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize and train the multinomial logistic regression model</span></span>
<span id="cb11-33"><a href="#cb11-33" aria-hidden="true" tabindex="-1"></a>logistic_model <span class="op">=</span> LogisticRegression(multi_class<span class="op">=</span><span class="st">'multinomial'</span>, solver<span class="op">=</span><span class="st">'lbfgs'</span>, max_iter<span class="op">=</span><span class="dv">1000</span>)</span>
<span id="cb11-34"><a href="#cb11-34" aria-hidden="true" tabindex="-1"></a>logistic_model.fit(X_train, y_train)</span>
<span id="cb11-35"><a href="#cb11-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-36"><a href="#cb11-36" aria-hidden="true" tabindex="-1"></a><span class="co"># Make predictions</span></span>
<span id="cb11-37"><a href="#cb11-37" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> logistic_model.predict(X_test)</span>
<span id="cb11-38"><a href="#cb11-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-39"><a href="#cb11-39" aria-hidden="true" tabindex="-1"></a><span class="co"># Evaluate the model</span></span>
<span id="cb11-40"><a href="#cb11-40" aria-hidden="true" tabindex="-1"></a>accuracy <span class="op">=</span> accuracy_score(y_test, y_pred)</span>
<span id="cb11-41"><a href="#cb11-41" aria-hidden="true" tabindex="-1"></a>conf_matrix <span class="op">=</span> confusion_matrix(y_test, y_pred)</span>
<span id="cb11-42"><a href="#cb11-42" aria-hidden="true" tabindex="-1"></a>class_report <span class="op">=</span> classification_report(y_test, y_pred)</span>
<span id="cb11-43"><a href="#cb11-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-44"><a href="#cb11-44" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'Accuracy: </span><span class="sc">{</span>accuracy<span class="sc">:.2f}</span><span class="ss">'</span>)</span>
<span id="cb11-45"><a href="#cb11-45" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Confusion Matrix:'</span>)</span>
<span id="cb11-46"><a href="#cb11-46" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(conf_matrix)</span>
<span id="cb11-47"><a href="#cb11-47" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Classification Report:'</span>)</span>
<span id="cb11-48"><a href="#cb11-48" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(class_report)</span>
<span id="cb11-49"><a href="#cb11-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-50"><a href="#cb11-50" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the model's parameters</span></span>
<span id="cb11-51"><a href="#cb11-51" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'Intercepts: </span><span class="sc">{</span>logistic_model<span class="sc">.</span>intercept_<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb11-52"><a href="#cb11-52" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'Coefficients: </span><span class="sc">{</span>logistic_model<span class="sc">.</span>coef_<span class="sc">}</span><span class="ss">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Accuracy: 0.57
Confusion Matrix:
[[9 1 3]
 [4 6 9]
 [0 0 8]]
Classification Report:
              precision    recall  f1-score   support

           0       0.69      0.69      0.69        13
           1       0.86      0.32      0.46        19
           2       0.40      1.00      0.57         8

    accuracy                           0.57        40
   macro avg       0.65      0.67      0.58        40
weighted avg       0.71      0.57      0.56        40

Intercepts: [ 8.97948195e-04  1.00509902e-05 -9.07999186e-04]
Coefficients: [[-5.79159626e-02  3.31799508e-05  2.32288011e-04 -4.79284122e-05]
 [ 7.68122008e-03 -1.77792564e-06 -2.50402229e-04  1.39359472e-04]
 [ 5.02347426e-02 -3.14020252e-05  1.81142184e-05 -9.14310598e-05]]</code></pre>
</div>
</div>
</section>
<section id="ordinal-logistic-regression" class="level2">
<h2 class="anchored" data-anchor-id="ordinal-logistic-regression">4.2.3 Ordinal Logistic Regression</h2>
<p>Ordinal logistic regression models the probability of an ordinal outcome (ordered categories) based on one or more predictor variables.</p>
<ul>
<li><p><strong>Example:</strong> Predicting the satisfaction level (very satisfied, satisfied, neutral, dissatisfied, very dissatisfied) of a customer based on service quality and response time.</p></li>
<li><p><strong>Formula:</strong> <span class="math display">\[
\log \left( \frac{P(Y \leq j)}{P(Y &gt; j)} \right) = \theta_j - (\beta_1 x_1 + \beta_2 x_2 + \cdots + \beta_p x_p)
\]</span> where <span class="math inline">\(P(Y \leq j)\)</span> is the probability of being in category <span class="math inline">\(j\)</span> or lower, and <span class="math inline">\(\theta_j\)</span> is the threshold parameter for category <span class="math inline">\(j\)</span>.</p></li>
<li><p><strong>Steps:</strong></p>
<ol type="1">
<li><p><strong>Estimate the parameters:</strong> Use maximum likelihood estimation to estimate <span class="math inline">\(\theta_j\)</span> and <span class="math inline">\(\beta_1, \beta_2, \ldots, \beta_p\)</span>.</p></li>
<li><p><strong>Make predictions:</strong> Calculate the probability of each category using the logistic function.</p></li>
</ol></li>
</ul>
<div id="2b0a868d" class="cell" data-execution_count="5">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> StandardScaler</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> statsmodels.api <span class="im">as</span> sm</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> statsmodels.miscmodels.ordinal_model <span class="im">import</span> OrderedModel</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> accuracy_score, confusion_matrix, classification_report</span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate synthetic data: Predicting customer satisfaction based on service quality and response time</span></span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a>service_quality <span class="op">=</span> np.random.randint(<span class="dv">1</span>, <span class="dv">6</span>, size<span class="op">=</span>(<span class="dv">200</span>, <span class="dv">1</span>))  <span class="co"># Scale from 1 to 5</span></span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a>response_time <span class="op">=</span> np.random.randint(<span class="dv">1</span>, <span class="dv">60</span>, size<span class="op">=</span>(<span class="dv">200</span>, <span class="dv">1</span>))  <span class="co"># Response time in minutes</span></span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a>satisfaction <span class="op">=</span> (<span class="fl">0.5</span> <span class="op">*</span> service_quality <span class="op">-</span> <span class="fl">0.1</span> <span class="op">*</span> response_time <span class="op">+</span> np.random.randn(<span class="dv">200</span>, <span class="dv">1</span>)).flatten()</span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a>satisfaction <span class="op">=</span> np.where(satisfaction <span class="op">&gt;</span> np.percentile(satisfaction, <span class="dv">80</span>), <span class="dv">4</span>,</span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a>                        np.where(satisfaction <span class="op">&gt;</span> np.percentile(satisfaction, <span class="dv">60</span>), <span class="dv">3</span>,</span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a>                                 np.where(satisfaction <span class="op">&gt;</span> np.percentile(satisfaction, <span class="dv">40</span>), <span class="dv">2</span>,</span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"></a>                                          np.where(satisfaction <span class="op">&gt;</span> np.percentile(satisfaction, <span class="dv">20</span>), <span class="dv">1</span>, <span class="dv">0</span>))))</span>
<span id="cb13-18"><a href="#cb13-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-19"><a href="#cb13-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert to pandas DataFrame</span></span>
<span id="cb13-20"><a href="#cb13-20" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> pd.DataFrame({<span class="st">'ServiceQuality'</span>: service_quality.flatten(), <span class="st">'ResponseTime'</span>: response_time.flatten(), <span class="st">'Satisfaction'</span>: satisfaction})</span>
<span id="cb13-21"><a href="#cb13-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-22"><a href="#cb13-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Feature matrix and target vector</span></span>
<span id="cb13-23"><a href="#cb13-23" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> data[[<span class="st">'ServiceQuality'</span>, <span class="st">'ResponseTime'</span>]]</span>
<span id="cb13-24"><a href="#cb13-24" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> data[<span class="st">'Satisfaction'</span>]</span>
<span id="cb13-25"><a href="#cb13-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-26"><a href="#cb13-26" aria-hidden="true" tabindex="-1"></a><span class="co"># Standardize the features</span></span>
<span id="cb13-27"><a href="#cb13-27" aria-hidden="true" tabindex="-1"></a>scaler <span class="op">=</span> StandardScaler()</span>
<span id="cb13-28"><a href="#cb13-28" aria-hidden="true" tabindex="-1"></a>X_scaled <span class="op">=</span> scaler.fit_transform(X)</span>
<span id="cb13-29"><a href="#cb13-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-30"><a href="#cb13-30" aria-hidden="true" tabindex="-1"></a><span class="co"># Split the data into training and testing sets</span></span>
<span id="cb13-31"><a href="#cb13-31" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X_scaled, y, test_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb13-32"><a href="#cb13-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-33"><a href="#cb13-33" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize and train the ordinal logistic regression model</span></span>
<span id="cb13-34"><a href="#cb13-34" aria-hidden="true" tabindex="-1"></a>ordinal_model <span class="op">=</span> OrderedModel(y_train, X_train, distr<span class="op">=</span><span class="st">'logit'</span>)</span>
<span id="cb13-35"><a href="#cb13-35" aria-hidden="true" tabindex="-1"></a>ordinal_results <span class="op">=</span> ordinal_model.fit(method<span class="op">=</span><span class="st">'bfgs'</span>)</span>
<span id="cb13-36"><a href="#cb13-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-37"><a href="#cb13-37" aria-hidden="true" tabindex="-1"></a><span class="co"># Make predictions</span></span>
<span id="cb13-38"><a href="#cb13-38" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> ordinal_results.model.predict(ordinal_results.params, exog<span class="op">=</span>X_test).argmax(axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb13-39"><a href="#cb13-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-40"><a href="#cb13-40" aria-hidden="true" tabindex="-1"></a><span class="co"># Evaluate the model</span></span>
<span id="cb13-41"><a href="#cb13-41" aria-hidden="true" tabindex="-1"></a>accuracy <span class="op">=</span> accuracy_score(y_test, y_pred)</span>
<span id="cb13-42"><a href="#cb13-42" aria-hidden="true" tabindex="-1"></a>conf_matrix <span class="op">=</span> confusion_matrix(y_test, y_pred)</span>
<span id="cb13-43"><a href="#cb13-43" aria-hidden="true" tabindex="-1"></a>class_report <span class="op">=</span> classification_report(y_test, y_pred)</span>
<span id="cb13-44"><a href="#cb13-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-45"><a href="#cb13-45" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'Accuracy: </span><span class="sc">{</span>accuracy<span class="sc">:.2f}</span><span class="ss">'</span>)</span>
<span id="cb13-46"><a href="#cb13-46" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Confusion Matrix:'</span>)</span>
<span id="cb13-47"><a href="#cb13-47" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(conf_matrix)</span>
<span id="cb13-48"><a href="#cb13-48" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Classification Report:'</span>)</span>
<span id="cb13-49"><a href="#cb13-49" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(class_report)</span>
<span id="cb13-50"><a href="#cb13-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-51"><a href="#cb13-51" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the model's parameters</span></span>
<span id="cb13-52"><a href="#cb13-52" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Model Parameters:'</span>)</span>
<span id="cb13-53"><a href="#cb13-53" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(ordinal_results.params)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Optimization terminated successfully.
         Current function value: 1.086932
         Iterations: 18
         Function evaluations: 19
         Gradient evaluations: 19
Accuracy: 0.57
Confusion Matrix:
[[6 2 0 0 0]
 [5 3 4 0 0]
 [0 0 7 1 0]
 [0 0 1 4 0]
 [0 0 1 3 3]]
Classification Report:
              precision    recall  f1-score   support

           0       0.55      0.75      0.63         8
           1       0.60      0.25      0.35        12
           2       0.54      0.88      0.67         8
           3       0.50      0.80      0.62         5
           4       1.00      0.43      0.60         7

    accuracy                           0.57        40
   macro avg       0.64      0.62      0.57        40
weighted avg       0.63      0.57      0.55        40

Model Parameters:
x1     0.880499
x2    -2.447663
0/1   -2.755626
1/2    0.568617
2/3    0.609612
3/4    0.707513
dtype: float64</code></pre>
</div>
</div>
</section>
<section id="maximum-likelihood-estimation" class="level2">
<h2 class="anchored" data-anchor-id="maximum-likelihood-estimation">4.2.4 Maximum Likelihood Estimation</h2>
<p>Maximum likelihood estimation (MLE) is a method used to estimate the parameters of a statistical model. It finds the parameter values that maximize the likelihood of observing the given data.</p>
<ul>
<li><p><strong>Example:</strong> Estimating the parameters of a logistic regression model predicting whether a patient has a disease based on their symptoms.</p></li>
<li><p><strong>Steps:</strong></p>
<ol type="1">
<li><p><strong>Define the likelihood function:</strong> The likelihood function is the probability of the observed data as a function of the parameters.</p></li>
<li><p><strong>Compute the log-likelihood:</strong> Taking the logarithm of the likelihood function simplifies the calculations and turns the product into a sum.</p>
<p>Formula: <span class="math display">\[
\log L(\beta) = \sum_{i=1}^{n} \left( y_i \log p_i + (1 - y_i) \log (1 - p_i) \right)
\]</span></p></li>
<li><p><strong>Differentiate the log-likelihood:</strong> Find the partial derivatives of the log-likelihood with respect to each parameter.</p></li>
<li><p><strong>Set the derivatives to zero and solve:</strong> Solve the resulting system of equations to find the parameter estimates.</p></li>
<li><p><strong>Iterate if necessary:</strong> Use iterative methods like Newton-Raphson or gradient descent if a closed-form solution is not available.</p></li>
</ol></li>
</ul>
<p>Advanced considerations in linear and logistic regression include:</p>
<ul>
<li><p><strong>Model Diagnostics:</strong> Assess model assumptions and fit using residual plots, goodness-of-fit tests, and other diagnostic tools.</p></li>
<li><p><strong>Regularization:</strong> Apply techniques like Lasso and Ridge regression to prevent overfitting and handle multicollinearity.</p></li>
<li><p><strong>Feature Engineering:</strong> Create interaction terms, polynomial features, and other transformations to capture complex relationships in the data.</p></li>
<li><p><strong>Validation:</strong> Use cross-validation and bootstrapping to ensure the model generalizes well to unseen data.</p></li>
<li><p><strong>Interpretability:</strong> Understand the coefficients and their implications, and use tools like SHAP values to explain model predictions.</p></li>
</ul>
<div id="455bde1d" class="cell" data-execution_count="6">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.optimize <span class="im">import</span> minimize</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> StandardScaler</span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> accuracy_score, confusion_matrix, classification_report</span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate synthetic data: Predicting disease presence based on symptoms</span></span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a>symptom1 <span class="op">=</span> np.random.rand(<span class="dv">200</span>, <span class="dv">1</span>)  <span class="co"># Symptom 1 (continuous)</span></span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a>symptom2 <span class="op">=</span> np.random.rand(<span class="dv">200</span>, <span class="dv">1</span>)  <span class="co"># Symptom 2 (continuous)</span></span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a>disease <span class="op">=</span> (<span class="fl">1.5</span> <span class="op">*</span> symptom1 <span class="op">+</span> <span class="fl">2.0</span> <span class="op">*</span> symptom2 <span class="op">+</span> np.random.randn(<span class="dv">200</span>, <span class="dv">1</span>)).flatten()</span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a>disease <span class="op">=</span> (disease <span class="op">&gt;</span> np.percentile(disease, <span class="dv">50</span>)).astype(<span class="bu">int</span>)  <span class="co"># Binary outcome (0 or 1)</span></span>
<span id="cb15-14"><a href="#cb15-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-15"><a href="#cb15-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert to pandas DataFrame</span></span>
<span id="cb15-16"><a href="#cb15-16" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> pd.DataFrame({<span class="st">'Symptom1'</span>: symptom1.flatten(), <span class="st">'Symptom2'</span>: symptom2.flatten(), <span class="st">'Disease'</span>: disease})</span>
<span id="cb15-17"><a href="#cb15-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-18"><a href="#cb15-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Feature matrix and target vector</span></span>
<span id="cb15-19"><a href="#cb15-19" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> data[[<span class="st">'Symptom1'</span>, <span class="st">'Symptom2'</span>]]</span>
<span id="cb15-20"><a href="#cb15-20" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> data[<span class="st">'Disease'</span>]</span>
<span id="cb15-21"><a href="#cb15-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-22"><a href="#cb15-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Standardize the features</span></span>
<span id="cb15-23"><a href="#cb15-23" aria-hidden="true" tabindex="-1"></a>scaler <span class="op">=</span> StandardScaler()</span>
<span id="cb15-24"><a href="#cb15-24" aria-hidden="true" tabindex="-1"></a>X_scaled <span class="op">=</span> scaler.fit_transform(X)</span>
<span id="cb15-25"><a href="#cb15-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-26"><a href="#cb15-26" aria-hidden="true" tabindex="-1"></a><span class="co"># Add intercept term to feature matrix</span></span>
<span id="cb15-27"><a href="#cb15-27" aria-hidden="true" tabindex="-1"></a>X_scaled <span class="op">=</span> np.c_[np.ones(X_scaled.shape[<span class="dv">0</span>]), X_scaled]</span>
<span id="cb15-28"><a href="#cb15-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-29"><a href="#cb15-29" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the logistic function</span></span>
<span id="cb15-30"><a href="#cb15-30" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> sigmoid(z):</span>
<span id="cb15-31"><a href="#cb15-31" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="dv">1</span> <span class="op">/</span> (<span class="dv">1</span> <span class="op">+</span> np.exp(<span class="op">-</span>z))</span>
<span id="cb15-32"><a href="#cb15-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-33"><a href="#cb15-33" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the log-likelihood function for logistic regression</span></span>
<span id="cb15-34"><a href="#cb15-34" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> log_likelihood(beta, X, y):</span>
<span id="cb15-35"><a href="#cb15-35" aria-hidden="true" tabindex="-1"></a>    z <span class="op">=</span> np.dot(X, beta)</span>
<span id="cb15-36"><a href="#cb15-36" aria-hidden="true" tabindex="-1"></a>    log_l <span class="op">=</span> np.<span class="bu">sum</span>(y <span class="op">*</span> z <span class="op">-</span> np.log(<span class="dv">1</span> <span class="op">+</span> np.exp(z)))</span>
<span id="cb15-37"><a href="#cb15-37" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="op">-</span>log_l  <span class="co"># Return negative log-likelihood for minimization</span></span>
<span id="cb15-38"><a href="#cb15-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-39"><a href="#cb15-39" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize parameters</span></span>
<span id="cb15-40"><a href="#cb15-40" aria-hidden="true" tabindex="-1"></a>initial_beta <span class="op">=</span> np.zeros(X_scaled.shape[<span class="dv">1</span>])</span>
<span id="cb15-41"><a href="#cb15-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-42"><a href="#cb15-42" aria-hidden="true" tabindex="-1"></a><span class="co"># Estimate the parameters using MLE</span></span>
<span id="cb15-43"><a href="#cb15-43" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> minimize(log_likelihood, initial_beta, args<span class="op">=</span>(X_scaled, y), method<span class="op">=</span><span class="st">'BFGS'</span>)</span>
<span id="cb15-44"><a href="#cb15-44" aria-hidden="true" tabindex="-1"></a>beta_hat <span class="op">=</span> result.x</span>
<span id="cb15-45"><a href="#cb15-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-46"><a href="#cb15-46" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the estimated parameters</span></span>
<span id="cb15-47"><a href="#cb15-47" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Estimated parameters (beta):'</span>, beta_hat)</span>
<span id="cb15-48"><a href="#cb15-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-49"><a href="#cb15-49" aria-hidden="true" tabindex="-1"></a><span class="co"># Make predictions using the estimated parameters</span></span>
<span id="cb15-50"><a href="#cb15-50" aria-hidden="true" tabindex="-1"></a>z <span class="op">=</span> np.dot(X_scaled, beta_hat)</span>
<span id="cb15-51"><a href="#cb15-51" aria-hidden="true" tabindex="-1"></a>y_pred_prob <span class="op">=</span> sigmoid(z)</span>
<span id="cb15-52"><a href="#cb15-52" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> (y_pred_prob <span class="op">&gt;=</span> <span class="fl">0.5</span>).astype(<span class="bu">int</span>)</span>
<span id="cb15-53"><a href="#cb15-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-54"><a href="#cb15-54" aria-hidden="true" tabindex="-1"></a><span class="co"># Evaluate the model</span></span>
<span id="cb15-55"><a href="#cb15-55" aria-hidden="true" tabindex="-1"></a>accuracy <span class="op">=</span> accuracy_score(y, y_pred)</span>
<span id="cb15-56"><a href="#cb15-56" aria-hidden="true" tabindex="-1"></a>conf_matrix <span class="op">=</span> confusion_matrix(y, y_pred)</span>
<span id="cb15-57"><a href="#cb15-57" aria-hidden="true" tabindex="-1"></a>class_report <span class="op">=</span> classification_report(y, y_pred)</span>
<span id="cb15-58"><a href="#cb15-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-59"><a href="#cb15-59" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'Accuracy: </span><span class="sc">{</span>accuracy<span class="sc">:.2f}</span><span class="ss">'</span>)</span>
<span id="cb15-60"><a href="#cb15-60" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Confusion Matrix:'</span>)</span>
<span id="cb15-61"><a href="#cb15-61" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(conf_matrix)</span>
<span id="cb15-62"><a href="#cb15-62" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Classification Report:'</span>)</span>
<span id="cb15-63"><a href="#cb15-63" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(class_report)</span>
<span id="cb15-64"><a href="#cb15-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-65"><a href="#cb15-65" aria-hidden="true" tabindex="-1"></a><span class="co"># Model Diagnostics: Residual Plot</span></span>
<span id="cb15-66"><a href="#cb15-66" aria-hidden="true" tabindex="-1"></a>residuals <span class="op">=</span> y <span class="op">-</span> y_pred_prob</span>
<span id="cb15-67"><a href="#cb15-67" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">6</span>))</span>
<span id="cb15-68"><a href="#cb15-68" aria-hidden="true" tabindex="-1"></a>plt.scatter(y_pred_prob, residuals, alpha<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb15-69"><a href="#cb15-69" aria-hidden="true" tabindex="-1"></a>plt.axhline(y<span class="op">=</span><span class="dv">0</span>, color<span class="op">=</span><span class="st">'r'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>)</span>
<span id="cb15-70"><a href="#cb15-70" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Predicted Probability'</span>)</span>
<span id="cb15-71"><a href="#cb15-71" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Residuals'</span>)</span>
<span id="cb15-72"><a href="#cb15-72" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Residual Plot'</span>)</span>
<span id="cb15-73"><a href="#cb15-73" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Estimated parameters (beta): [-0.01385074  0.51100559  0.81264592]
Accuracy: 0.69
Confusion Matrix:
[[66 34]
 [29 71]]
Classification Report:
              precision    recall  f1-score   support

           0       0.69      0.66      0.68       100
           1       0.68      0.71      0.69       100

    accuracy                           0.69       200
   macro avg       0.69      0.69      0.68       200
weighted avg       0.69      0.69      0.68       200
</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="chapter4_basic_supervised_learning_algorithms_files/figure-html/cell-7-output-2.png" width="832" height="523" class="figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="k-nearest-neighbors-k-nn" class="level1">
<h1>4.3 k-Nearest Neighbors (k-NN)</h1>
<p>k-Nearest Neighbors (k-NN) is a non-parametric, lazy learning algorithm used for both classification and regression. It makes predictions based on the k training samples that are closest to a given query point.</p>
<section id="distance-metrics" class="level2">
<h2 class="anchored" data-anchor-id="distance-metrics">4.3.1 Distance Metrics</h2>
<p>Distance metrics are crucial in k-NN as they determine how the similarity between data points is measured. Different metrics can significantly impact the algorithm’s performance.</p>
<section id="euclidean-distance" class="level4">
<h4 class="anchored" data-anchor-id="euclidean-distance">4.3.1.1 Euclidean Distance</h4>
<p>Euclidean distance is the most common distance metric used in k-NN. It calculates the straight-line distance between two points in Euclidean space.</p>
<ul>
<li><p><strong>Formula:</strong> <span class="math display">\[
d(x, y) = \sqrt{\sum_{i=1}^{n} (x_i - y_i)^2}
\]</span> where <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> are two points in n-dimensional space.</p></li>
<li><p><strong>Example:</strong> Calculating the distance between two points (2, 3) and (5, 7).</p></li>
</ul>
</section>
<section id="manhattan-distance" class="level4">
<h4 class="anchored" data-anchor-id="manhattan-distance">4.3.1.2 Manhattan Distance</h4>
<p>Manhattan distance, also known as L1 distance or city block distance, calculates the distance between two points by summing the absolute differences of their coordinates.</p>
<ul>
<li><p><strong>Formula:</strong> <span class="math display">\[
d(x, y) = \sum_{i=1}^{n} |x_i - y_i|
\]</span></p></li>
<li><p><strong>Example:</strong> Calculating the distance between two points (2, 3) and (5, 7).</p></li>
</ul>
</section>
<section id="minkowski-distance" class="level4">
<h4 class="anchored" data-anchor-id="minkowski-distance">4.3.1.3 Minkowski Distance</h4>
<p>Minkowski distance is a generalized form of Euclidean and Manhattan distances. It introduces a parameter <span class="math inline">\(p\)</span> that determines the type of distance metric.</p>
<ul>
<li><p><strong>Formula:</strong> <span class="math display">\[
d(x, y) = \left( \sum_{i=1}^{n} |x_i - y_i|^p \right)^{\frac{1}{p}}
\]</span></p></li>
<li><p><strong>Example:</strong> When <span class="math inline">\(p = 2\)</span>, Minkowski distance is equivalent to Euclidean distance. When <span class="math inline">\(p = 1\)</span>, it is equivalent to Manhattan distance.</p></li>
</ul>
</section>
</section>
<section id="choosing-the-optimal-k" class="level2">
<h2 class="anchored" data-anchor-id="choosing-the-optimal-k">4.3.2 Choosing the Optimal k</h2>
<p>Choosing the optimal number of neighbors (k) is crucial for the performance of the k-NN algorithm. The optimal k can be found using methods like cross-validation.</p>
<ul>
<li><p><strong>Steps:</strong></p>
<ol type="1">
<li><p><strong>Initialize a range of k values:</strong> For example, k from 1 to 20.</p></li>
<li><p><strong>Perform cross-validation:</strong> Use k-fold cross-validation to evaluate the performance of the model for each value of k.</p></li>
<li><p><strong>Select the k with the best performance:</strong> Choose the k that results in the lowest cross-validation error.</p></li>
</ol></li>
<li><p><strong>Example:</strong> Using 5-fold cross-validation to find the optimal k for a dataset.</p></li>
</ul>
</section>
<section id="weighted-k-nn" class="level2">
<h2 class="anchored" data-anchor-id="weighted-k-nn">4.3.3 Weighted k-NN</h2>
<p>In weighted k-NN, closer neighbors have more influence on the prediction than distant ones. This can improve the performance of the algorithm, especially when there is a large variation in distances among the nearest neighbors.</p>
<ul>
<li><p><strong>Formula for weighted k-NN classification:</strong> <span class="math display">\[
\hat{y} = \frac{\sum_{i=1}^{k} w_i y_i}{\sum_{i=1}^{k} w_i}
\]</span> where <span class="math inline">\(w_i = \frac{1}{d(x, x_i)}\)</span> and <span class="math inline">\(y_i\)</span> is the class label of the i-th nearest neighbor.</p></li>
<li><p><strong>Steps:</strong></p>
<ol type="1">
<li><p><strong>Compute distances:</strong> Calculate the distances between the query point and all points in the training set.</p></li>
<li><p><strong>Assign weights:</strong> Assign a weight to each of the k nearest neighbors based on their distances.</p></li>
<li><p><strong>Make predictions:</strong> For classification, take a weighted vote of the neighbors. For regression, compute a weighted average.</p></li>
</ol></li>
<li><p><strong>Example:</strong> Using weighted k-NN to predict the price of a house based on its features.</p></li>
</ul>
</section>
<section id="k-nn-for-regression" class="level2">
<h2 class="anchored" data-anchor-id="k-nn-for-regression">4.3.4 k-NN for Regression</h2>
<p>k-NN can also be used for regression tasks, where the goal is to predict a continuous value rather than a class label.</p>
<ul>
<li><p><strong>Formula for k-NN regression:</strong> <span class="math display">\[
\hat{y} = \frac{1}{k} \sum_{i=1}^{k} y_i
\]</span> where <span class="math inline">\(\hat{y}\)</span> is the predicted value, and <span class="math inline">\(y_i\)</span> are the values of the k nearest neighbors.</p></li>
<li><p><strong>Steps:</strong></p>
<ol type="1">
<li><p><strong>Compute distances:</strong> Calculate the distances between the query point and all points in the training set.</p></li>
<li><p><strong>Select k nearest neighbors:</strong> Identify the k points with the smallest distances.</p></li>
<li><p><strong>Make predictions:</strong> Compute the average (or weighted average) of the target values of the k nearest neighbors.</p></li>
</ol></li>
<li><p><strong>Example:</strong> Using k-NN regression to predict the temperature based on historical weather data.</p></li>
</ul>
<p>Advanced considerations in k-NN include:</p>
<ul>
<li><p><strong>Scaling:</strong> Ensure features are on the same scale, as distance metrics are sensitive to the scale of the data.</p></li>
<li><p><strong>Dimensionality Reduction:</strong> Use techniques like PCA to reduce the dimensionality of the data, as high-dimensional spaces can lead to sparse data and reduce the effectiveness of k-NN.</p></li>
<li><p><strong>Efficiency:</strong> Implement efficient search algorithms like KD-trees or ball trees to speed up the nearest neighbor search, especially for large datasets.</p></li>
</ul>
<div id="0d4488d6" class="cell" data-execution_count="7">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split, cross_val_score</span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.neighbors <span class="im">import</span> KNeighborsClassifier, KNeighborsRegressor</span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> StandardScaler</span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> accuracy_score, mean_squared_error</span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate synthetic data for k-NN classification and regression</span></span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb17-12"><a href="#cb17-12" aria-hidden="true" tabindex="-1"></a>features <span class="op">=</span> np.random.rand(<span class="dv">200</span>, <span class="dv">2</span>)  <span class="co"># Two features</span></span>
<span id="cb17-13"><a href="#cb17-13" aria-hidden="true" tabindex="-1"></a>target_class <span class="op">=</span> (features[:, <span class="dv">0</span>] <span class="op">+</span> features[:, <span class="dv">1</span>] <span class="op">+</span> np.random.randn(<span class="dv">200</span>) <span class="op">*</span> <span class="fl">0.1</span> <span class="op">&gt;</span> <span class="dv">1</span>).astype(<span class="bu">int</span>)  <span class="co"># Binary classification target</span></span>
<span id="cb17-14"><a href="#cb17-14" aria-hidden="true" tabindex="-1"></a>target_reg <span class="op">=</span> features[:, <span class="dv">0</span>] <span class="op">+</span> features[:, <span class="dv">1</span>] <span class="op">+</span> np.random.randn(<span class="dv">200</span>) <span class="op">*</span> <span class="fl">0.1</span>  <span class="co"># Regression target</span></span>
<span id="cb17-15"><a href="#cb17-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-16"><a href="#cb17-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert to pandas DataFrame</span></span>
<span id="cb17-17"><a href="#cb17-17" aria-hidden="true" tabindex="-1"></a>data_class <span class="op">=</span> pd.DataFrame({<span class="st">'Feature1'</span>: features[:, <span class="dv">0</span>], <span class="st">'Feature2'</span>: features[:, <span class="dv">1</span>], <span class="st">'Target'</span>: target_class})</span>
<span id="cb17-18"><a href="#cb17-18" aria-hidden="true" tabindex="-1"></a>data_reg <span class="op">=</span> pd.DataFrame({<span class="st">'Feature1'</span>: features[:, <span class="dv">0</span>], <span class="st">'Feature2'</span>: features[:, <span class="dv">1</span>], <span class="st">'Target'</span>: target_reg})</span>
<span id="cb17-19"><a href="#cb17-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-20"><a href="#cb17-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Split the data into training and testing sets for classification</span></span>
<span id="cb17-21"><a href="#cb17-21" aria-hidden="true" tabindex="-1"></a>X_class <span class="op">=</span> data_class[[<span class="st">'Feature1'</span>, <span class="st">'Feature2'</span>]]</span>
<span id="cb17-22"><a href="#cb17-22" aria-hidden="true" tabindex="-1"></a>y_class <span class="op">=</span> data_class[<span class="st">'Target'</span>]</span>
<span id="cb17-23"><a href="#cb17-23" aria-hidden="true" tabindex="-1"></a>X_train_class, X_test_class, y_train_class, y_test_class <span class="op">=</span> train_test_split(X_class, y_class, test_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb17-24"><a href="#cb17-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-25"><a href="#cb17-25" aria-hidden="true" tabindex="-1"></a><span class="co"># Split the data into training and testing sets for regression</span></span>
<span id="cb17-26"><a href="#cb17-26" aria-hidden="true" tabindex="-1"></a>X_reg <span class="op">=</span> data_reg[[<span class="st">'Feature1'</span>, <span class="st">'Feature2'</span>]]</span>
<span id="cb17-27"><a href="#cb17-27" aria-hidden="true" tabindex="-1"></a>y_reg <span class="op">=</span> data_reg[<span class="st">'Target'</span>]</span>
<span id="cb17-28"><a href="#cb17-28" aria-hidden="true" tabindex="-1"></a>X_train_reg, X_test_reg, y_train_reg, y_test_reg <span class="op">=</span> train_test_split(X_reg, y_reg, test_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb17-29"><a href="#cb17-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-30"><a href="#cb17-30" aria-hidden="true" tabindex="-1"></a><span class="co"># Standardize the features</span></span>
<span id="cb17-31"><a href="#cb17-31" aria-hidden="true" tabindex="-1"></a>scaler_class <span class="op">=</span> StandardScaler()</span>
<span id="cb17-32"><a href="#cb17-32" aria-hidden="true" tabindex="-1"></a>X_train_class_scaled <span class="op">=</span> scaler_class.fit_transform(X_train_class)</span>
<span id="cb17-33"><a href="#cb17-33" aria-hidden="true" tabindex="-1"></a>X_test_class_scaled <span class="op">=</span> scaler_class.transform(X_test_class)</span>
<span id="cb17-34"><a href="#cb17-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-35"><a href="#cb17-35" aria-hidden="true" tabindex="-1"></a>scaler_reg <span class="op">=</span> StandardScaler()</span>
<span id="cb17-36"><a href="#cb17-36" aria-hidden="true" tabindex="-1"></a>X_train_reg_scaled <span class="op">=</span> scaler_reg.fit_transform(X_train_reg)</span>
<span id="cb17-37"><a href="#cb17-37" aria-hidden="true" tabindex="-1"></a>X_test_reg_scaled <span class="op">=</span> scaler_reg.transform(X_test_reg)</span>
<span id="cb17-38"><a href="#cb17-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-39"><a href="#cb17-39" aria-hidden="true" tabindex="-1"></a><span class="co"># 4.3.1.1 Euclidean Distance</span></span>
<span id="cb17-40"><a href="#cb17-40" aria-hidden="true" tabindex="-1"></a>point1 <span class="op">=</span> np.array([<span class="dv">2</span>, <span class="dv">3</span>])</span>
<span id="cb17-41"><a href="#cb17-41" aria-hidden="true" tabindex="-1"></a>point2 <span class="op">=</span> np.array([<span class="dv">5</span>, <span class="dv">7</span>])</span>
<span id="cb17-42"><a href="#cb17-42" aria-hidden="true" tabindex="-1"></a>euclidean_distance <span class="op">=</span> np.sqrt(np.<span class="bu">sum</span>((point1 <span class="op">-</span> point2) <span class="op">**</span> <span class="dv">2</span>))</span>
<span id="cb17-43"><a href="#cb17-43" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'Euclidean Distance between </span><span class="sc">{</span>point1<span class="sc">}</span><span class="ss"> and </span><span class="sc">{</span>point2<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>euclidean_distance<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb17-44"><a href="#cb17-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-45"><a href="#cb17-45" aria-hidden="true" tabindex="-1"></a><span class="co"># 4.3.1.2 Manhattan Distance</span></span>
<span id="cb17-46"><a href="#cb17-46" aria-hidden="true" tabindex="-1"></a>manhattan_distance <span class="op">=</span> np.<span class="bu">sum</span>(np.<span class="bu">abs</span>(point1 <span class="op">-</span> point2))</span>
<span id="cb17-47"><a href="#cb17-47" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'Manhattan Distance between </span><span class="sc">{</span>point1<span class="sc">}</span><span class="ss"> and </span><span class="sc">{</span>point2<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>manhattan_distance<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb17-48"><a href="#cb17-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-49"><a href="#cb17-49" aria-hidden="true" tabindex="-1"></a><span class="co"># 4.3.1.3 Minkowski Distance</span></span>
<span id="cb17-50"><a href="#cb17-50" aria-hidden="true" tabindex="-1"></a>p <span class="op">=</span> <span class="dv">3</span>  <span class="co"># Change p to 2 for Euclidean distance, 1 for Manhattan distance</span></span>
<span id="cb17-51"><a href="#cb17-51" aria-hidden="true" tabindex="-1"></a>minkowski_distance <span class="op">=</span> np.<span class="bu">sum</span>(np.<span class="bu">abs</span>(point1 <span class="op">-</span> point2) <span class="op">**</span> p) <span class="op">**</span> (<span class="dv">1</span> <span class="op">/</span> p)</span>
<span id="cb17-52"><a href="#cb17-52" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'Minkowski Distance between </span><span class="sc">{</span>point1<span class="sc">}</span><span class="ss"> and </span><span class="sc">{</span>point2<span class="sc">}</span><span class="ss"> with p=</span><span class="sc">{</span>p<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>minkowski_distance<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb17-53"><a href="#cb17-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-54"><a href="#cb17-54" aria-hidden="true" tabindex="-1"></a><span class="co"># 4.3.2 Choosing the Optimal k using 5-fold Cross-Validation for Classification</span></span>
<span id="cb17-55"><a href="#cb17-55" aria-hidden="true" tabindex="-1"></a>k_range <span class="op">=</span> <span class="bu">range</span>(<span class="dv">1</span>, <span class="dv">21</span>)</span>
<span id="cb17-56"><a href="#cb17-56" aria-hidden="true" tabindex="-1"></a>cross_val_scores <span class="op">=</span> []</span>
<span id="cb17-57"><a href="#cb17-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-58"><a href="#cb17-58" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> k <span class="kw">in</span> k_range:</span>
<span id="cb17-59"><a href="#cb17-59" aria-hidden="true" tabindex="-1"></a>    knn <span class="op">=</span> KNeighborsClassifier(n_neighbors<span class="op">=</span>k)</span>
<span id="cb17-60"><a href="#cb17-60" aria-hidden="true" tabindex="-1"></a>    scores <span class="op">=</span> cross_val_score(knn, X_train_class_scaled, y_train_class, cv<span class="op">=</span><span class="dv">5</span>, scoring<span class="op">=</span><span class="st">'accuracy'</span>)</span>
<span id="cb17-61"><a href="#cb17-61" aria-hidden="true" tabindex="-1"></a>    cross_val_scores.append(scores.mean())</span>
<span id="cb17-62"><a href="#cb17-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-63"><a href="#cb17-63" aria-hidden="true" tabindex="-1"></a>optimal_k <span class="op">=</span> k_range[np.argmax(cross_val_scores)]</span>
<span id="cb17-64"><a href="#cb17-64" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'Optimal k for classification: </span><span class="sc">{</span>optimal_k<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb17-65"><a href="#cb17-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-66"><a href="#cb17-66" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">6</span>))</span>
<span id="cb17-67"><a href="#cb17-67" aria-hidden="true" tabindex="-1"></a>plt.plot(k_range, cross_val_scores, marker<span class="op">=</span><span class="st">'o'</span>)</span>
<span id="cb17-68"><a href="#cb17-68" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'k'</span>)</span>
<span id="cb17-69"><a href="#cb17-69" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Cross-Validated Accuracy'</span>)</span>
<span id="cb17-70"><a href="#cb17-70" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Choosing the Optimal k for k-NN Classification'</span>)</span>
<span id="cb17-71"><a href="#cb17-71" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb17-72"><a href="#cb17-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-73"><a href="#cb17-73" aria-hidden="true" tabindex="-1"></a><span class="co"># Train the k-NN classifier with the optimal k</span></span>
<span id="cb17-74"><a href="#cb17-74" aria-hidden="true" tabindex="-1"></a>knn_classifier <span class="op">=</span> KNeighborsClassifier(n_neighbors<span class="op">=</span>optimal_k)</span>
<span id="cb17-75"><a href="#cb17-75" aria-hidden="true" tabindex="-1"></a>knn_classifier.fit(X_train_class_scaled, y_train_class)</span>
<span id="cb17-76"><a href="#cb17-76" aria-hidden="true" tabindex="-1"></a>y_pred_class <span class="op">=</span> knn_classifier.predict(X_test_class_scaled)</span>
<span id="cb17-77"><a href="#cb17-77" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-78"><a href="#cb17-78" aria-hidden="true" tabindex="-1"></a><span class="co"># Evaluate the classifier</span></span>
<span id="cb17-79"><a href="#cb17-79" aria-hidden="true" tabindex="-1"></a>accuracy <span class="op">=</span> accuracy_score(y_test_class, y_pred_class)</span>
<span id="cb17-80"><a href="#cb17-80" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'Accuracy of k-NN classifier with k=</span><span class="sc">{</span>optimal_k<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>accuracy<span class="sc">:.2f}</span><span class="ss">'</span>)</span>
<span id="cb17-81"><a href="#cb17-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-82"><a href="#cb17-82" aria-hidden="true" tabindex="-1"></a><span class="co"># 4.3.3 Weighted k-NN for Classification</span></span>
<span id="cb17-83"><a href="#cb17-83" aria-hidden="true" tabindex="-1"></a>weighted_knn_classifier <span class="op">=</span> KNeighborsClassifier(n_neighbors<span class="op">=</span>optimal_k, weights<span class="op">=</span><span class="st">'distance'</span>)</span>
<span id="cb17-84"><a href="#cb17-84" aria-hidden="true" tabindex="-1"></a>weighted_knn_classifier.fit(X_train_class_scaled, y_train_class)</span>
<span id="cb17-85"><a href="#cb17-85" aria-hidden="true" tabindex="-1"></a>y_pred_weighted_class <span class="op">=</span> weighted_knn_classifier.predict(X_test_class_scaled)</span>
<span id="cb17-86"><a href="#cb17-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-87"><a href="#cb17-87" aria-hidden="true" tabindex="-1"></a><span class="co"># Evaluate the weighted classifier</span></span>
<span id="cb17-88"><a href="#cb17-88" aria-hidden="true" tabindex="-1"></a>weighted_accuracy <span class="op">=</span> accuracy_score(y_test_class, y_pred_weighted_class)</span>
<span id="cb17-89"><a href="#cb17-89" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'Accuracy of weighted k-NN classifier with k=</span><span class="sc">{</span>optimal_k<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>weighted_accuracy<span class="sc">:.2f}</span><span class="ss">'</span>)</span>
<span id="cb17-90"><a href="#cb17-90" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-91"><a href="#cb17-91" aria-hidden="true" tabindex="-1"></a><span class="co"># 4.3.4 k-NN for Regression</span></span>
<span id="cb17-92"><a href="#cb17-92" aria-hidden="true" tabindex="-1"></a>knn_regressor <span class="op">=</span> KNeighborsRegressor(n_neighbors<span class="op">=</span>optimal_k)</span>
<span id="cb17-93"><a href="#cb17-93" aria-hidden="true" tabindex="-1"></a>knn_regressor.fit(X_train_reg_scaled, y_train_reg)</span>
<span id="cb17-94"><a href="#cb17-94" aria-hidden="true" tabindex="-1"></a>y_pred_reg <span class="op">=</span> knn_regressor.predict(X_test_reg_scaled)</span>
<span id="cb17-95"><a href="#cb17-95" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-96"><a href="#cb17-96" aria-hidden="true" tabindex="-1"></a><span class="co"># Evaluate the regressor</span></span>
<span id="cb17-97"><a href="#cb17-97" aria-hidden="true" tabindex="-1"></a>mse <span class="op">=</span> mean_squared_error(y_test_reg, y_pred_reg)</span>
<span id="cb17-98"><a href="#cb17-98" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'Mean Squared Error of k-NN regressor with k=</span><span class="sc">{</span>optimal_k<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>mse<span class="sc">:.2f}</span><span class="ss">'</span>)</span>
<span id="cb17-99"><a href="#cb17-99" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-100"><a href="#cb17-100" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">6</span>))</span>
<span id="cb17-101"><a href="#cb17-101" aria-hidden="true" tabindex="-1"></a>plt.scatter(y_test_reg, y_pred_reg)</span>
<span id="cb17-102"><a href="#cb17-102" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'True Values'</span>)</span>
<span id="cb17-103"><a href="#cb17-103" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Predicted Values'</span>)</span>
<span id="cb17-104"><a href="#cb17-104" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'k-NN Regression: True vs Predicted Values'</span>)</span>
<span id="cb17-105"><a href="#cb17-105" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Euclidean Distance between [2 3] and [5 7]: 5.0
Manhattan Distance between [2 3] and [5 7]: 7
Minkowski Distance between [2 3] and [5 7] with p=3: 4.497941445275415
Optimal k for classification: 12</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="chapter4_basic_supervised_learning_algorithms_files/figure-html/cell-8-output-2.png" width="821" height="523" class="figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Accuracy of k-NN classifier with k=12: 0.90
Accuracy of weighted k-NN classifier with k=12: 0.93
Mean Squared Error of k-NN regressor with k=12: 0.01</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="chapter4_basic_supervised_learning_algorithms_files/figure-html/cell-8-output-4.png" width="812" height="523" class="figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="decision-trees" class="level1">
<h1>4.4 Decision Trees</h1>
<p>Decision trees are a type of supervised learning algorithm used for both classification and regression tasks. They work by splitting the data into subsets based on the values of the input features, creating a tree-like structure of decisions.</p>
<section id="information-gain-and-entropy" class="level2">
<h2 class="anchored" data-anchor-id="information-gain-and-entropy">4.4.1 Information Gain and Entropy</h2>
<p>Information gain measures the reduction in entropy, or uncertainty, when a dataset is split on an attribute. It is used to decide which feature to split on at each step in the tree.</p>
<ul>
<li><strong>Entropy:</strong></li>
</ul>
<p><span class="math display">\[
H(S) = - \sum_{i=1}^{c} p_i \log_2(p_i)
\]</span></p>
<p>where <span class="math inline">\(S\)</span> is the dataset, <span class="math inline">\(c\)</span> is the number of classes, and <span class="math inline">\(p_i\)</span> is the proportion of instances in class <span class="math inline">\(i\)</span>.</p>
<ul>
<li><strong>Information Gain:</strong></li>
</ul>
<p><span class="math display">\[
IG(T, a) = H(T) - \sum_{v \in \text{Values}(a)} \frac{|T_v|}{|T|} H(T_v)
\]</span></p>
<p>where <span class="math inline">\(T\)</span> is the dataset, <span class="math inline">\(a\)</span> is the attribute, <span class="math inline">\(v\)</span> represents values of the attribute, <span class="math inline">\(T_v\)</span> is the subset of <span class="math inline">\(T\)</span> for which attribute <span class="math inline">\(a\)</span> has value <span class="math inline">\(v\)</span>, and <span class="math inline">\(H(T_v)\)</span> is the entropy of <span class="math inline">\(T_v\)</span>.</p>
<ul>
<li><strong>Example:</strong> Calculating the information gain for splitting a dataset on a feature like “outlook” in a weather dataset.</li>
</ul>
<section id="steps-to-calculate-information-gain" class="level4">
<h4 class="anchored" data-anchor-id="steps-to-calculate-information-gain">Steps to Calculate Information Gain:</h4>
<ol type="1">
<li><p><strong>Calculate the entropy of the entire dataset:</strong> Determine the proportion of each class in the dataset and apply the entropy formula.</p></li>
<li><p><strong>Split the dataset on the chosen attribute:</strong> Divide the data based on the different values of the attribute.</p></li>
<li><p><strong>Calculate the entropy of each subset:</strong> For each subset created by the split, calculate its entropy.</p></li>
<li><p><strong>Compute the weighted average of these entropies:</strong> Weight each subset’s entropy by its proportion in the dataset.</p></li>
<li><p><strong>Subtract this value from the original entropy:</strong> The result is the information gain.</p></li>
</ol>
</section>
</section>
<section id="gini-impurity" class="level2">
<h2 class="anchored" data-anchor-id="gini-impurity">4.4.2 Gini Impurity</h2>
<p>Gini impurity measures the frequency at which any element of the dataset would be mislabeled if it were randomly labeled according to the distribution of labels in the subset.</p>
<ul>
<li><strong>Formula:</strong></li>
</ul>
<p><span class="math display">\[
Gini(T) = 1 - \sum_{i=1}^{c} p_i^2
\]</span></p>
<p>where <span class="math inline">\(T\)</span> is the dataset, <span class="math inline">\(c\)</span> is the number of classes, and <span class="math inline">\(p_i\)</span> is the proportion of instances in class <span class="math inline">\(i\)</span>.</p>
<ul>
<li><strong>Example:</strong> Calculating the Gini impurity for a node in a decision tree.</li>
</ul>
<section id="steps-to-calculate-gini-impurity" class="level4">
<h4 class="anchored" data-anchor-id="steps-to-calculate-gini-impurity">Steps to Calculate Gini Impurity:</h4>
<ol type="1">
<li><p><strong>Determine the proportion of each class in the dataset:</strong> Compute <span class="math inline">\(p_i\)</span> for each class.</p></li>
<li><p><strong>Square each proportion and sum them:</strong> Sum the squared proportions.</p></li>
<li><p><strong>Subtract the sum from 1:</strong> The result is the Gini impurity.</p></li>
</ol>
<div id="1531264d" class="cell" data-execution_count="8">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> collections <span class="im">import</span> Counter</span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate synthetic data for a simple weather dataset</span></span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> {</span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Outlook'</span>: [<span class="st">'Sunny'</span>, <span class="st">'Sunny'</span>, <span class="st">'Overcast'</span>, <span class="st">'Rainy'</span>, <span class="st">'Rainy'</span>, <span class="st">'Rainy'</span>, <span class="st">'Overcast'</span>, <span class="st">'Sunny'</span>, <span class="st">'Sunny'</span>, <span class="st">'Rainy'</span>, <span class="st">'Sunny'</span>, <span class="st">'Overcast'</span>, <span class="st">'Overcast'</span>, <span class="st">'Rainy'</span>],</span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Temperature'</span>: [<span class="st">'Hot'</span>, <span class="st">'Hot'</span>, <span class="st">'Hot'</span>, <span class="st">'Mild'</span>, <span class="st">'Cool'</span>, <span class="st">'Cool'</span>, <span class="st">'Cool'</span>, <span class="st">'Mild'</span>, <span class="st">'Cool'</span>, <span class="st">'Mild'</span>, <span class="st">'Mild'</span>, <span class="st">'Mild'</span>, <span class="st">'Hot'</span>, <span class="st">'Mild'</span>],</span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Humidity'</span>: [<span class="st">'High'</span>, <span class="st">'High'</span>, <span class="st">'High'</span>, <span class="st">'High'</span>, <span class="st">'Normal'</span>, <span class="st">'Normal'</span>, <span class="st">'Normal'</span>, <span class="st">'High'</span>, <span class="st">'Normal'</span>, <span class="st">'Normal'</span>, <span class="st">'Normal'</span>, <span class="st">'High'</span>, <span class="st">'Normal'</span>, <span class="st">'High'</span>],</span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Windy'</span>: [<span class="va">False</span>, <span class="va">True</span>, <span class="va">False</span>, <span class="va">False</span>, <span class="va">False</span>, <span class="va">True</span>, <span class="va">True</span>, <span class="va">False</span>, <span class="va">False</span>, <span class="va">False</span>, <span class="va">True</span>, <span class="va">True</span>, <span class="va">False</span>, <span class="va">True</span>],</span>
<span id="cb20-11"><a href="#cb20-11" aria-hidden="true" tabindex="-1"></a>    <span class="st">'PlayTennis'</span>: [<span class="st">'No'</span>, <span class="st">'No'</span>, <span class="st">'Yes'</span>, <span class="st">'Yes'</span>, <span class="st">'Yes'</span>, <span class="st">'No'</span>, <span class="st">'Yes'</span>, <span class="st">'No'</span>, <span class="st">'Yes'</span>, <span class="st">'Yes'</span>, <span class="st">'Yes'</span>, <span class="st">'Yes'</span>, <span class="st">'Yes'</span>, <span class="st">'No'</span>]</span>
<span id="cb20-12"><a href="#cb20-12" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb20-13"><a href="#cb20-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-14"><a href="#cb20-14" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.DataFrame(data)</span>
<span id="cb20-15"><a href="#cb20-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-16"><a href="#cb20-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Function to calculate entropy</span></span>
<span id="cb20-17"><a href="#cb20-17" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> entropy(y):</span>
<span id="cb20-18"><a href="#cb20-18" aria-hidden="true" tabindex="-1"></a>    counts <span class="op">=</span> Counter(y)</span>
<span id="cb20-19"><a href="#cb20-19" aria-hidden="true" tabindex="-1"></a>    probabilities <span class="op">=</span> [count <span class="op">/</span> <span class="bu">len</span>(y) <span class="cf">for</span> count <span class="kw">in</span> counts.values()]</span>
<span id="cb20-20"><a href="#cb20-20" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="op">-</span><span class="bu">sum</span>(p <span class="op">*</span> np.log2(p) <span class="cf">for</span> p <span class="kw">in</span> probabilities)</span>
<span id="cb20-21"><a href="#cb20-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-22"><a href="#cb20-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Function to calculate information gain</span></span>
<span id="cb20-23"><a href="#cb20-23" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> information_gain(df, split_attribute, target_attribute<span class="op">=</span><span class="st">'PlayTennis'</span>):</span>
<span id="cb20-24"><a href="#cb20-24" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Calculate the entropy of the full dataset</span></span>
<span id="cb20-25"><a href="#cb20-25" aria-hidden="true" tabindex="-1"></a>    original_entropy <span class="op">=</span> entropy(df[target_attribute])</span>
<span id="cb20-26"><a href="#cb20-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-27"><a href="#cb20-27" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Split the dataset by the unique values of the split attribute</span></span>
<span id="cb20-28"><a href="#cb20-28" aria-hidden="true" tabindex="-1"></a>    subsets <span class="op">=</span> [df[df[split_attribute] <span class="op">==</span> value] <span class="cf">for</span> value <span class="kw">in</span> df[split_attribute].unique()]</span>
<span id="cb20-29"><a href="#cb20-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-30"><a href="#cb20-30" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Calculate the weighted entropy of the subsets</span></span>
<span id="cb20-31"><a href="#cb20-31" aria-hidden="true" tabindex="-1"></a>    subset_entropy <span class="op">=</span> <span class="bu">sum</span>((<span class="bu">len</span>(subset) <span class="op">/</span> <span class="bu">len</span>(df)) <span class="op">*</span> entropy(subset[target_attribute]) <span class="cf">for</span> subset <span class="kw">in</span> subsets)</span>
<span id="cb20-32"><a href="#cb20-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-33"><a href="#cb20-33" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Calculate the information gain</span></span>
<span id="cb20-34"><a href="#cb20-34" aria-hidden="true" tabindex="-1"></a>    info_gain <span class="op">=</span> original_entropy <span class="op">-</span> subset_entropy</span>
<span id="cb20-35"><a href="#cb20-35" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> info_gain</span>
<span id="cb20-36"><a href="#cb20-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-37"><a href="#cb20-37" aria-hidden="true" tabindex="-1"></a><span class="co"># Example: Calculating information gain for the "Outlook" attribute</span></span>
<span id="cb20-38"><a href="#cb20-38" aria-hidden="true" tabindex="-1"></a>info_gain_outlook <span class="op">=</span> information_gain(df, <span class="st">'Outlook'</span>)</span>
<span id="cb20-39"><a href="#cb20-39" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'Information Gain for Outlook: </span><span class="sc">{</span>info_gain_outlook<span class="sc">:.4f}</span><span class="ss">'</span>)</span>
<span id="cb20-40"><a href="#cb20-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-41"><a href="#cb20-41" aria-hidden="true" tabindex="-1"></a><span class="co"># Function to calculate Gini impurity</span></span>
<span id="cb20-42"><a href="#cb20-42" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> gini_impurity(y):</span>
<span id="cb20-43"><a href="#cb20-43" aria-hidden="true" tabindex="-1"></a>    counts <span class="op">=</span> Counter(y)</span>
<span id="cb20-44"><a href="#cb20-44" aria-hidden="true" tabindex="-1"></a>    probabilities <span class="op">=</span> [count <span class="op">/</span> <span class="bu">len</span>(y) <span class="cf">for</span> count <span class="kw">in</span> counts.values()]</span>
<span id="cb20-45"><a href="#cb20-45" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="dv">1</span> <span class="op">-</span> <span class="bu">sum</span>(p <span class="op">**</span> <span class="dv">2</span> <span class="cf">for</span> p <span class="kw">in</span> probabilities)</span>
<span id="cb20-46"><a href="#cb20-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-47"><a href="#cb20-47" aria-hidden="true" tabindex="-1"></a><span class="co"># Example: Calculating Gini impurity for the PlayTennis attribute</span></span>
<span id="cb20-48"><a href="#cb20-48" aria-hidden="true" tabindex="-1"></a>gini_play_tennis <span class="op">=</span> gini_impurity(df[<span class="st">'PlayTennis'</span>])</span>
<span id="cb20-49"><a href="#cb20-49" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'Gini Impurity for PlayTennis: </span><span class="sc">{</span>gini_play_tennis<span class="sc">:.4f}</span><span class="ss">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Information Gain for Outlook: 0.2467
Gini Impurity for PlayTennis: 0.4592</code></pre>
</div>
</div>
</section>
</section>
<section id="cart-algorithm" class="level2">
<h2 class="anchored" data-anchor-id="cart-algorithm">4.4.3 CART Algorithm</h2>
<p>The Classification and Regression Tree (CART) algorithm is a popular decision tree algorithm that uses Gini impurity for classification and mean squared error for regression.</p>
<section id="steps-of-the-cart-algorithm" class="level4">
<h4 class="anchored" data-anchor-id="steps-of-the-cart-algorithm">Steps of the CART Algorithm:</h4>
<ol type="1">
<li><p><strong>Splitting:</strong></p>
<ul>
<li><p>At each node, split the data on the feature that results in the highest information gain (classification) or the lowest mean squared error (regression).</p></li>
<li><p>For classification:</p>
<ul>
<li>Calculate the Gini impurity for each possible split and choose the one that minimizes the impurity.</li>
</ul></li>
<li><p>For regression:</p>
<ul>
<li>Calculate the mean squared error for each possible split and choose the one that minimizes the error.</li>
</ul></li>
</ul></li>
<li><p><strong>Stopping Criteria:</strong></p>
<ul>
<li><p>Stop splitting when a predefined criterion is met, such as a maximum tree depth or a minimum number of samples per leaf.</p></li>
<li><p>Common stopping criteria include:</p>
<ul>
<li><p>Maximum depth of the tree.</p></li>
<li><p>Minimum number of samples required to split an internal node.</p></li>
<li><p>Minimum number of samples required to be at a leaf node.</p></li>
</ul></li>
</ul></li>
<li><p><strong>Prediction:</strong></p>
<ul>
<li><p>For classification:</p>
<ul>
<li>Assign the most common class in the leaf node to any new data point that falls into that leaf.</li>
</ul></li>
<li><p>For regression:</p>
<ul>
<li>Assign the mean value of the target variable in the leaf node to any new data point that falls into that leaf.</li>
</ul></li>
</ul></li>
</ol>
<ul>
<li><strong>Example:</strong> Using the CART algorithm to build a decision tree for predicting whether a passenger survived the Titanic disaster.</li>
</ul>
<div id="56824aca" class="cell" data-execution_count="9">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.tree <span class="im">import</span> DecisionTreeClassifier, plot_tree</span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> accuracy_score, confusion_matrix, classification_report</span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Load the Iris dataset</span></span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> load_iris</span>
<span id="cb22-9"><a href="#cb22-9" aria-hidden="true" tabindex="-1"></a>iris <span class="op">=</span> load_iris()</span>
<span id="cb22-10"><a href="#cb22-10" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> pd.DataFrame(data<span class="op">=</span>iris.data, columns<span class="op">=</span>iris.feature_names)</span>
<span id="cb22-11"><a href="#cb22-11" aria-hidden="true" tabindex="-1"></a>data[<span class="st">'target'</span>] <span class="op">=</span> iris.target</span>
<span id="cb22-12"><a href="#cb22-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-13"><a href="#cb22-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Display the first few rows of the dataset</span></span>
<span id="cb22-14"><a href="#cb22-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(data.head())</span>
<span id="cb22-15"><a href="#cb22-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-16"><a href="#cb22-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Select features and the target variable</span></span>
<span id="cb22-17"><a href="#cb22-17" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> data.drop(<span class="st">'target'</span>, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb22-18"><a href="#cb22-18" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> data[<span class="st">'target'</span>]</span>
<span id="cb22-19"><a href="#cb22-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-20"><a href="#cb22-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Split the data into training and testing sets</span></span>
<span id="cb22-21"><a href="#cb22-21" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, y, test_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb22-22"><a href="#cb22-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-23"><a href="#cb22-23" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize the CART model (Decision Tree Classifier)</span></span>
<span id="cb22-24"><a href="#cb22-24" aria-hidden="true" tabindex="-1"></a>cart_model <span class="op">=</span> DecisionTreeClassifier(criterion<span class="op">=</span><span class="st">'gini'</span>, max_depth<span class="op">=</span><span class="dv">5</span>, min_samples_split<span class="op">=</span><span class="dv">20</span>, min_samples_leaf<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb22-25"><a href="#cb22-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-26"><a href="#cb22-26" aria-hidden="true" tabindex="-1"></a><span class="co"># Train the model</span></span>
<span id="cb22-27"><a href="#cb22-27" aria-hidden="true" tabindex="-1"></a>cart_model.fit(X_train, y_train)</span>
<span id="cb22-28"><a href="#cb22-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-29"><a href="#cb22-29" aria-hidden="true" tabindex="-1"></a><span class="co"># Predict the outcomes on the test set</span></span>
<span id="cb22-30"><a href="#cb22-30" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> cart_model.predict(X_test)</span>
<span id="cb22-31"><a href="#cb22-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-32"><a href="#cb22-32" aria-hidden="true" tabindex="-1"></a><span class="co"># Evaluate the model</span></span>
<span id="cb22-33"><a href="#cb22-33" aria-hidden="true" tabindex="-1"></a>accuracy <span class="op">=</span> accuracy_score(y_test, y_pred)</span>
<span id="cb22-34"><a href="#cb22-34" aria-hidden="true" tabindex="-1"></a>conf_matrix <span class="op">=</span> confusion_matrix(y_test, y_pred)</span>
<span id="cb22-35"><a href="#cb22-35" aria-hidden="true" tabindex="-1"></a>class_report <span class="op">=</span> classification_report(y_test, y_pred)</span>
<span id="cb22-36"><a href="#cb22-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-37"><a href="#cb22-37" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'Accuracy: </span><span class="sc">{</span>accuracy <span class="op">*</span> <span class="dv">100</span><span class="sc">:.2f}</span><span class="ss">%'</span>)</span>
<span id="cb22-38"><a href="#cb22-38" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Confusion Matrix:'</span>)</span>
<span id="cb22-39"><a href="#cb22-39" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(conf_matrix)</span>
<span id="cb22-40"><a href="#cb22-40" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Classification Report:'</span>)</span>
<span id="cb22-41"><a href="#cb22-41" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(class_report)</span>
<span id="cb22-42"><a href="#cb22-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-43"><a href="#cb22-43" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize the decision tree</span></span>
<span id="cb22-44"><a href="#cb22-44" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">20</span>, <span class="dv">10</span>))</span>
<span id="cb22-45"><a href="#cb22-45" aria-hidden="true" tabindex="-1"></a>plot_tree(cart_model, feature_names<span class="op">=</span>iris.feature_names, class_names<span class="op">=</span>iris.target_names, filled<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb22-46"><a href="#cb22-46" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Decision Tree Trained on Iris Dataset'</span>)</span>
<span id="cb22-47"><a href="#cb22-47" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \
0                5.1               3.5                1.4               0.2   
1                4.9               3.0                1.4               0.2   
2                4.7               3.2                1.3               0.2   
3                4.6               3.1                1.5               0.2   
4                5.0               3.6                1.4               0.2   

   target  
0       0  
1       0  
2       0  
3       0  
4       0  
Accuracy: 96.67%
Confusion Matrix:
[[10  0  0]
 [ 0  8  1]
 [ 0  0 11]]
Classification Report:
              precision    recall  f1-score   support

           0       1.00      1.00      1.00        10
           1       1.00      0.89      0.94         9
           2       0.92      1.00      0.96        11

    accuracy                           0.97        30
   macro avg       0.97      0.96      0.97        30
weighted avg       0.97      0.97      0.97        30
</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="chapter4_basic_supervised_learning_algorithms_files/figure-html/cell-10-output-2.png" width="1507" height="778" class="figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="pruning-techniques" class="level2">
<h2 class="anchored" data-anchor-id="pruning-techniques">4.4.4 Pruning Techniques</h2>
<p>Pruning techniques are used to reduce the size of a decision tree by removing parts of the tree that do not provide additional power in classifying instances. This helps to prevent overfitting.</p>
<section id="pre-pruning" class="level4">
<h4 class="anchored" data-anchor-id="pre-pruning">4.4.4.1 Pre-pruning</h4>
<p>Pre-pruning, also known as early stopping, halts the growth of the tree before it reaches its maximum size.</p>
<ul>
<li><p><strong>Methods:</strong></p>
<ol type="1">
<li><p><strong>Maximum Depth:</strong> Limit the maximum depth of the tree.</p></li>
<li><p><strong>Minimum Samples per Leaf:</strong> Require a minimum number of samples in each leaf node.</p></li>
<li><p><strong>Minimum Information Gain:</strong> Set a threshold for the minimum information gain required to make a split.</p></li>
</ol></li>
<li><p><strong>Example:</strong> Limiting the maximum depth of a decision tree to 5 levels to prevent overfitting.</p></li>
</ul>
</section>
<section id="post-pruning" class="level4">
<h4 class="anchored" data-anchor-id="post-pruning">4.4.4.2 Post-pruning</h4>
<p>Post-pruning involves building the entire tree first and then removing nodes that do not provide significant improvements.</p>
<ul>
<li><p><strong>Methods:</strong></p>
<ol type="1">
<li><p><strong>Cost Complexity Pruning:</strong> Use a validation set to prune branches that do not improve the validation accuracy.</p></li>
<li><p><strong>Reduced Error Pruning:</strong> Evaluate the impact of removing each node on the training set and prune nodes that do not reduce accuracy.</p></li>
</ol></li>
<li><p><strong>Example:</strong> Using cross-validation to determine which branches of the tree to prune after it has been fully grown.</p></li>
</ul>
</section>
<section id="steps-for-post-pruning" class="level4">
<h4 class="anchored" data-anchor-id="steps-for-post-pruning">Steps for Post-pruning:</h4>
<ol type="1">
<li><p><strong>Grow the full tree:</strong> Allow the decision tree to grow to its maximum size.</p></li>
<li><p><strong>Evaluate each node for pruning:</strong> For each non-leaf node, evaluate whether its removal (and replacement with a leaf node) would improve the model’s performance on a validation set.</p></li>
<li><p><strong>Prune the tree:</strong> Remove nodes that do not contribute to improved performance or reduce complexity without harming the model’s accuracy.</p></li>
</ol>
<div id="cbcb37ce" class="cell" data-execution_count="10">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split, cross_val_score</span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.tree <span class="im">import</span> DecisionTreeClassifier, plot_tree</span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> accuracy_score, confusion_matrix, classification_report</span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-8"><a href="#cb24-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Load the Titanic dataset from Kaggle</span></span>
<span id="cb24-9"><a href="#cb24-9" aria-hidden="true" tabindex="-1"></a>url <span class="op">=</span> <span class="st">'https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv'</span></span>
<span id="cb24-10"><a href="#cb24-10" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> pd.read_csv(url)</span>
<span id="cb24-11"><a href="#cb24-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-12"><a href="#cb24-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Select relevant features and the target variable</span></span>
<span id="cb24-13"><a href="#cb24-13" aria-hidden="true" tabindex="-1"></a>features <span class="op">=</span> [<span class="st">'Pclass'</span>, <span class="st">'Sex'</span>, <span class="st">'Age'</span>, <span class="st">'SibSp'</span>, <span class="st">'Parch'</span>, <span class="st">'Fare'</span>]</span>
<span id="cb24-14"><a href="#cb24-14" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> data.dropna(subset<span class="op">=</span>[<span class="st">'Age'</span>])</span>
<span id="cb24-15"><a href="#cb24-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-16"><a href="#cb24-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert categorical variables to numerical</span></span>
<span id="cb24-17"><a href="#cb24-17" aria-hidden="true" tabindex="-1"></a>data[<span class="st">'Sex'</span>] <span class="op">=</span> data[<span class="st">'Sex'</span>].<span class="bu">map</span>({<span class="st">'male'</span>: <span class="dv">0</span>, <span class="st">'female'</span>: <span class="dv">1</span>})</span>
<span id="cb24-18"><a href="#cb24-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-19"><a href="#cb24-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Fill missing values with the median for numerical features</span></span>
<span id="cb24-20"><a href="#cb24-20" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> feature <span class="kw">in</span> [<span class="st">'Age'</span>, <span class="st">'Fare'</span>]:</span>
<span id="cb24-21"><a href="#cb24-21" aria-hidden="true" tabindex="-1"></a>    data[feature].fillna(data[feature].median(), inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb24-22"><a href="#cb24-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-23"><a href="#cb24-23" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> data[features]</span>
<span id="cb24-24"><a href="#cb24-24" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> data[<span class="st">'Survived'</span>]</span>
<span id="cb24-25"><a href="#cb24-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-26"><a href="#cb24-26" aria-hidden="true" tabindex="-1"></a><span class="co"># Split the data into training and testing sets</span></span>
<span id="cb24-27"><a href="#cb24-27" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, y, test_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb24-28"><a href="#cb24-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-29"><a href="#cb24-29" aria-hidden="true" tabindex="-1"></a><span class="co"># Pre-pruning Example: Limiting the maximum depth of a decision tree</span></span>
<span id="cb24-30"><a href="#cb24-30" aria-hidden="true" tabindex="-1"></a>pre_pruned_model <span class="op">=</span> DecisionTreeClassifier(criterion<span class="op">=</span><span class="st">'gini'</span>, max_depth<span class="op">=</span><span class="dv">5</span>, min_samples_split<span class="op">=</span><span class="dv">20</span>, min_samples_leaf<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb24-31"><a href="#cb24-31" aria-hidden="true" tabindex="-1"></a>pre_pruned_model.fit(X_train, y_train)</span>
<span id="cb24-32"><a href="#cb24-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-33"><a href="#cb24-33" aria-hidden="true" tabindex="-1"></a><span class="co"># Predict the outcomes on the test set</span></span>
<span id="cb24-34"><a href="#cb24-34" aria-hidden="true" tabindex="-1"></a>y_pred_pre_pruned <span class="op">=</span> pre_pruned_model.predict(X_test)</span>
<span id="cb24-35"><a href="#cb24-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-36"><a href="#cb24-36" aria-hidden="true" tabindex="-1"></a><span class="co"># Evaluate the pre-pruned model</span></span>
<span id="cb24-37"><a href="#cb24-37" aria-hidden="true" tabindex="-1"></a>accuracy_pre_pruned <span class="op">=</span> accuracy_score(y_test, y_pred_pre_pruned)</span>
<span id="cb24-38"><a href="#cb24-38" aria-hidden="true" tabindex="-1"></a>conf_matrix_pre_pruned <span class="op">=</span> confusion_matrix(y_test, y_pred_pre_pruned)</span>
<span id="cb24-39"><a href="#cb24-39" aria-hidden="true" tabindex="-1"></a>class_report_pre_pruned <span class="op">=</span> classification_report(y_test, y_pred_pre_pruned)</span>
<span id="cb24-40"><a href="#cb24-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-41"><a href="#cb24-41" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'Pre-pruned Model Accuracy: </span><span class="sc">{</span>accuracy_pre_pruned <span class="op">*</span> <span class="dv">100</span><span class="sc">:.2f}</span><span class="ss">%'</span>)</span>
<span id="cb24-42"><a href="#cb24-42" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Pre-pruned Model Confusion Matrix:'</span>)</span>
<span id="cb24-43"><a href="#cb24-43" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(conf_matrix_pre_pruned)</span>
<span id="cb24-44"><a href="#cb24-44" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Pre-pruned Model Classification Report:'</span>)</span>
<span id="cb24-45"><a href="#cb24-45" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(class_report_pre_pruned)</span>
<span id="cb24-46"><a href="#cb24-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-47"><a href="#cb24-47" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize the pre-pruned decision tree</span></span>
<span id="cb24-48"><a href="#cb24-48" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">20</span>, <span class="dv">10</span>))</span>
<span id="cb24-49"><a href="#cb24-49" aria-hidden="true" tabindex="-1"></a>plot_tree(pre_pruned_model, feature_names<span class="op">=</span>features, class_names<span class="op">=</span>[<span class="st">'Not Survived'</span>, <span class="st">'Survived'</span>], filled<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb24-50"><a href="#cb24-50" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Pre-pruned Decision Tree'</span>)</span>
<span id="cb24-51"><a href="#cb24-51" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb24-52"><a href="#cb24-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-53"><a href="#cb24-53" aria-hidden="true" tabindex="-1"></a><span class="co"># Post-pruning Example: Cost Complexity Pruning</span></span>
<span id="cb24-54"><a href="#cb24-54" aria-hidden="true" tabindex="-1"></a><span class="co"># Grow a large tree first</span></span>
<span id="cb24-55"><a href="#cb24-55" aria-hidden="true" tabindex="-1"></a>full_model <span class="op">=</span> DecisionTreeClassifier(criterion<span class="op">=</span><span class="st">'gini'</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb24-56"><a href="#cb24-56" aria-hidden="true" tabindex="-1"></a>full_model.fit(X_train, y_train)</span>
<span id="cb24-57"><a href="#cb24-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-58"><a href="#cb24-58" aria-hidden="true" tabindex="-1"></a><span class="co"># Perform cost complexity pruning</span></span>
<span id="cb24-59"><a href="#cb24-59" aria-hidden="true" tabindex="-1"></a>path <span class="op">=</span> full_model.cost_complexity_pruning_path(X_train, y_train)</span>
<span id="cb24-60"><a href="#cb24-60" aria-hidden="true" tabindex="-1"></a>ccp_alphas, impurities <span class="op">=</span> path.ccp_alphas, path.impurities</span>
<span id="cb24-61"><a href="#cb24-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-62"><a href="#cb24-62" aria-hidden="true" tabindex="-1"></a><span class="co"># Train models for each alpha value</span></span>
<span id="cb24-63"><a href="#cb24-63" aria-hidden="true" tabindex="-1"></a>models <span class="op">=</span> []</span>
<span id="cb24-64"><a href="#cb24-64" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> ccp_alpha <span class="kw">in</span> ccp_alphas:</span>
<span id="cb24-65"><a href="#cb24-65" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> DecisionTreeClassifier(random_state<span class="op">=</span><span class="dv">42</span>, ccp_alpha<span class="op">=</span>ccp_alpha)</span>
<span id="cb24-66"><a href="#cb24-66" aria-hidden="true" tabindex="-1"></a>    model.fit(X_train, y_train)</span>
<span id="cb24-67"><a href="#cb24-67" aria-hidden="true" tabindex="-1"></a>    models.append(model)</span>
<span id="cb24-68"><a href="#cb24-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-69"><a href="#cb24-69" aria-hidden="true" tabindex="-1"></a><span class="co"># Evaluate each model using cross-validation and choose the best one</span></span>
<span id="cb24-70"><a href="#cb24-70" aria-hidden="true" tabindex="-1"></a>cv_scores <span class="op">=</span> [np.mean(cross_val_score(model, X_train, y_train, cv<span class="op">=</span><span class="dv">5</span>)) <span class="cf">for</span> model <span class="kw">in</span> models]</span>
<span id="cb24-71"><a href="#cb24-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-72"><a href="#cb24-72" aria-hidden="true" tabindex="-1"></a><span class="co"># Select the best model</span></span>
<span id="cb24-73"><a href="#cb24-73" aria-hidden="true" tabindex="-1"></a>best_alpha <span class="op">=</span> ccp_alphas[np.argmax(cv_scores)]</span>
<span id="cb24-74"><a href="#cb24-74" aria-hidden="true" tabindex="-1"></a>post_pruned_model <span class="op">=</span> DecisionTreeClassifier(random_state<span class="op">=</span><span class="dv">42</span>, ccp_alpha<span class="op">=</span>best_alpha)</span>
<span id="cb24-75"><a href="#cb24-75" aria-hidden="true" tabindex="-1"></a>post_pruned_model.fit(X_train, y_train)</span>
<span id="cb24-76"><a href="#cb24-76" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-77"><a href="#cb24-77" aria-hidden="true" tabindex="-1"></a><span class="co"># Predict the outcomes on the test set</span></span>
<span id="cb24-78"><a href="#cb24-78" aria-hidden="true" tabindex="-1"></a>y_pred_post_pruned <span class="op">=</span> post_pruned_model.predict(X_test)</span>
<span id="cb24-79"><a href="#cb24-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-80"><a href="#cb24-80" aria-hidden="true" tabindex="-1"></a><span class="co"># Evaluate the post-pruned model</span></span>
<span id="cb24-81"><a href="#cb24-81" aria-hidden="true" tabindex="-1"></a>accuracy_post_pruned <span class="op">=</span> accuracy_score(y_test, y_pred_post_pruned)</span>
<span id="cb24-82"><a href="#cb24-82" aria-hidden="true" tabindex="-1"></a>conf_matrix_post_pruned <span class="op">=</span> confusion_matrix(y_test, y_pred_post_pruned)</span>
<span id="cb24-83"><a href="#cb24-83" aria-hidden="true" tabindex="-1"></a>class_report_post_pruned <span class="op">=</span> classification_report(y_test, y_pred_post_pruned)</span>
<span id="cb24-84"><a href="#cb24-84" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-85"><a href="#cb24-85" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'Post-pruned Model Accuracy: </span><span class="sc">{</span>accuracy_post_pruned <span class="op">*</span> <span class="dv">100</span><span class="sc">:.2f}</span><span class="ss">%'</span>)</span>
<span id="cb24-86"><a href="#cb24-86" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Post-pruned Model Confusion Matrix:'</span>)</span>
<span id="cb24-87"><a href="#cb24-87" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(conf_matrix_post_pruned)</span>
<span id="cb24-88"><a href="#cb24-88" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Post-pruned Model Classification Report:'</span>)</span>
<span id="cb24-89"><a href="#cb24-89" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(class_report_post_pruned)</span>
<span id="cb24-90"><a href="#cb24-90" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-91"><a href="#cb24-91" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize the post-pruned decision tree</span></span>
<span id="cb24-92"><a href="#cb24-92" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">20</span>, <span class="dv">10</span>))</span>
<span id="cb24-93"><a href="#cb24-93" aria-hidden="true" tabindex="-1"></a>plot_tree(post_pruned_model, feature_names<span class="op">=</span>features, class_names<span class="op">=</span>[<span class="st">'Not Survived'</span>, <span class="st">'Survived'</span>], filled<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb24-94"><a href="#cb24-94" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Post-pruned Decision Tree'</span>)</span>
<span id="cb24-95"><a href="#cb24-95" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stderr">
<pre><code>/var/folders/v8/l5r44ftx4g5bx2y5fhdpcmmh0000gn/T/ipykernel_20846/2672598698.py:21: FutureWarning:

A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.
The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.

For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.



/var/folders/v8/l5r44ftx4g5bx2y5fhdpcmmh0000gn/T/ipykernel_20846/2672598698.py:21: FutureWarning:

A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.
The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.

For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.


</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Pre-pruned Model Accuracy: 76.92%
Pre-pruned Model Confusion Matrix:
[[73 14]
 [19 37]]
Pre-pruned Model Classification Report:
              precision    recall  f1-score   support

           0       0.79      0.84      0.82        87
           1       0.73      0.66      0.69        56

    accuracy                           0.77       143
   macro avg       0.76      0.75      0.75       143
weighted avg       0.77      0.77      0.77       143
</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="chapter4_basic_supervised_learning_algorithms_files/figure-html/cell-11-output-3.png" width="1507" height="778" class="figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Post-pruned Model Accuracy: 75.52%
Post-pruned Model Confusion Matrix:
[[69 18]
 [17 39]]
Post-pruned Model Classification Report:
              precision    recall  f1-score   support

           0       0.80      0.79      0.80        87
           1       0.68      0.70      0.69        56

    accuracy                           0.76       143
   macro avg       0.74      0.74      0.74       143
weighted avg       0.76      0.76      0.76       143
</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="chapter4_basic_supervised_learning_algorithms_files/figure-html/cell-11-output-5.png" width="1507" height="778" class="figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="handling-missing-values-in-decision-trees" class="level2">
<h2 class="anchored" data-anchor-id="handling-missing-values-in-decision-trees">4.4.5 Handling Missing Values in Decision Trees</h2>
<p>Decision trees can handle missing values in several ways to ensure that the model remains robust and accurate.</p>
<ul>
<li><p><strong>Methods:</strong></p>
<ol type="1">
<li><p><strong>Surrogate Splits:</strong> Use an alternative feature to make the split when the primary feature is missing.</p>
<ul>
<li><strong>Example:</strong> If a data point is missing a value for the primary split feature, use another highly correlated feature to make the split.</li>
</ul></li>
<li><p><strong>Missing Value Indicator:</strong> Create a binary feature that indicates whether the value for the feature is missing.</p>
<ul>
<li><strong>Example:</strong> Add an extra feature that is 1 if the value is missing and 0 otherwise.</li>
</ul></li>
<li><p><strong>Imputation:</strong> Fill in missing values using methods like mean, median, or mode imputation before training the tree.</p>
<ul>
<li><strong>Example:</strong> Replace missing values in a dataset with the mean value of the respective feature.</li>
</ul></li>
</ol></li>
</ul>
<section id="steps-for-handling-missing-values" class="level4">
<h4 class="anchored" data-anchor-id="steps-for-handling-missing-values">Steps for Handling Missing Values:</h4>
<ol type="1">
<li><p><strong>Identify missing values:</strong> Determine which features and instances have missing values.</p></li>
<li><p><strong>Choose a handling method:</strong> Decide whether to use surrogate splits, a missing value indicator, or imputation.</p></li>
<li><p><strong>Implement the chosen method:</strong> Apply the selected method to the dataset before or during the tree-building process.</p></li>
</ol>
<p>Advanced considerations in decision trees include:</p>
<ul>
<li><p><strong>Ensemble Methods:</strong> Combine multiple decision trees using methods like bagging (e.g., Random Forests) or boosting (e.g., Gradient Boosting) to improve accuracy and robustness.</p>
<ul>
<li><p><strong>Bagging:</strong> Builds multiple decision trees on different subsets of the data and aggregates their predictions.</p></li>
<li><p><strong>Boosting:</strong> Sequentially builds trees, where each new tree focuses on correcting the errors made by the previous trees.</p></li>
</ul></li>
<li><p><strong>Feature Importance:</strong> Evaluate the importance of features based on how often they are used in splits across the trees in an ensemble.</p>
<ul>
<li><strong>Example:</strong> Calculate feature importance scores for a Random Forest model to determine which features contribute most to the predictions.</li>
</ul></li>
<li><p><strong>Hyperparameter Tuning:</strong> Use techniques like grid search or random search to find the best hyperparameters for the decision tree model.</p>
<ul>
<li><strong>Example:</strong> Perform a grid search over parameters like maximum depth, minimum samples per leaf, and the number of features to consider at each split.</li>
</ul></li>
<li><p><strong>Interpretability:</strong> Decision trees are highly interpretable, but complex models like Random Forests can be analyzed using tools like feature importance scores and partial dependence plots.</p>
<ul>
<li><strong>Partial Dependence Plots:</strong> Show the relationship between a feature and the predicted outcome, marginalizing over the values of other features.</li>
</ul></li>
</ul>
</section>
<section id="detailed-examples-and-applications" class="level4">
<h4 class="anchored" data-anchor-id="detailed-examples-and-applications">Detailed Examples and Applications:</h4>
<ol type="1">
<li><p><strong>Building a Decision Tree from Scratch:</strong></p>
<ul>
<li><p><strong>Step 1:</strong> Load the dataset and prepare the data.</p></li>
<li><p><strong>Step 2:</strong> Define the splitting criteria (e.g., Gini impurity or information gain).</p></li>
<li><p><strong>Step 3:</strong> Recursively split the dataset based on the chosen criteria until the stopping condition is met.</p></li>
<li><p><strong>Step 4:</strong> Assign the majority class or mean value of the target variable to the leaf nodes.</p></li>
<li><p><strong>Step 5:</strong> Evaluate the model’s performance on a validation set.</p></li>
</ul></li>
<li><p><strong>Using Decision Trees in Practice:</strong></p>
<ul>
<li><p><strong>Classification Example:</strong> Predicting customer churn based on demographic and usage data.</p></li>
<li><p><strong>Regression Example:</strong> Estimating property prices based on features like location, size, and age.</p></li>
</ul></li>
<li><p><strong>Visualizing Decision Trees:</strong></p>
<ul>
<li><p>Use visualization tools and libraries (e.g., Graphviz, Matplotlib) to plot the structure of decision trees.</p></li>
<li><p><strong>Example:</strong> Visualize the tree structure to understand how the model makes decisions and identify the most important splits.</p></li>
</ul></li>
</ol>
<p>By following these detailed steps and considerations, decision trees can be effectively utilized for various machine learning tasks, providing both predictive power and interpretability.</p>
<div id="bbe14521" class="cell" data-execution_count="11">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split, GridSearchCV</span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.tree <span class="im">import</span> DecisionTreeClassifier, plot_tree</span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> accuracy_score, confusion_matrix, classification_report</span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb28-6"><a href="#cb28-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb28-7"><a href="#cb28-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.impute <span class="im">import</span> SimpleImputer</span>
<span id="cb28-8"><a href="#cb28-8" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.ensemble <span class="im">import</span> RandomForestClassifier</span>
<span id="cb28-9"><a href="#cb28-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-10"><a href="#cb28-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Load the Titanic dataset from Kaggle</span></span>
<span id="cb28-11"><a href="#cb28-11" aria-hidden="true" tabindex="-1"></a>url <span class="op">=</span> <span class="st">'https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv'</span></span>
<span id="cb28-12"><a href="#cb28-12" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> pd.read_csv(url)</span>
<span id="cb28-13"><a href="#cb28-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-14"><a href="#cb28-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Select relevant features and the target variable</span></span>
<span id="cb28-15"><a href="#cb28-15" aria-hidden="true" tabindex="-1"></a>features <span class="op">=</span> [<span class="st">'Pclass'</span>, <span class="st">'Sex'</span>, <span class="st">'Age'</span>, <span class="st">'SibSp'</span>, <span class="st">'Parch'</span>, <span class="st">'Fare'</span>, <span class="st">'Embarked'</span>]</span>
<span id="cb28-16"><a href="#cb28-16" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> data.dropna(subset<span class="op">=</span>[<span class="st">'Embarked'</span>])</span>
<span id="cb28-17"><a href="#cb28-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-18"><a href="#cb28-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert categorical variables to numerical</span></span>
<span id="cb28-19"><a href="#cb28-19" aria-hidden="true" tabindex="-1"></a>data[<span class="st">'Sex'</span>] <span class="op">=</span> data[<span class="st">'Sex'</span>].<span class="bu">map</span>({<span class="st">'male'</span>: <span class="dv">0</span>, <span class="st">'female'</span>: <span class="dv">1</span>})</span>
<span id="cb28-20"><a href="#cb28-20" aria-hidden="true" tabindex="-1"></a>data[<span class="st">'Embarked'</span>] <span class="op">=</span> data[<span class="st">'Embarked'</span>].<span class="bu">map</span>({<span class="st">'C'</span>: <span class="dv">0</span>, <span class="st">'Q'</span>: <span class="dv">1</span>, <span class="st">'S'</span>: <span class="dv">2</span>})</span>
<span id="cb28-21"><a href="#cb28-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-22"><a href="#cb28-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Identify missing values</span></span>
<span id="cb28-23"><a href="#cb28-23" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(data.isnull().<span class="bu">sum</span>())</span>
<span id="cb28-24"><a href="#cb28-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-25"><a href="#cb28-25" aria-hidden="true" tabindex="-1"></a><span class="co"># Imputation: Fill missing values using mean for 'Age' and 'Fare'</span></span>
<span id="cb28-26"><a href="#cb28-26" aria-hidden="true" tabindex="-1"></a>imputer <span class="op">=</span> SimpleImputer(strategy<span class="op">=</span><span class="st">'mean'</span>)</span>
<span id="cb28-27"><a href="#cb28-27" aria-hidden="true" tabindex="-1"></a>data[[<span class="st">'Age'</span>, <span class="st">'Fare'</span>]] <span class="op">=</span> imputer.fit_transform(data[[<span class="st">'Age'</span>, <span class="st">'Fare'</span>]])</span>
<span id="cb28-28"><a href="#cb28-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-29"><a href="#cb28-29" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a missing value indicator for 'Age' and 'Fare'</span></span>
<span id="cb28-30"><a href="#cb28-30" aria-hidden="true" tabindex="-1"></a>data[<span class="st">'Age_missing'</span>] <span class="op">=</span> data[<span class="st">'Age'</span>].isnull().astype(<span class="bu">int</span>)</span>
<span id="cb28-31"><a href="#cb28-31" aria-hidden="true" tabindex="-1"></a>data[<span class="st">'Fare_missing'</span>] <span class="op">=</span> data[<span class="st">'Fare'</span>].isnull().astype(<span class="bu">int</span>)</span>
<span id="cb28-32"><a href="#cb28-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-33"><a href="#cb28-33" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> data[features <span class="op">+</span> [<span class="st">'Age_missing'</span>, <span class="st">'Fare_missing'</span>]]</span>
<span id="cb28-34"><a href="#cb28-34" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> data[<span class="st">'Survived'</span>]</span>
<span id="cb28-35"><a href="#cb28-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-36"><a href="#cb28-36" aria-hidden="true" tabindex="-1"></a><span class="co"># Split the data into training and testing sets</span></span>
<span id="cb28-37"><a href="#cb28-37" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, y, test_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb28-38"><a href="#cb28-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-39"><a href="#cb28-39" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize the Decision Tree model with Grid Search for hyperparameter tuning</span></span>
<span id="cb28-40"><a href="#cb28-40" aria-hidden="true" tabindex="-1"></a>param_grid <span class="op">=</span> {</span>
<span id="cb28-41"><a href="#cb28-41" aria-hidden="true" tabindex="-1"></a>    <span class="st">'max_depth'</span>: [<span class="dv">3</span>, <span class="dv">5</span>, <span class="dv">7</span>, <span class="dv">10</span>],</span>
<span id="cb28-42"><a href="#cb28-42" aria-hidden="true" tabindex="-1"></a>    <span class="st">'min_samples_split'</span>: [<span class="dv">2</span>, <span class="dv">10</span>, <span class="dv">20</span>],</span>
<span id="cb28-43"><a href="#cb28-43" aria-hidden="true" tabindex="-1"></a>    <span class="st">'min_samples_leaf'</span>: [<span class="dv">1</span>, <span class="dv">5</span>, <span class="dv">10</span>]</span>
<span id="cb28-44"><a href="#cb28-44" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb28-45"><a href="#cb28-45" aria-hidden="true" tabindex="-1"></a>grid_search <span class="op">=</span> GridSearchCV(DecisionTreeClassifier(criterion<span class="op">=</span><span class="st">'gini'</span>, random_state<span class="op">=</span><span class="dv">42</span>), param_grid, cv<span class="op">=</span><span class="dv">5</span>, scoring<span class="op">=</span><span class="st">'accuracy'</span>)</span>
<span id="cb28-46"><a href="#cb28-46" aria-hidden="true" tabindex="-1"></a>grid_search.fit(X_train, y_train)</span>
<span id="cb28-47"><a href="#cb28-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-48"><a href="#cb28-48" aria-hidden="true" tabindex="-1"></a><span class="co"># Train the best model</span></span>
<span id="cb28-49"><a href="#cb28-49" aria-hidden="true" tabindex="-1"></a>best_model <span class="op">=</span> grid_search.best_estimator_</span>
<span id="cb28-50"><a href="#cb28-50" aria-hidden="true" tabindex="-1"></a>best_model.fit(X_train, y_train)</span>
<span id="cb28-51"><a href="#cb28-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-52"><a href="#cb28-52" aria-hidden="true" tabindex="-1"></a><span class="co"># Predict the outcomes on the test set</span></span>
<span id="cb28-53"><a href="#cb28-53" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> best_model.predict(X_test)</span>
<span id="cb28-54"><a href="#cb28-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-55"><a href="#cb28-55" aria-hidden="true" tabindex="-1"></a><span class="co"># Evaluate the model</span></span>
<span id="cb28-56"><a href="#cb28-56" aria-hidden="true" tabindex="-1"></a>accuracy <span class="op">=</span> accuracy_score(y_test, y_pred)</span>
<span id="cb28-57"><a href="#cb28-57" aria-hidden="true" tabindex="-1"></a>conf_matrix <span class="op">=</span> confusion_matrix(y_test, y_pred)</span>
<span id="cb28-58"><a href="#cb28-58" aria-hidden="true" tabindex="-1"></a>class_report <span class="op">=</span> classification_report(y_test, y_pred)</span>
<span id="cb28-59"><a href="#cb28-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-60"><a href="#cb28-60" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'Best Model Accuracy: </span><span class="sc">{</span>accuracy <span class="op">*</span> <span class="dv">100</span><span class="sc">:.2f}</span><span class="ss">%'</span>)</span>
<span id="cb28-61"><a href="#cb28-61" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Best Model Confusion Matrix:'</span>)</span>
<span id="cb28-62"><a href="#cb28-62" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(conf_matrix)</span>
<span id="cb28-63"><a href="#cb28-63" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Best Model Classification Report:'</span>)</span>
<span id="cb28-64"><a href="#cb28-64" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(class_report)</span>
<span id="cb28-65"><a href="#cb28-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-66"><a href="#cb28-66" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize the decision tree</span></span>
<span id="cb28-67"><a href="#cb28-67" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">20</span>, <span class="dv">10</span>))</span>
<span id="cb28-68"><a href="#cb28-68" aria-hidden="true" tabindex="-1"></a>plot_tree(best_model, feature_names<span class="op">=</span>X.columns, class_names<span class="op">=</span>[<span class="st">'Not Survived'</span>, <span class="st">'Survived'</span>], filled<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb28-69"><a href="#cb28-69" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Decision Tree with Handling Missing Values'</span>)</span>
<span id="cb28-70"><a href="#cb28-70" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb28-71"><a href="#cb28-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-72"><a href="#cb28-72" aria-hidden="true" tabindex="-1"></a><span class="co"># Train a Random Forest model</span></span>
<span id="cb28-73"><a href="#cb28-73" aria-hidden="true" tabindex="-1"></a>rf_model <span class="op">=</span> RandomForestClassifier(n_estimators<span class="op">=</span><span class="dv">100</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb28-74"><a href="#cb28-74" aria-hidden="true" tabindex="-1"></a>rf_model.fit(X_train, y_train)</span>
<span id="cb28-75"><a href="#cb28-75" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-76"><a href="#cb28-76" aria-hidden="true" tabindex="-1"></a><span class="co"># Predict the outcomes on the test set</span></span>
<span id="cb28-77"><a href="#cb28-77" aria-hidden="true" tabindex="-1"></a>y_pred_rf <span class="op">=</span> rf_model.predict(X_test)</span>
<span id="cb28-78"><a href="#cb28-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-79"><a href="#cb28-79" aria-hidden="true" tabindex="-1"></a><span class="co"># Evaluate the Random Forest model</span></span>
<span id="cb28-80"><a href="#cb28-80" aria-hidden="true" tabindex="-1"></a>accuracy_rf <span class="op">=</span> accuracy_score(y_test, y_pred_rf)</span>
<span id="cb28-81"><a href="#cb28-81" aria-hidden="true" tabindex="-1"></a>conf_matrix_rf <span class="op">=</span> confusion_matrix(y_test, y_pred_rf)</span>
<span id="cb28-82"><a href="#cb28-82" aria-hidden="true" tabindex="-1"></a>class_report_rf <span class="op">=</span> classification_report(y_test, y_pred_rf)</span>
<span id="cb28-83"><a href="#cb28-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-84"><a href="#cb28-84" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'Random Forest Model Accuracy: </span><span class="sc">{</span>accuracy_rf <span class="op">*</span> <span class="dv">100</span><span class="sc">:.2f}</span><span class="ss">%'</span>)</span>
<span id="cb28-85"><a href="#cb28-85" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Random Forest Model Confusion Matrix:'</span>)</span>
<span id="cb28-86"><a href="#cb28-86" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(conf_matrix_rf)</span>
<span id="cb28-87"><a href="#cb28-87" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Random Forest Model Classification Report:'</span>)</span>
<span id="cb28-88"><a href="#cb28-88" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(class_report_rf)</span>
<span id="cb28-89"><a href="#cb28-89" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-90"><a href="#cb28-90" aria-hidden="true" tabindex="-1"></a><span class="co"># Feature Importance in Random Forest</span></span>
<span id="cb28-91"><a href="#cb28-91" aria-hidden="true" tabindex="-1"></a>importances <span class="op">=</span> rf_model.feature_importances_</span>
<span id="cb28-92"><a href="#cb28-92" aria-hidden="true" tabindex="-1"></a>indices <span class="op">=</span> np.argsort(importances)[::<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb28-93"><a href="#cb28-93" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">6</span>))</span>
<span id="cb28-94"><a href="#cb28-94" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Feature Importances'</span>)</span>
<span id="cb28-95"><a href="#cb28-95" aria-hidden="true" tabindex="-1"></a>plt.bar(<span class="bu">range</span>(X.shape[<span class="dv">1</span>]), importances[indices], align<span class="op">=</span><span class="st">'center'</span>)</span>
<span id="cb28-96"><a href="#cb28-96" aria-hidden="true" tabindex="-1"></a>plt.xticks(<span class="bu">range</span>(X.shape[<span class="dv">1</span>]), [X.columns[i] <span class="cf">for</span> i <span class="kw">in</span> indices], rotation<span class="op">=</span><span class="dv">90</span>)</span>
<span id="cb28-97"><a href="#cb28-97" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb28-98"><a href="#cb28-98" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>PassengerId      0
Survived         0
Pclass           0
Name             0
Sex              0
Age            177
SibSp            0
Parch            0
Ticket           0
Fare             0
Cabin          687
Embarked         0
dtype: int64
Best Model Accuracy: 82.02%
Best Model Confusion Matrix:
[[92 17]
 [15 54]]
Best Model Classification Report:
              precision    recall  f1-score   support

           0       0.86      0.84      0.85       109
           1       0.76      0.78      0.77        69

    accuracy                           0.82       178
   macro avg       0.81      0.81      0.81       178
weighted avg       0.82      0.82      0.82       178
</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="chapter4_basic_supervised_learning_algorithms_files/figure-html/cell-12-output-2.png" width="1507" height="778" class="figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Random Forest Model Accuracy: 76.40%
Random Forest Model Confusion Matrix:
[[85 24]
 [18 51]]
Random Forest Model Classification Report:
              precision    recall  f1-score   support

           0       0.83      0.78      0.80       109
           1       0.68      0.74      0.71        69

    accuracy                           0.76       178
   macro avg       0.75      0.76      0.76       178
weighted avg       0.77      0.76      0.77       178
</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="chapter4_basic_supervised_learning_algorithms_files/figure-html/cell-12-output-4.png" width="1142" height="566" class="figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
</section>
</section>
<section id="naive-bayes" class="level1">
<h1>4.5 Naive Bayes</h1>
<p>Naive Bayes is a family of simple probabilistic classifiers based on Bayes’ theorem with strong (naive) independence assumptions between the features. It is highly efficient and effective for various classification tasks.</p>
<section id="gaussian-naive-bayes" class="level2">
<h2 class="anchored" data-anchor-id="gaussian-naive-bayes">4.5.1 Gaussian Naive Bayes</h2>
<p>Gaussian Naive Bayes is used for continuous data that follows a Gaussian (normal) distribution. It assumes that the continuous values associated with each feature are distributed according to a Gaussian distribution.</p>
<ul>
<li><p><strong>Formula:</strong> <span class="math display">\[
P(x_i | y) = \frac{1}{\sqrt{2 \pi \sigma_y^2}} \exp \left( -\frac{(x_i - \mu_y)^2}{2 \sigma_y^2} \right)
\]</span> where <span class="math inline">\(P(x_i | y)\)</span> is the probability of feature <span class="math inline">\(x_i\)</span> given class <span class="math inline">\(y\)</span>, <span class="math inline">\(\mu_y\)</span> is the mean of the feature <span class="math inline">\(x_i\)</span> for class <span class="math inline">\(y\)</span>, and <span class="math inline">\(\sigma_y^2\)</span> is the variance of the feature <span class="math inline">\(x_i\)</span> for class <span class="math inline">\(y\)</span>.</p></li>
<li><p><strong>Example:</strong> Classifying iris flowers based on petal and sepal measurements.</p></li>
</ul>
<section id="steps" class="level4">
<h4 class="anchored" data-anchor-id="steps">Steps:</h4>
<ol type="1">
<li><p><strong>Calculate the mean and variance:</strong> For each feature in the dataset, calculate the mean and variance for each class.</p></li>
<li><p><strong>Apply the Gaussian formula:</strong> Use the Gaussian probability density function to calculate the probability of each feature given the class.</p></li>
<li><p><strong>Compute the posterior probability:</strong> Use Bayes’ theorem to compute the posterior probability for each class.</p></li>
<li><p><strong>Predict the class:</strong> Choose the class with the highest posterior probability.</p></li>
</ol>
<div id="57c4540e" class="cell" data-execution_count="12">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.naive_bayes <span class="im">import</span> GaussianNB</span>
<span id="cb31-5"><a href="#cb31-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> accuracy_score, confusion_matrix, classification_report</span>
<span id="cb31-6"><a href="#cb31-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb31-7"><a href="#cb31-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb31-8"><a href="#cb31-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-9"><a href="#cb31-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Load the Iris dataset</span></span>
<span id="cb31-10"><a href="#cb31-10" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> load_iris</span>
<span id="cb31-11"><a href="#cb31-11" aria-hidden="true" tabindex="-1"></a>iris <span class="op">=</span> load_iris()</span>
<span id="cb31-12"><a href="#cb31-12" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> pd.DataFrame(data<span class="op">=</span>iris.data, columns<span class="op">=</span>iris.feature_names)</span>
<span id="cb31-13"><a href="#cb31-13" aria-hidden="true" tabindex="-1"></a>data[<span class="st">'target'</span>] <span class="op">=</span> iris.target</span>
<span id="cb31-14"><a href="#cb31-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-15"><a href="#cb31-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Display the first few rows of the dataset</span></span>
<span id="cb31-16"><a href="#cb31-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(data.head())</span>
<span id="cb31-17"><a href="#cb31-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-18"><a href="#cb31-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Feature matrix and target vector</span></span>
<span id="cb31-19"><a href="#cb31-19" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> data.drop(<span class="st">'target'</span>, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb31-20"><a href="#cb31-20" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> data[<span class="st">'target'</span>]</span>
<span id="cb31-21"><a href="#cb31-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-22"><a href="#cb31-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Split the data into training and testing sets</span></span>
<span id="cb31-23"><a href="#cb31-23" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, y, test_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb31-24"><a href="#cb31-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-25"><a href="#cb31-25" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize and train the Gaussian Naive Bayes model</span></span>
<span id="cb31-26"><a href="#cb31-26" aria-hidden="true" tabindex="-1"></a>gnb <span class="op">=</span> GaussianNB()</span>
<span id="cb31-27"><a href="#cb31-27" aria-hidden="true" tabindex="-1"></a>gnb.fit(X_train, y_train)</span>
<span id="cb31-28"><a href="#cb31-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-29"><a href="#cb31-29" aria-hidden="true" tabindex="-1"></a><span class="co"># Make predictions</span></span>
<span id="cb31-30"><a href="#cb31-30" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> gnb.predict(X_test)</span>
<span id="cb31-31"><a href="#cb31-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-32"><a href="#cb31-32" aria-hidden="true" tabindex="-1"></a><span class="co"># Evaluate the model</span></span>
<span id="cb31-33"><a href="#cb31-33" aria-hidden="true" tabindex="-1"></a>accuracy <span class="op">=</span> accuracy_score(y_test, y_pred)</span>
<span id="cb31-34"><a href="#cb31-34" aria-hidden="true" tabindex="-1"></a>conf_matrix <span class="op">=</span> confusion_matrix(y_test, y_pred)</span>
<span id="cb31-35"><a href="#cb31-35" aria-hidden="true" tabindex="-1"></a>class_report <span class="op">=</span> classification_report(y_test, y_pred)</span>
<span id="cb31-36"><a href="#cb31-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-37"><a href="#cb31-37" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'Accuracy: </span><span class="sc">{</span>accuracy<span class="sc">:.2f}</span><span class="ss">'</span>)</span>
<span id="cb31-38"><a href="#cb31-38" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Confusion Matrix:'</span>)</span>
<span id="cb31-39"><a href="#cb31-39" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(conf_matrix)</span>
<span id="cb31-40"><a href="#cb31-40" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Classification Report:'</span>)</span>
<span id="cb31-41"><a href="#cb31-41" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(class_report)</span>
<span id="cb31-42"><a href="#cb31-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-43"><a href="#cb31-43" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize the confusion matrix</span></span>
<span id="cb31-44"><a href="#cb31-44" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">6</span>))</span>
<span id="cb31-45"><a href="#cb31-45" aria-hidden="true" tabindex="-1"></a>sns.heatmap(conf_matrix, annot<span class="op">=</span><span class="va">True</span>, fmt<span class="op">=</span><span class="st">'d'</span>, cmap<span class="op">=</span><span class="st">'Blues'</span>, xticklabels<span class="op">=</span>iris.target_names, yticklabels<span class="op">=</span>iris.target_names)</span>
<span id="cb31-46"><a href="#cb31-46" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Predicted'</span>)</span>
<span id="cb31-47"><a href="#cb31-47" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Actual'</span>)</span>
<span id="cb31-48"><a href="#cb31-48" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Confusion Matrix for Gaussian Naive Bayes'</span>)</span>
<span id="cb31-49"><a href="#cb31-49" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \
0                5.1               3.5                1.4               0.2   
1                4.9               3.0                1.4               0.2   
2                4.7               3.2                1.3               0.2   
3                4.6               3.1                1.5               0.2   
4                5.0               3.6                1.4               0.2   

   target  
0       0  
1       0  
2       0  
3       0  
4       0  
Accuracy: 1.00
Confusion Matrix:
[[10  0  0]
 [ 0  9  0]
 [ 0  0 11]]
Classification Report:
              precision    recall  f1-score   support

           0       1.00      1.00      1.00        10
           1       1.00      1.00      1.00         9
           2       1.00      1.00      1.00        11

    accuracy                           1.00        30
   macro avg       1.00      1.00      1.00        30
weighted avg       1.00      1.00      1.00        30
</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="chapter4_basic_supervised_learning_algorithms_files/figure-html/cell-13-output-2.png" width="741" height="523" class="figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="multinomial-naive-bayes" class="level2">
<h2 class="anchored" data-anchor-id="multinomial-naive-bayes">4.5.2 Multinomial Naive Bayes</h2>
<p>Multinomial Naive Bayes is suitable for discrete data, especially for text classification where the data can be represented as word counts or term frequencies.</p>
<ul>
<li><p><strong>Formula:</strong> <span class="math display">\[
P(x_i | y) = \frac{\text{Count}(x_i, y) + 1}{\sum_{i} \text{Count}(x_i, y) + n}
\]</span> where <span class="math inline">\(P(x_i | y)\)</span> is the probability of feature <span class="math inline">\(x_i\)</span> given class <span class="math inline">\(y\)</span>, <span class="math inline">\(\text{Count}(x_i, y)\)</span> is the count of feature <span class="math inline">\(x_i\)</span> in class <span class="math inline">\(y\)</span>, and <span class="math inline">\(n\)</span> is the total number of features.</p></li>
<li><p><strong>Example:</strong> Classifying emails as spam or not spam based on the frequency of words.</p></li>
</ul>
<section id="steps-1" class="level4">
<h4 class="anchored" data-anchor-id="steps-1">Steps:</h4>
<ol type="1">
<li><p><strong>Count the occurrences:</strong> Count the occurrences of each word in the training set for each class.</p></li>
<li><p><strong>Calculate the probability:</strong> Calculate the probability of each word given the class using the formula above.</p></li>
<li><p><strong>Compute the posterior probability:</strong> Use Bayes’ theorem to compute the posterior probability for each class.</p></li>
<li><p><strong>Predict the class:</strong> Choose the class with the highest posterior probability.</p></li>
</ol>
<div id="99fc938b" class="cell" data-execution_count="13">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb33-4"><a href="#cb33-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.feature_extraction.text <span class="im">import</span> CountVectorizer</span>
<span id="cb33-5"><a href="#cb33-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.naive_bayes <span class="im">import</span> MultinomialNB</span>
<span id="cb33-6"><a href="#cb33-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> accuracy_score, confusion_matrix, classification_report</span>
<span id="cb33-7"><a href="#cb33-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb33-8"><a href="#cb33-8" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb33-9"><a href="#cb33-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-10"><a href="#cb33-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate a simple dataset for spam classification</span></span>
<span id="cb33-11"><a href="#cb33-11" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> {</span>
<span id="cb33-12"><a href="#cb33-12" aria-hidden="true" tabindex="-1"></a>    <span class="st">'text'</span>: [</span>
<span id="cb33-13"><a href="#cb33-13" aria-hidden="true" tabindex="-1"></a>        <span class="st">'Free money now'</span>, <span class="st">'Call this number for a free prize'</span>, <span class="st">'Hello, how are you?'</span>,</span>
<span id="cb33-14"><a href="#cb33-14" aria-hidden="true" tabindex="-1"></a>        <span class="st">'Win a free vacation'</span>, <span class="st">'Can we schedule a meeting?'</span>, <span class="st">'You have won a lottery'</span>,</span>
<span id="cb33-15"><a href="#cb33-15" aria-hidden="true" tabindex="-1"></a>        <span class="st">'This is not spam, just checking in'</span>, <span class="st">'Free entry in a contest'</span>, <span class="st">'Cheap loans available'</span>,</span>
<span id="cb33-16"><a href="#cb33-16" aria-hidden="true" tabindex="-1"></a>        <span class="st">'Are you coming to the party?'</span>, <span class="st">'Earn extra cash easily'</span>, <span class="st">'Your loan is approved'</span>,</span>
<span id="cb33-17"><a href="#cb33-17" aria-hidden="true" tabindex="-1"></a>        <span class="st">'Dinner tonight?'</span>, <span class="st">'Lowest price guarantee'</span>, <span class="st">'Let’s catch up soon'</span></span>
<span id="cb33-18"><a href="#cb33-18" aria-hidden="true" tabindex="-1"></a>    ],</span>
<span id="cb33-19"><a href="#cb33-19" aria-hidden="true" tabindex="-1"></a>    <span class="st">'label'</span>: [<span class="st">'spam'</span>, <span class="st">'spam'</span>, <span class="st">'not spam'</span>, <span class="st">'spam'</span>, <span class="st">'not spam'</span>, <span class="st">'spam'</span>, <span class="st">'not spam'</span>, <span class="st">'spam'</span>, <span class="st">'spam'</span>,</span>
<span id="cb33-20"><a href="#cb33-20" aria-hidden="true" tabindex="-1"></a>              <span class="st">'not spam'</span>, <span class="st">'spam'</span>, <span class="st">'spam'</span>, <span class="st">'not spam'</span>, <span class="st">'spam'</span>, <span class="st">'not spam'</span>]</span>
<span id="cb33-21"><a href="#cb33-21" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb33-22"><a href="#cb33-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-23"><a href="#cb33-23" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.DataFrame(data)</span>
<span id="cb33-24"><a href="#cb33-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-25"><a href="#cb33-25" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert labels to binary values</span></span>
<span id="cb33-26"><a href="#cb33-26" aria-hidden="true" tabindex="-1"></a>df[<span class="st">'label'</span>] <span class="op">=</span> df[<span class="st">'label'</span>].<span class="bu">map</span>({<span class="st">'spam'</span>: <span class="dv">1</span>, <span class="st">'not spam'</span>: <span class="dv">0</span>})</span>
<span id="cb33-27"><a href="#cb33-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-28"><a href="#cb33-28" aria-hidden="true" tabindex="-1"></a><span class="co"># Split the data into training and testing sets</span></span>
<span id="cb33-29"><a href="#cb33-29" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(df[<span class="st">'text'</span>], df[<span class="st">'label'</span>], test_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb33-30"><a href="#cb33-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-31"><a href="#cb33-31" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert the text data to word count vectors</span></span>
<span id="cb33-32"><a href="#cb33-32" aria-hidden="true" tabindex="-1"></a>vectorizer <span class="op">=</span> CountVectorizer()</span>
<span id="cb33-33"><a href="#cb33-33" aria-hidden="true" tabindex="-1"></a>X_train_counts <span class="op">=</span> vectorizer.fit_transform(X_train)</span>
<span id="cb33-34"><a href="#cb33-34" aria-hidden="true" tabindex="-1"></a>X_test_counts <span class="op">=</span> vectorizer.transform(X_test)</span>
<span id="cb33-35"><a href="#cb33-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-36"><a href="#cb33-36" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize and train the Multinomial Naive Bayes model</span></span>
<span id="cb33-37"><a href="#cb33-37" aria-hidden="true" tabindex="-1"></a>mnb <span class="op">=</span> MultinomialNB()</span>
<span id="cb33-38"><a href="#cb33-38" aria-hidden="true" tabindex="-1"></a>mnb.fit(X_train_counts, y_train)</span>
<span id="cb33-39"><a href="#cb33-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-40"><a href="#cb33-40" aria-hidden="true" tabindex="-1"></a><span class="co"># Make predictions</span></span>
<span id="cb33-41"><a href="#cb33-41" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> mnb.predict(X_test_counts)</span>
<span id="cb33-42"><a href="#cb33-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-43"><a href="#cb33-43" aria-hidden="true" tabindex="-1"></a><span class="co"># Evaluate the model</span></span>
<span id="cb33-44"><a href="#cb33-44" aria-hidden="true" tabindex="-1"></a>accuracy <span class="op">=</span> accuracy_score(y_test, y_pred)</span>
<span id="cb33-45"><a href="#cb33-45" aria-hidden="true" tabindex="-1"></a>conf_matrix <span class="op">=</span> confusion_matrix(y_test, y_pred)</span>
<span id="cb33-46"><a href="#cb33-46" aria-hidden="true" tabindex="-1"></a>class_report <span class="op">=</span> classification_report(y_test, y_pred)</span>
<span id="cb33-47"><a href="#cb33-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-48"><a href="#cb33-48" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'Accuracy: </span><span class="sc">{</span>accuracy<span class="sc">:.2f}</span><span class="ss">'</span>)</span>
<span id="cb33-49"><a href="#cb33-49" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Confusion Matrix:'</span>)</span>
<span id="cb33-50"><a href="#cb33-50" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(conf_matrix)</span>
<span id="cb33-51"><a href="#cb33-51" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Classification Report:'</span>)</span>
<span id="cb33-52"><a href="#cb33-52" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(class_report)</span>
<span id="cb33-53"><a href="#cb33-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-54"><a href="#cb33-54" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize the confusion matrix</span></span>
<span id="cb33-55"><a href="#cb33-55" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">6</span>))</span>
<span id="cb33-56"><a href="#cb33-56" aria-hidden="true" tabindex="-1"></a>sns.heatmap(conf_matrix, annot<span class="op">=</span><span class="va">True</span>, fmt<span class="op">=</span><span class="st">'d'</span>, cmap<span class="op">=</span><span class="st">'Blues'</span>, xticklabels<span class="op">=</span>[<span class="st">'Not Spam'</span>, <span class="st">'Spam'</span>], yticklabels<span class="op">=</span>[<span class="st">'Not Spam'</span>, <span class="st">'Spam'</span>])</span>
<span id="cb33-57"><a href="#cb33-57" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Predicted'</span>)</span>
<span id="cb33-58"><a href="#cb33-58" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Actual'</span>)</span>
<span id="cb33-59"><a href="#cb33-59" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Confusion Matrix for Multinomial Naive Bayes'</span>)</span>
<span id="cb33-60"><a href="#cb33-60" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Accuracy: 0.67
Confusion Matrix:
[[1 0]
 [1 1]]
Classification Report:
              precision    recall  f1-score   support

           0       0.50      1.00      0.67         1
           1       1.00      0.50      0.67         2

    accuracy                           0.67         3
   macro avg       0.75      0.75      0.67         3
weighted avg       0.83      0.67      0.67         3
</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="chapter4_basic_supervised_learning_algorithms_files/figure-html/cell-14-output-2.png" width="745" height="523" class="figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="bernoulli-naive-bayes" class="level2">
<h2 class="anchored" data-anchor-id="bernoulli-naive-bayes">4.5.3 Bernoulli Naive Bayes</h2>
<p>Bernoulli Naive Bayes is used for binary/boolean features. It assumes that features are binary (0 or 1) and models the presence or absence of features.</p>
<ul>
<li><p><strong>Formula:</strong> <span class="math display">\[
P(x_i | y) = p_i^{x_i} (1 - p_i)^{1 - x_i}
\]</span> where <span class="math inline">\(P(x_i | y)\)</span> is the probability of feature <span class="math inline">\(x_i\)</span> given class <span class="math inline">\(y\)</span>, and <span class="math inline">\(p_i\)</span> is the probability of feature <span class="math inline">\(x_i\)</span> occurring in class <span class="math inline">\(y\)</span>.</p></li>
<li><p><strong>Example:</strong> Classifying documents based on the presence or absence of specific words.</p></li>
</ul>
<section id="steps-2" class="level4">
<h4 class="anchored" data-anchor-id="steps-2">Steps:</h4>
<ol type="1">
<li><p><strong>Calculate the probabilities:</strong> For each feature, calculate the probability of it being present (1) or absent (0) for each class.</p></li>
<li><p><strong>Apply the Bernoulli formula:</strong> Use the Bernoulli distribution to calculate the probability of each feature given the class.</p></li>
<li><p><strong>Compute the posterior probability:</strong> Use Bayes’ theorem to compute the posterior probability for each class.</p></li>
<li><p><strong>Predict the class:</strong> Choose the class with the highest posterior probability.</p></li>
</ol>
<div id="fc66d678" class="cell" data-execution_count="14">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb35-4"><a href="#cb35-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.feature_extraction.text <span class="im">import</span> CountVectorizer</span>
<span id="cb35-5"><a href="#cb35-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.naive_bayes <span class="im">import</span> BernoulliNB</span>
<span id="cb35-6"><a href="#cb35-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> accuracy_score, confusion_matrix, classification_report</span>
<span id="cb35-7"><a href="#cb35-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-8"><a href="#cb35-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Sample text data</span></span>
<span id="cb35-9"><a href="#cb35-9" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> {</span>
<span id="cb35-10"><a href="#cb35-10" aria-hidden="true" tabindex="-1"></a>    <span class="st">'text'</span>: [</span>
<span id="cb35-11"><a href="#cb35-11" aria-hidden="true" tabindex="-1"></a>        <span class="st">'I love programming in Python'</span>,</span>
<span id="cb35-12"><a href="#cb35-12" aria-hidden="true" tabindex="-1"></a>        <span class="st">'Python is an excellent programming language'</span>,</span>
<span id="cb35-13"><a href="#cb35-13" aria-hidden="true" tabindex="-1"></a>        <span class="st">'I hate bugs in the code'</span>,</span>
<span id="cb35-14"><a href="#cb35-14" aria-hidden="true" tabindex="-1"></a>        <span class="st">'Debugging code can be challenging'</span>,</span>
<span id="cb35-15"><a href="#cb35-15" aria-hidden="true" tabindex="-1"></a>        <span class="st">'I love solving problems using Python'</span>,</span>
<span id="cb35-16"><a href="#cb35-16" aria-hidden="true" tabindex="-1"></a>        <span class="st">'I hate syntax errors'</span></span>
<span id="cb35-17"><a href="#cb35-17" aria-hidden="true" tabindex="-1"></a>    ],</span>
<span id="cb35-18"><a href="#cb35-18" aria-hidden="true" tabindex="-1"></a>    <span class="st">'label'</span>: [<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">0</span>]  <span class="co"># 1: Positive, 0: Negative</span></span>
<span id="cb35-19"><a href="#cb35-19" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb35-20"><a href="#cb35-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-21"><a href="#cb35-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert to DataFrame</span></span>
<span id="cb35-22"><a href="#cb35-22" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.DataFrame(data)</span>
<span id="cb35-23"><a href="#cb35-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-24"><a href="#cb35-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 1: Vectorize the text data (convert text to binary features)</span></span>
<span id="cb35-25"><a href="#cb35-25" aria-hidden="true" tabindex="-1"></a>vectorizer <span class="op">=</span> CountVectorizer(binary<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb35-26"><a href="#cb35-26" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> vectorizer.fit_transform(df[<span class="st">'text'</span>]).toarray()</span>
<span id="cb35-27"><a href="#cb35-27" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> df[<span class="st">'label'</span>]</span>
<span id="cb35-28"><a href="#cb35-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-29"><a href="#cb35-29" aria-hidden="true" tabindex="-1"></a><span class="co"># Split the data into training and testing sets</span></span>
<span id="cb35-30"><a href="#cb35-30" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, y, test_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb35-31"><a href="#cb35-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-32"><a href="#cb35-32" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 2: Train Bernoulli Naive Bayes model</span></span>
<span id="cb35-33"><a href="#cb35-33" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> BernoulliNB()</span>
<span id="cb35-34"><a href="#cb35-34" aria-hidden="true" tabindex="-1"></a>model.fit(X_train, y_train)</span>
<span id="cb35-35"><a href="#cb35-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-36"><a href="#cb35-36" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 3: Predict the outcomes on the test set</span></span>
<span id="cb35-37"><a href="#cb35-37" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> model.predict(X_test)</span>
<span id="cb35-38"><a href="#cb35-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-39"><a href="#cb35-39" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 4: Evaluate the model</span></span>
<span id="cb35-40"><a href="#cb35-40" aria-hidden="true" tabindex="-1"></a>accuracy <span class="op">=</span> accuracy_score(y_test, y_pred)</span>
<span id="cb35-41"><a href="#cb35-41" aria-hidden="true" tabindex="-1"></a>conf_matrix <span class="op">=</span> confusion_matrix(y_test, y_pred)</span>
<span id="cb35-42"><a href="#cb35-42" aria-hidden="true" tabindex="-1"></a>class_report <span class="op">=</span> classification_report(y_test, y_pred)</span>
<span id="cb35-43"><a href="#cb35-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-44"><a href="#cb35-44" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'Model Accuracy: </span><span class="sc">{</span>accuracy <span class="op">*</span> <span class="dv">100</span><span class="sc">:.2f}</span><span class="ss">%'</span>)</span>
<span id="cb35-45"><a href="#cb35-45" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Confusion Matrix:'</span>)</span>
<span id="cb35-46"><a href="#cb35-46" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(conf_matrix)</span>
<span id="cb35-47"><a href="#cb35-47" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Classification Report:'</span>)</span>
<span id="cb35-48"><a href="#cb35-48" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(class_report)</span>
<span id="cb35-49"><a href="#cb35-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-50"><a href="#cb35-50" aria-hidden="true" tabindex="-1"></a><span class="co"># Advanced: Show the presence/absence matrix for the features</span></span>
<span id="cb35-51"><a href="#cb35-51" aria-hidden="true" tabindex="-1"></a>feature_names <span class="op">=</span> vectorizer.get_feature_names_out()</span>
<span id="cb35-52"><a href="#cb35-52" aria-hidden="true" tabindex="-1"></a>presence_absence_matrix <span class="op">=</span> pd.DataFrame(X, columns<span class="op">=</span>feature_names)</span>
<span id="cb35-53"><a href="#cb35-53" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Presence/Absence Matrix:'</span>)</span>
<span id="cb35-54"><a href="#cb35-54" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(presence_absence_matrix)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Model Accuracy: 100.00%
Confusion Matrix:
[[2]]
Classification Report:
              precision    recall  f1-score   support

           1       1.00      1.00      1.00         2

    accuracy                           1.00         2
   macro avg       1.00      1.00      1.00         2
weighted avg       1.00      1.00      1.00         2

Presence/Absence Matrix:
   an  be  bugs  can  challenging  code  debugging  errors  excellent  hate  \
0   0   0     0    0            0     0          0       0          0     0   
1   1   0     0    0            0     0          0       0          1     0   
2   0   0     1    0            0     1          0       0          0     1   
3   0   1     0    1            1     1          1       0          0     0   
4   0   0     0    0            0     0          0       0          0     0   
5   0   0     0    0            0     0          0       1          0     1   

   ...  is  language  love  problems  programming  python  solving  syntax  \
0  ...   0         0     1         0            1       1        0       0   
1  ...   1         1     0         0            1       1        0       0   
2  ...   0         0     0         0            0       0        0       0   
3  ...   0         0     0         0            0       0        0       0   
4  ...   0         0     1         1            0       1        1       0   
5  ...   0         0     0         0            0       0        0       1   

   the  using  
0    0      0  
1    0      0  
2    1      0  
3    0      0  
4    0      1  
5    0      0  

[6 rows x 21 columns]</code></pre>
</div>
</div>
</section>
</section>
<section id="complement-naive-bayes" class="level2">
<h2 class="anchored" data-anchor-id="complement-naive-bayes">4.5.4 Complement Naive Bayes</h2>
<p>Complement Naive Bayes is designed to address the imbalance issue of Multinomial Naive Bayes by taking into account the complement of each class. It is particularly useful for text classification with imbalanced data.</p>
<ul>
<li><p><strong>Formula:</strong> <span class="math display">\[
P(x_i | y) = \frac{\text{Count}(x_i, \text{not } y) + 1}{\sum_{i} \text{Count}(x_i, \text{not } y) + n}
\]</span> where <span class="math inline">\(P(x_i | y)\)</span> is the probability of feature <span class="math inline">\(x_i\)</span> given the complement of class <span class="math inline">\(y\)</span>, and <span class="math inline">\(\text{Count}(x_i, \text{not } y)\)</span> is the count of feature <span class="math inline">\(x_i\)</span> in all classes except <span class="math inline">\(y\)</span>.</p></li>
<li><p><strong>Example:</strong> Classifying news articles into different categories, handling imbalanced categories effectively.</p></li>
</ul>
<section id="steps-3" class="level4">
<h4 class="anchored" data-anchor-id="steps-3">Steps:</h4>
<ol type="1">
<li><p><strong>Calculate the complement counts:</strong> Count the occurrences of each word in the training set for all classes except the one being considered.</p></li>
<li><p><strong>Calculate the probability:</strong> Calculate the probability of each word given the complement of the class using the formula above.</p></li>
<li><p><strong>Compute the posterior probability:</strong> Use Bayes’ theorem to compute the posterior probability for each class.</p></li>
<li><p><strong>Predict the class:</strong> Choose the class with the highest posterior probability.</p></li>
</ol>
<div id="9fada77c" class="cell" data-execution_count="15">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb37-3"><a href="#cb37-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb37-4"><a href="#cb37-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.feature_extraction.text <span class="im">import</span> CountVectorizer</span>
<span id="cb37-5"><a href="#cb37-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.naive_bayes <span class="im">import</span> ComplementNB</span>
<span id="cb37-6"><a href="#cb37-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> accuracy_score, confusion_matrix, classification_report</span>
<span id="cb37-7"><a href="#cb37-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-8"><a href="#cb37-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Sample text data</span></span>
<span id="cb37-9"><a href="#cb37-9" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> {</span>
<span id="cb37-10"><a href="#cb37-10" aria-hidden="true" tabindex="-1"></a>    <span class="st">'text'</span>: [</span>
<span id="cb37-11"><a href="#cb37-11" aria-hidden="true" tabindex="-1"></a>        <span class="st">'I love programming in Python'</span>,</span>
<span id="cb37-12"><a href="#cb37-12" aria-hidden="true" tabindex="-1"></a>        <span class="st">'Python is an excellent programming language'</span>,</span>
<span id="cb37-13"><a href="#cb37-13" aria-hidden="true" tabindex="-1"></a>        <span class="st">'I hate bugs in the code'</span>,</span>
<span id="cb37-14"><a href="#cb37-14" aria-hidden="true" tabindex="-1"></a>        <span class="st">'Debugging code can be challenging'</span>,</span>
<span id="cb37-15"><a href="#cb37-15" aria-hidden="true" tabindex="-1"></a>        <span class="st">'I love solving problems using Python'</span>,</span>
<span id="cb37-16"><a href="#cb37-16" aria-hidden="true" tabindex="-1"></a>        <span class="st">'I hate syntax errors'</span></span>
<span id="cb37-17"><a href="#cb37-17" aria-hidden="true" tabindex="-1"></a>    ],</span>
<span id="cb37-18"><a href="#cb37-18" aria-hidden="true" tabindex="-1"></a>    <span class="st">'label'</span>: [<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">0</span>]  <span class="co"># 1: Positive, 0: Negative</span></span>
<span id="cb37-19"><a href="#cb37-19" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb37-20"><a href="#cb37-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-21"><a href="#cb37-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert to DataFrame</span></span>
<span id="cb37-22"><a href="#cb37-22" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.DataFrame(data)</span>
<span id="cb37-23"><a href="#cb37-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-24"><a href="#cb37-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 1: Vectorize the text data (convert text to binary features)</span></span>
<span id="cb37-25"><a href="#cb37-25" aria-hidden="true" tabindex="-1"></a>vectorizer <span class="op">=</span> CountVectorizer()</span>
<span id="cb37-26"><a href="#cb37-26" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> vectorizer.fit_transform(df[<span class="st">'text'</span>]).toarray()</span>
<span id="cb37-27"><a href="#cb37-27" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> df[<span class="st">'label'</span>]</span>
<span id="cb37-28"><a href="#cb37-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-29"><a href="#cb37-29" aria-hidden="true" tabindex="-1"></a><span class="co"># Split the data into training and testing sets</span></span>
<span id="cb37-30"><a href="#cb37-30" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, y, test_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb37-31"><a href="#cb37-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-32"><a href="#cb37-32" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 2: Train Complement Naive Bayes model</span></span>
<span id="cb37-33"><a href="#cb37-33" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> ComplementNB()</span>
<span id="cb37-34"><a href="#cb37-34" aria-hidden="true" tabindex="-1"></a>model.fit(X_train, y_train)</span>
<span id="cb37-35"><a href="#cb37-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-36"><a href="#cb37-36" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 3: Predict the outcomes on the test set</span></span>
<span id="cb37-37"><a href="#cb37-37" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> model.predict(X_test)</span>
<span id="cb37-38"><a href="#cb37-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-39"><a href="#cb37-39" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 4: Evaluate the model</span></span>
<span id="cb37-40"><a href="#cb37-40" aria-hidden="true" tabindex="-1"></a>accuracy <span class="op">=</span> accuracy_score(y_test, y_pred)</span>
<span id="cb37-41"><a href="#cb37-41" aria-hidden="true" tabindex="-1"></a>conf_matrix <span class="op">=</span> confusion_matrix(y_test, y_pred)</span>
<span id="cb37-42"><a href="#cb37-42" aria-hidden="true" tabindex="-1"></a>class_report <span class="op">=</span> classification_report(y_test, y_pred)</span>
<span id="cb37-43"><a href="#cb37-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-44"><a href="#cb37-44" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'Model Accuracy: </span><span class="sc">{</span>accuracy <span class="op">*</span> <span class="dv">100</span><span class="sc">:.2f}</span><span class="ss">%'</span>)</span>
<span id="cb37-45"><a href="#cb37-45" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Confusion Matrix:'</span>)</span>
<span id="cb37-46"><a href="#cb37-46" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(conf_matrix)</span>
<span id="cb37-47"><a href="#cb37-47" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Classification Report:'</span>)</span>
<span id="cb37-48"><a href="#cb37-48" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(class_report)</span>
<span id="cb37-49"><a href="#cb37-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-50"><a href="#cb37-50" aria-hidden="true" tabindex="-1"></a><span class="co"># Advanced: Show the word counts and probabilities for each class complement</span></span>
<span id="cb37-51"><a href="#cb37-51" aria-hidden="true" tabindex="-1"></a>feature_names <span class="op">=</span> vectorizer.get_feature_names_out()</span>
<span id="cb37-52"><a href="#cb37-52" aria-hidden="true" tabindex="-1"></a>class_complement_counts <span class="op">=</span> model.feature_count_</span>
<span id="cb37-53"><a href="#cb37-53" aria-hidden="true" tabindex="-1"></a>class_complement_log_probs <span class="op">=</span> model.feature_log_prob_</span>
<span id="cb37-54"><a href="#cb37-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-55"><a href="#cb37-55" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Word Counts for Each Class Complement:'</span>)</span>
<span id="cb37-56"><a href="#cb37-56" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(pd.DataFrame(class_complement_counts, columns<span class="op">=</span>feature_names))</span>
<span id="cb37-57"><a href="#cb37-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-58"><a href="#cb37-58" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Log Probabilities for Each Class Complement:'</span>)</span>
<span id="cb37-59"><a href="#cb37-59" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(pd.DataFrame(class_complement_log_probs, columns<span class="op">=</span>feature_names))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Model Accuracy: 100.00%
Confusion Matrix:
[[2]]
Classification Report:
              precision    recall  f1-score   support

           1       1.00      1.00      1.00         2

    accuracy                           1.00         2
   macro avg       1.00      1.00      1.00         2
weighted avg       1.00      1.00      1.00         2

Word Counts for Each Class Complement:
    an   be  bugs  can  challenging  code  debugging  errors  excellent  hate  \
0  0.0  1.0   1.0  1.0          1.0   2.0        1.0     1.0        0.0   2.0   
1  0.0  0.0   0.0  0.0          0.0   0.0        0.0     0.0        0.0   0.0   

   ...   is  language  love  problems  programming  python  solving  syntax  \
0  ...  0.0       0.0   0.0       0.0          0.0     0.0      0.0     1.0   
1  ...  0.0       0.0   1.0       1.0          0.0     1.0      1.0     0.0   

   the  using  
0  1.0    0.0  
1  0.0    1.0  

[2 rows x 21 columns]
Log Probabilities for Each Class Complement:
         an        be      bugs       can  challenging      code  debugging  \
0  3.258097  3.258097  3.258097  3.258097     3.258097  3.258097   3.258097   
1  3.526361  2.833213  2.833213  2.833213     2.833213  2.427748   2.833213   

     errors  excellent      hate  ...        is  language      love  problems  \
0  3.258097   3.258097  3.258097  ...  3.258097  3.258097  2.564949  2.564949   
1  2.833213   3.526361  2.427748  ...  3.526361  3.526361  3.526361  3.526361   

   programming    python   solving    syntax       the     using  
0     3.258097  2.564949  2.564949  3.258097  3.258097  2.564949  
1     3.526361  3.526361  3.526361  2.833213  2.833213  3.526361  

[2 rows x 21 columns]</code></pre>
</div>
</div>
</section>
</section>
<section id="handling-continuous-features" class="level2">
<h2 class="anchored" data-anchor-id="handling-continuous-features">4.5.5 Handling Continuous Features</h2>
<p>Naive Bayes algorithms typically handle continuous features by assuming they follow a Gaussian distribution (as in Gaussian Naive Bayes). However, other techniques can be used to handle continuous features effectively.</p>
<section id="methods" class="level4">
<h4 class="anchored" data-anchor-id="methods">Methods:</h4>
<ol type="1">
<li><p><strong>Discretization:</strong> Convert continuous features into discrete bins.</p>
<ul>
<li><strong>Example:</strong> Convert age into age groups (e.g., 0-10, 11-20, etc.).</li>
</ul></li>
<li><p><strong>Gaussian Naive Bayes:</strong> Assume continuous features follow a Gaussian distribution.</p>
<ul>
<li><strong>Example:</strong> Use the Gaussian probability density function to model height or weight.</li>
</ul></li>
<li><p><strong>Kernel Density Estimation (KDE):</strong> Estimate the probability density function of the continuous features non-parametrically.</p>
<ul>
<li><strong>Example:</strong> Use KDE to model the distribution of continuous features more flexibly.</li>
</ul></li>
<li><p><strong>Quantile Transformation:</strong> Transform continuous features to follow a uniform or normal distribution.</p>
<ul>
<li><strong>Example:</strong> Use quantile transformation to make continuous features more suitable for Naive Bayes.</li>
</ul></li>
</ol>
</section>
<section id="steps-for-handling-continuous-features" class="level4">
<h4 class="anchored" data-anchor-id="steps-for-handling-continuous-features">Steps for Handling Continuous Features:</h4>
<ol type="1">
<li><p><strong>Choose a method:</strong> Decide whether to discretize, use Gaussian assumptions, apply KDE, or perform quantile transformation.</p></li>
<li><p><strong>Transform the features:</strong> Apply the chosen method to transform continuous features.</p></li>
<li><p><strong>Incorporate into Naive Bayes model:</strong> Use the transformed features in the Naive Bayes algorithm.</p></li>
</ol>
<p>Advanced considerations in Naive Bayes include:</p>
<ul>
<li><p><strong>Feature Independence:</strong> Although Naive Bayes assumes feature independence, it often performs well even when this assumption is violated.</p></li>
<li><p><strong>Class Imbalance:</strong> Complement Naive Bayes can be particularly useful for handling class imbalance issues.</p></li>
<li><p><strong>Efficiency:</strong> Naive Bayes is highly efficient and scales well to large datasets, making it suitable for real-time prediction tasks.</p></li>
<li><p><strong>Interpretability:</strong> The model’s probabilistic nature makes it interpretable, allowing easy understanding of how predictions are made based on feature probabilities.</p></li>
</ul>
<div id="3a1734c8" class="cell" data-execution_count="16">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb39-3"><a href="#cb39-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb39-4"><a href="#cb39-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.naive_bayes <span class="im">import</span> GaussianNB</span>
<span id="cb39-5"><a href="#cb39-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> KBinsDiscretizer, QuantileTransformer</span>
<span id="cb39-6"><a href="#cb39-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> accuracy_score, confusion_matrix, classification_report</span>
<span id="cb39-7"><a href="#cb39-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-8"><a href="#cb39-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Sample data with continuous features</span></span>
<span id="cb39-9"><a href="#cb39-9" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> {</span>
<span id="cb39-10"><a href="#cb39-10" aria-hidden="true" tabindex="-1"></a>    <span class="st">'height'</span>: [<span class="fl">5.1</span>, <span class="fl">6.0</span>, <span class="fl">5.5</span>, <span class="fl">5.8</span>, <span class="fl">6.1</span>, <span class="fl">5.2</span>, <span class="fl">5.9</span>, <span class="fl">6.2</span>],</span>
<span id="cb39-11"><a href="#cb39-11" aria-hidden="true" tabindex="-1"></a>    <span class="st">'weight'</span>: [<span class="dv">120</span>, <span class="dv">150</span>, <span class="dv">130</span>, <span class="dv">160</span>, <span class="dv">170</span>, <span class="dv">125</span>, <span class="dv">140</span>, <span class="dv">175</span>],</span>
<span id="cb39-12"><a href="#cb39-12" aria-hidden="true" tabindex="-1"></a>    <span class="st">'age'</span>: [<span class="dv">23</span>, <span class="dv">45</span>, <span class="dv">34</span>, <span class="dv">50</span>, <span class="dv">40</span>, <span class="dv">22</span>, <span class="dv">30</span>, <span class="dv">60</span>],</span>
<span id="cb39-13"><a href="#cb39-13" aria-hidden="true" tabindex="-1"></a>    <span class="st">'label'</span>: [<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>]  <span class="co"># 1: Class A, 0: Class B</span></span>
<span id="cb39-14"><a href="#cb39-14" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb39-15"><a href="#cb39-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-16"><a href="#cb39-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert to DataFrame</span></span>
<span id="cb39-17"><a href="#cb39-17" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.DataFrame(data)</span>
<span id="cb39-18"><a href="#cb39-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-19"><a href="#cb39-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Split the data into training and testing sets</span></span>
<span id="cb39-20"><a href="#cb39-20" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> df[[<span class="st">'height'</span>, <span class="st">'weight'</span>, <span class="st">'age'</span>]]</span>
<span id="cb39-21"><a href="#cb39-21" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> df[<span class="st">'label'</span>]</span>
<span id="cb39-22"><a href="#cb39-22" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, y, test_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb39-23"><a href="#cb39-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-24"><a href="#cb39-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Method 1: Gaussian Naive Bayes</span></span>
<span id="cb39-25"><a href="#cb39-25" aria-hidden="true" tabindex="-1"></a>gnb <span class="op">=</span> GaussianNB()</span>
<span id="cb39-26"><a href="#cb39-26" aria-hidden="true" tabindex="-1"></a>gnb.fit(X_train, y_train)</span>
<span id="cb39-27"><a href="#cb39-27" aria-hidden="true" tabindex="-1"></a>y_pred_gnb <span class="op">=</span> gnb.predict(X_test)</span>
<span id="cb39-28"><a href="#cb39-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-29"><a href="#cb39-29" aria-hidden="true" tabindex="-1"></a><span class="co"># Evaluate Gaussian Naive Bayes model</span></span>
<span id="cb39-30"><a href="#cb39-30" aria-hidden="true" tabindex="-1"></a>accuracy_gnb <span class="op">=</span> accuracy_score(y_test, y_pred_gnb)</span>
<span id="cb39-31"><a href="#cb39-31" aria-hidden="true" tabindex="-1"></a>conf_matrix_gnb <span class="op">=</span> confusion_matrix(y_test, y_pred_gnb)</span>
<span id="cb39-32"><a href="#cb39-32" aria-hidden="true" tabindex="-1"></a>class_report_gnb <span class="op">=</span> classification_report(y_test, y_pred_gnb)</span>
<span id="cb39-33"><a href="#cb39-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-34"><a href="#cb39-34" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'Gaussian Naive Bayes Model Accuracy: </span><span class="sc">{</span>accuracy_gnb <span class="op">*</span> <span class="dv">100</span><span class="sc">:.2f}</span><span class="ss">%'</span>)</span>
<span id="cb39-35"><a href="#cb39-35" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Gaussian Naive Bayes Confusion Matrix:'</span>)</span>
<span id="cb39-36"><a href="#cb39-36" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(conf_matrix_gnb)</span>
<span id="cb39-37"><a href="#cb39-37" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Gaussian Naive Bayes Classification Report:'</span>)</span>
<span id="cb39-38"><a href="#cb39-38" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(class_report_gnb)</span>
<span id="cb39-39"><a href="#cb39-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-40"><a href="#cb39-40" aria-hidden="true" tabindex="-1"></a><span class="co"># Method 2: Discretization</span></span>
<span id="cb39-41"><a href="#cb39-41" aria-hidden="true" tabindex="-1"></a>discretizer <span class="op">=</span> KBinsDiscretizer(n_bins<span class="op">=</span><span class="dv">3</span>, encode<span class="op">=</span><span class="st">'ordinal'</span>, strategy<span class="op">=</span><span class="st">'uniform'</span>)</span>
<span id="cb39-42"><a href="#cb39-42" aria-hidden="true" tabindex="-1"></a>X_train_discretized <span class="op">=</span> discretizer.fit_transform(X_train)</span>
<span id="cb39-43"><a href="#cb39-43" aria-hidden="true" tabindex="-1"></a>X_test_discretized <span class="op">=</span> discretizer.transform(X_test)</span>
<span id="cb39-44"><a href="#cb39-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-45"><a href="#cb39-45" aria-hidden="true" tabindex="-1"></a><span class="co"># Train Naive Bayes model on discretized features</span></span>
<span id="cb39-46"><a href="#cb39-46" aria-hidden="true" tabindex="-1"></a>gnb_discretized <span class="op">=</span> GaussianNB()</span>
<span id="cb39-47"><a href="#cb39-47" aria-hidden="true" tabindex="-1"></a>gnb_discretized.fit(X_train_discretized, y_train)</span>
<span id="cb39-48"><a href="#cb39-48" aria-hidden="true" tabindex="-1"></a>y_pred_discretized <span class="op">=</span> gnb_discretized.predict(X_test_discretized)</span>
<span id="cb39-49"><a href="#cb39-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-50"><a href="#cb39-50" aria-hidden="true" tabindex="-1"></a><span class="co"># Evaluate Naive Bayes model on discretized features</span></span>
<span id="cb39-51"><a href="#cb39-51" aria-hidden="true" tabindex="-1"></a>accuracy_discretized <span class="op">=</span> accuracy_score(y_test, y_pred_discretized)</span>
<span id="cb39-52"><a href="#cb39-52" aria-hidden="true" tabindex="-1"></a>conf_matrix_discretized <span class="op">=</span> confusion_matrix(y_test, y_pred_discretized)</span>
<span id="cb39-53"><a href="#cb39-53" aria-hidden="true" tabindex="-1"></a>class_report_discretized <span class="op">=</span> classification_report(y_test, y_pred_discretized)</span>
<span id="cb39-54"><a href="#cb39-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-55"><a href="#cb39-55" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'Discretized Naive Bayes Model Accuracy: </span><span class="sc">{</span>accuracy_discretized <span class="op">*</span> <span class="dv">100</span><span class="sc">:.2f}</span><span class="ss">%'</span>)</span>
<span id="cb39-56"><a href="#cb39-56" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Discretized Naive Bayes Confusion Matrix:'</span>)</span>
<span id="cb39-57"><a href="#cb39-57" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(conf_matrix_discretized)</span>
<span id="cb39-58"><a href="#cb39-58" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Discretized Naive Bayes Classification Report:'</span>)</span>
<span id="cb39-59"><a href="#cb39-59" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(class_report_discretized)</span>
<span id="cb39-60"><a href="#cb39-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-61"><a href="#cb39-61" aria-hidden="true" tabindex="-1"></a><span class="co"># Method 3: Quantile Transformation</span></span>
<span id="cb39-62"><a href="#cb39-62" aria-hidden="true" tabindex="-1"></a>quantile_transformer <span class="op">=</span> QuantileTransformer(output_distribution<span class="op">=</span><span class="st">'normal'</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb39-63"><a href="#cb39-63" aria-hidden="true" tabindex="-1"></a>X_train_quantile <span class="op">=</span> quantile_transformer.fit_transform(X_train)</span>
<span id="cb39-64"><a href="#cb39-64" aria-hidden="true" tabindex="-1"></a>X_test_quantile <span class="op">=</span> quantile_transformer.transform(X_test)</span>
<span id="cb39-65"><a href="#cb39-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-66"><a href="#cb39-66" aria-hidden="true" tabindex="-1"></a><span class="co"># Train Naive Bayes model on quantile transformed features</span></span>
<span id="cb39-67"><a href="#cb39-67" aria-hidden="true" tabindex="-1"></a>gnb_quantile <span class="op">=</span> GaussianNB()</span>
<span id="cb39-68"><a href="#cb39-68" aria-hidden="true" tabindex="-1"></a>gnb_quantile.fit(X_train_quantile, y_train)</span>
<span id="cb39-69"><a href="#cb39-69" aria-hidden="true" tabindex="-1"></a>y_pred_quantile <span class="op">=</span> gnb_quantile.predict(X_test_quantile)</span>
<span id="cb39-70"><a href="#cb39-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-71"><a href="#cb39-71" aria-hidden="true" tabindex="-1"></a><span class="co"># Evaluate Naive Bayes model on quantile transformed features</span></span>
<span id="cb39-72"><a href="#cb39-72" aria-hidden="true" tabindex="-1"></a>accuracy_quantile <span class="op">=</span> accuracy_score(y_test, y_pred_quantile)</span>
<span id="cb39-73"><a href="#cb39-73" aria-hidden="true" tabindex="-1"></a>conf_matrix_quantile <span class="op">=</span> confusion_matrix(y_test, y_pred_quantile)</span>
<span id="cb39-74"><a href="#cb39-74" aria-hidden="true" tabindex="-1"></a>class_report_quantile <span class="op">=</span> classification_report(y_test, y_pred_quantile)</span>
<span id="cb39-75"><a href="#cb39-75" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-76"><a href="#cb39-76" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'Quantile Transformed Naive Bayes Model Accuracy: </span><span class="sc">{</span>accuracy_quantile <span class="op">*</span> <span class="dv">100</span><span class="sc">:.2f}</span><span class="ss">%'</span>)</span>
<span id="cb39-77"><a href="#cb39-77" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Quantile Transformed Naive Bayes Confusion Matrix:'</span>)</span>
<span id="cb39-78"><a href="#cb39-78" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(conf_matrix_quantile)</span>
<span id="cb39-79"><a href="#cb39-79" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Quantile Transformed Naive Bayes Classification Report:'</span>)</span>
<span id="cb39-80"><a href="#cb39-80" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(class_report_quantile)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Gaussian Naive Bayes Model Accuracy: 100.00%
Gaussian Naive Bayes Confusion Matrix:
[[1 0]
 [0 1]]
Gaussian Naive Bayes Classification Report:
              precision    recall  f1-score   support

           0       1.00      1.00      1.00         1
           1       1.00      1.00      1.00         1

    accuracy                           1.00         2
   macro avg       1.00      1.00      1.00         2
weighted avg       1.00      1.00      1.00         2

Discretized Naive Bayes Model Accuracy: 100.00%
Discretized Naive Bayes Confusion Matrix:
[[1 0]
 [0 1]]
Discretized Naive Bayes Classification Report:
              precision    recall  f1-score   support

           0       1.00      1.00      1.00         1
           1       1.00      1.00      1.00         1

    accuracy                           1.00         2
   macro avg       1.00      1.00      1.00         2
weighted avg       1.00      1.00      1.00         2

Quantile Transformed Naive Bayes Model Accuracy: 100.00%
Quantile Transformed Naive Bayes Confusion Matrix:
[[1 0]
 [0 1]]
Quantile Transformed Naive Bayes Classification Report:
              precision    recall  f1-score   support

           0       1.00      1.00      1.00         1
           1       1.00      1.00      1.00         1

    accuracy                           1.00         2
   macro avg       1.00      1.00      1.00         2
weighted avg       1.00      1.00      1.00         2
</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>/Users/ravishankar/miniforge3/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:2627: UserWarning:

n_quantiles (1000) is greater than the total number of samples (6). n_quantiles is set to n_samples.
</code></pre>
</div>
</div>
</section>
</section>
</section>
<section id="support-vector-machines-svm-basics" class="level1">
<h1>4.6 Support Vector Machines (SVM) Basics</h1>
<p>Support Vector Machines (SVM) are powerful supervised learning models used for classification and regression tasks. They work by finding the hyperplane that best separates the classes in the feature space.</p>
<section id="linear-svm" class="level2">
<h2 class="anchored" data-anchor-id="linear-svm">4.6.1 Linear SVM</h2>
<p>Linear SVM aims to find the hyperplane that maximizes the margin between the two classes in a linearly separable dataset.</p>
<ul>
<li><p><strong>Objective:</strong> Maximize the margin between the hyperplane and the nearest data points from both classes (support vectors).</p></li>
<li><p><strong>Equation of Hyperplane:</strong> <span class="math display">\[
\mathbf{w} \cdot \mathbf{x} + b = 0
\]</span> where <span class="math inline">\(\mathbf{w}\)</span> is the weight vector, <span class="math inline">\(\mathbf{x}\)</span> is the feature vector, and <span class="math inline">\(b\)</span> is the bias term.</p></li>
<li><p><strong>Optimization Problem:</strong> <span class="math display">\[
\min_{\mathbf{w}, b} \frac{1}{2} \|\mathbf{w}\|^2
\]</span> subject to <span class="math display">\[
y_i (\mathbf{w} \cdot \mathbf{x_i} + b) \geq 1, \quad \forall i
\]</span> where <span class="math inline">\(y_i\)</span> is the class label of <span class="math inline">\(\mathbf{x_i}\)</span>.</p></li>
<li><p><strong>Example:</strong> Classifying emails as spam or not spam based on their content.</p></li>
</ul>
<section id="steps-4" class="level4">
<h4 class="anchored" data-anchor-id="steps-4">Steps:</h4>
<ol type="1">
<li><p><strong>Compute the weight vector and bias:</strong> Solve the optimization problem to find <span class="math inline">\(\mathbf{w}\)</span> and <span class="math inline">\(b\)</span>.</p></li>
<li><p><strong>Classify new data points:</strong> Use the equation of the hyperplane to predict the class of new data points.</p></li>
</ol>
</section>
<section id="detailed-steps-for-solving-the-optimization-problem" class="level4">
<h4 class="anchored" data-anchor-id="detailed-steps-for-solving-the-optimization-problem">Detailed Steps for Solving the Optimization Problem:</h4>
<ol type="1">
<li><p><strong>Formulate the primal optimization problem:</strong> Set up the problem to minimize the norm of the weight vector while ensuring all data points are correctly classified.</p></li>
<li><p><strong>Convert to dual form:</strong> Use Lagrange multipliers to convert the primal problem into a dual problem, which is easier to solve when dealing with large datasets.</p></li>
<li><p><strong>Solve the dual problem:</strong> Use quadratic programming (QP) solvers to find the optimal values of the Lagrange multipliers.</p></li>
<li><p><strong>Recover the primal solution:</strong> Use the optimal Lagrange multipliers to compute the weight vector and bias term.</p></li>
</ol>
<div id="fac7227c" class="cell" data-execution_count="17">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb42"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.optimize <span class="im">import</span> minimize</span>
<span id="cb42-3"><a href="#cb42-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> make_classification</span>
<span id="cb42-4"><a href="#cb42-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb42-5"><a href="#cb42-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> StandardScaler</span>
<span id="cb42-6"><a href="#cb42-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> accuracy_score</span>
<span id="cb42-7"><a href="#cb42-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-8"><a href="#cb42-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Generating a linearly separable dataset</span></span>
<span id="cb42-9"><a href="#cb42-9" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> make_classification(n_samples<span class="op">=</span><span class="dv">100</span>, n_features<span class="op">=</span><span class="dv">2</span>, n_classes<span class="op">=</span><span class="dv">2</span>, n_clusters_per_class<span class="op">=</span><span class="dv">1</span>, n_informative<span class="op">=</span><span class="dv">2</span>, n_redundant<span class="op">=</span><span class="dv">0</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb42-10"><a href="#cb42-10" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> <span class="dv">2</span> <span class="op">*</span> y <span class="op">-</span> <span class="dv">1</span>  <span class="co"># Convert labels to {-1, 1}</span></span>
<span id="cb42-11"><a href="#cb42-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-12"><a href="#cb42-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Split the dataset into training and test sets</span></span>
<span id="cb42-13"><a href="#cb42-13" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, y, test_size<span class="op">=</span><span class="fl">0.3</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb42-14"><a href="#cb42-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-15"><a href="#cb42-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Standardize the features</span></span>
<span id="cb42-16"><a href="#cb42-16" aria-hidden="true" tabindex="-1"></a>scaler <span class="op">=</span> StandardScaler()</span>
<span id="cb42-17"><a href="#cb42-17" aria-hidden="true" tabindex="-1"></a>X_train <span class="op">=</span> scaler.fit_transform(X_train)</span>
<span id="cb42-18"><a href="#cb42-18" aria-hidden="true" tabindex="-1"></a>X_test <span class="op">=</span> scaler.transform(X_test)</span>
<span id="cb42-19"><a href="#cb42-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-20"><a href="#cb42-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the primal optimization problem</span></span>
<span id="cb42-21"><a href="#cb42-21" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> objective(w):</span>
<span id="cb42-22"><a href="#cb42-22" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="fl">0.5</span> <span class="op">*</span> np.dot(w[:<span class="op">-</span><span class="dv">1</span>], w[:<span class="op">-</span><span class="dv">1</span>])  <span class="co"># Minimize the norm of the weight vector</span></span>
<span id="cb42-23"><a href="#cb42-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-24"><a href="#cb42-24" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> constraint(w, X, y, i):</span>
<span id="cb42-25"><a href="#cb42-25" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> y[i] <span class="op">*</span> (np.dot(X[i], w[:<span class="op">-</span><span class="dv">1</span>]) <span class="op">+</span> w[<span class="op">-</span><span class="dv">1</span>]) <span class="op">-</span> <span class="dv">1</span></span>
<span id="cb42-26"><a href="#cb42-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-27"><a href="#cb42-27" aria-hidden="true" tabindex="-1"></a>constraints <span class="op">=</span> [{<span class="st">'type'</span>: <span class="st">'ineq'</span>, <span class="st">'fun'</span>: <span class="kw">lambda</span> w, X<span class="op">=</span>X_train, y<span class="op">=</span>y_train, i<span class="op">=</span>i: constraint(w, X, y, i)} <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(y_train))]</span>
<span id="cb42-28"><a href="#cb42-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-29"><a href="#cb42-29" aria-hidden="true" tabindex="-1"></a><span class="co"># Initial guess for the weights and bias</span></span>
<span id="cb42-30"><a href="#cb42-30" aria-hidden="true" tabindex="-1"></a>w0 <span class="op">=</span> np.zeros(X_train.shape[<span class="dv">1</span>] <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb42-31"><a href="#cb42-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-32"><a href="#cb42-32" aria-hidden="true" tabindex="-1"></a><span class="co"># Solve the primal problem using a quadratic programming solver</span></span>
<span id="cb42-33"><a href="#cb42-33" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> minimize(objective, w0, constraints<span class="op">=</span>constraints, method<span class="op">=</span><span class="st">'SLSQP'</span>)</span>
<span id="cb42-34"><a href="#cb42-34" aria-hidden="true" tabindex="-1"></a>w_opt <span class="op">=</span> result.x</span>
<span id="cb42-35"><a href="#cb42-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-36"><a href="#cb42-36" aria-hidden="true" tabindex="-1"></a><span class="co"># Extract the weight vector and bias term</span></span>
<span id="cb42-37"><a href="#cb42-37" aria-hidden="true" tabindex="-1"></a>w <span class="op">=</span> w_opt[:<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb42-38"><a href="#cb42-38" aria-hidden="true" tabindex="-1"></a>b <span class="op">=</span> w_opt[<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb42-39"><a href="#cb42-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-40"><a href="#cb42-40" aria-hidden="true" tabindex="-1"></a><span class="co"># Function to classify new data points</span></span>
<span id="cb42-41"><a href="#cb42-41" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> classify(X):</span>
<span id="cb42-42"><a href="#cb42-42" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.sign(np.dot(X, w) <span class="op">+</span> b)</span>
<span id="cb42-43"><a href="#cb42-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-44"><a href="#cb42-44" aria-hidden="true" tabindex="-1"></a><span class="co"># Predict the class of the test set</span></span>
<span id="cb42-45"><a href="#cb42-45" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> classify(X_test)</span>
<span id="cb42-46"><a href="#cb42-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-47"><a href="#cb42-47" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate the accuracy of the classifier</span></span>
<span id="cb42-48"><a href="#cb42-48" aria-hidden="true" tabindex="-1"></a>accuracy <span class="op">=</span> accuracy_score(y_test, y_pred)</span>
<span id="cb42-49"><a href="#cb42-49" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'Accuracy: </span><span class="sc">{</span>accuracy <span class="op">*</span> <span class="dv">100</span><span class="sc">:.2f}</span><span class="ss">%'</span>)</span>
<span id="cb42-50"><a href="#cb42-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-51"><a href="#cb42-51" aria-hidden="true" tabindex="-1"></a><span class="co"># Example usage: Classifying new emails as spam or not spam based on their content</span></span>
<span id="cb42-52"><a href="#cb42-52" aria-hidden="true" tabindex="-1"></a>new_data <span class="op">=</span> scaler.transform([[<span class="fl">0.5</span>, <span class="op">-</span><span class="fl">1.5</span>], [<span class="fl">2.0</span>, <span class="fl">1.0</span>]])  <span class="co"># Example new data points</span></span>
<span id="cb42-53"><a href="#cb42-53" aria-hidden="true" tabindex="-1"></a>predictions <span class="op">=</span> classify(new_data)</span>
<span id="cb42-54"><a href="#cb42-54" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'Predictions: </span><span class="sc">{</span>predictions<span class="sc">}</span><span class="ss">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Accuracy: 100.00%
Predictions: [-1.  1.]</code></pre>
</div>
</div>
</section>
</section>
<section id="kernel-trick-introduction" class="level2">
<h2 class="anchored" data-anchor-id="kernel-trick-introduction">4.6.2 Kernel Trick Introduction</h2>
<p>The kernel trick allows SVMs to perform classification in higher-dimensional spaces without explicitly computing the coordinates of the data in that space. It does this by computing the inner products of the data points in the feature space using a kernel function.</p>
<ul>
<li><strong>Kernel Function:</strong> A function that computes the dot product of the data points in the feature space. <span class="math display">\[
K(\mathbf{x_i}, \mathbf{x_j}) = \phi(\mathbf{x_i}) \cdot \phi(\mathbf{x_j})
\]</span> where <span class="math inline">\(\phi\)</span> is a mapping from the input space to the feature space.</li>
</ul>
<section id="polynomial-kernel" class="level4">
<h4 class="anchored" data-anchor-id="polynomial-kernel">4.6.2.1 Polynomial Kernel</h4>
<p>The polynomial kernel computes the similarity between data points as if they were transformed into a higher polynomial space.</p>
<ul>
<li><p><strong>Formula:</strong> <span class="math display">\[
K(\mathbf{x_i}, \mathbf{x_j}) = (\mathbf{x_i} \cdot \mathbf{x_j} + c)^d
\]</span> where <span class="math inline">\(d\)</span> is the degree of the polynomial and <span class="math inline">\(c\)</span> is a constant.</p></li>
<li><p><strong>Example:</strong> Classifying data points that are not linearly separable in the original space but can be separated using a polynomial decision boundary.</p></li>
</ul>
</section>
<section id="radial-basis-function-rbf-kernel" class="level4">
<h4 class="anchored" data-anchor-id="radial-basis-function-rbf-kernel">4.6.2.2 Radial Basis Function (RBF) Kernel</h4>
<p>The RBF kernel, also known as the Gaussian kernel, computes the similarity based on the distance between data points in the feature space.</p>
<ul>
<li><p><strong>Formula:</strong> <span class="math display">\[
K(\mathbf{x_i}, \mathbf{x_j}) = \exp \left( -\frac{\|\mathbf{x_i} - \mathbf{x_j}\|^2}{2\sigma^2} \right)
\]</span> where <span class="math inline">\(\sigma\)</span> is the bandwidth parameter.</p></li>
<li><p><strong>Example:</strong> Classifying complex datasets with non-linear decision boundaries.</p></li>
</ul>
</section>
<section id="detailed-steps-for-using-kernels-in-svm" class="level4">
<h4 class="anchored" data-anchor-id="detailed-steps-for-using-kernels-in-svm">Detailed Steps for Using Kernels in SVM:</h4>
<ol type="1">
<li><p><strong>Choose a kernel function:</strong> Select an appropriate kernel function based on the data characteristics (e.g., linear, polynomial, RBF).</p></li>
<li><p><strong>Compute the kernel matrix:</strong> Calculate the kernel matrix <span class="math inline">\(K\)</span> where each element <span class="math inline">\(K_{ij}\)</span> is the kernel function applied to data points <span class="math inline">\(\mathbf{x_i}\)</span> and <span class="math inline">\(\mathbf{x_j}\)</span>.</p></li>
<li><p><strong>Solve the dual problem:</strong> Use the kernel matrix in the dual formulation of the SVM optimization problem.</p></li>
<li><p><strong>Make predictions:</strong> Use the kernel function to compute the decision function for new data points.</p></li>
</ol>
<div id="63a0c11f" class="cell" data-execution_count="18">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb44"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb44-2"><a href="#cb44-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb44-3"><a href="#cb44-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb44-4"><a href="#cb44-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.feature_extraction.text <span class="im">import</span> CountVectorizer</span>
<span id="cb44-5"><a href="#cb44-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.svm <span class="im">import</span> SVC</span>
<span id="cb44-6"><a href="#cb44-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> accuracy_score, confusion_matrix, classification_report</span>
<span id="cb44-7"><a href="#cb44-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-8"><a href="#cb44-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Sample text data</span></span>
<span id="cb44-9"><a href="#cb44-9" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> {</span>
<span id="cb44-10"><a href="#cb44-10" aria-hidden="true" tabindex="-1"></a>    <span class="st">'text'</span>: [</span>
<span id="cb44-11"><a href="#cb44-11" aria-hidden="true" tabindex="-1"></a>        <span class="st">'Buy cheap products now'</span>,</span>
<span id="cb44-12"><a href="#cb44-12" aria-hidden="true" tabindex="-1"></a>        <span class="st">'Limited time offer for exclusive deals'</span>,</span>
<span id="cb44-13"><a href="#cb44-13" aria-hidden="true" tabindex="-1"></a>        <span class="st">'Win a lottery today'</span>,</span>
<span id="cb44-14"><a href="#cb44-14" aria-hidden="true" tabindex="-1"></a>        <span class="st">'Hello, how are you?'</span>,</span>
<span id="cb44-15"><a href="#cb44-15" aria-hidden="true" tabindex="-1"></a>        <span class="st">'Meeting at 5 PM'</span>,</span>
<span id="cb44-16"><a href="#cb44-16" aria-hidden="true" tabindex="-1"></a>        <span class="st">'Project deadline is tomorrow'</span></span>
<span id="cb44-17"><a href="#cb44-17" aria-hidden="true" tabindex="-1"></a>    ],</span>
<span id="cb44-18"><a href="#cb44-18" aria-hidden="true" tabindex="-1"></a>    <span class="st">'label'</span>: [<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>]  <span class="co"># 1: Spam, 0: Not Spam</span></span>
<span id="cb44-19"><a href="#cb44-19" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb44-20"><a href="#cb44-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-21"><a href="#cb44-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert to DataFrame</span></span>
<span id="cb44-22"><a href="#cb44-22" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.DataFrame(data)</span>
<span id="cb44-23"><a href="#cb44-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-24"><a href="#cb44-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 1: Vectorize the text data (convert text to features)</span></span>
<span id="cb44-25"><a href="#cb44-25" aria-hidden="true" tabindex="-1"></a>vectorizer <span class="op">=</span> CountVectorizer()</span>
<span id="cb44-26"><a href="#cb44-26" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> vectorizer.fit_transform(df[<span class="st">'text'</span>]).toarray()</span>
<span id="cb44-27"><a href="#cb44-27" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> df[<span class="st">'label'</span>]</span>
<span id="cb44-28"><a href="#cb44-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-29"><a href="#cb44-29" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert labels to -1 and 1 for SVM</span></span>
<span id="cb44-30"><a href="#cb44-30" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> np.where(y <span class="op">==</span> <span class="dv">1</span>, <span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb44-31"><a href="#cb44-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-32"><a href="#cb44-32" aria-hidden="true" tabindex="-1"></a><span class="co"># Split the data into training and testing sets</span></span>
<span id="cb44-33"><a href="#cb44-33" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, y, test_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb44-34"><a href="#cb44-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-35"><a href="#cb44-35" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 2: Train SVM model using Polynomial Kernel</span></span>
<span id="cb44-36"><a href="#cb44-36" aria-hidden="true" tabindex="-1"></a>degree <span class="op">=</span> <span class="dv">3</span></span>
<span id="cb44-37"><a href="#cb44-37" aria-hidden="true" tabindex="-1"></a>poly_svm <span class="op">=</span> SVC(kernel<span class="op">=</span><span class="st">'poly'</span>, degree<span class="op">=</span>degree, C<span class="op">=</span><span class="fl">1.0</span>)</span>
<span id="cb44-38"><a href="#cb44-38" aria-hidden="true" tabindex="-1"></a>poly_svm.fit(X_train, y_train)</span>
<span id="cb44-39"><a href="#cb44-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-40"><a href="#cb44-40" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 3: Predict the outcomes on the test set using Polynomial Kernel</span></span>
<span id="cb44-41"><a href="#cb44-41" aria-hidden="true" tabindex="-1"></a>y_pred_poly <span class="op">=</span> poly_svm.predict(X_test)</span>
<span id="cb44-42"><a href="#cb44-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-43"><a href="#cb44-43" aria-hidden="true" tabindex="-1"></a><span class="co"># Evaluate the model using Polynomial Kernel</span></span>
<span id="cb44-44"><a href="#cb44-44" aria-hidden="true" tabindex="-1"></a>accuracy_poly <span class="op">=</span> accuracy_score(y_test, y_pred_poly)</span>
<span id="cb44-45"><a href="#cb44-45" aria-hidden="true" tabindex="-1"></a>conf_matrix_poly <span class="op">=</span> confusion_matrix(y_test, y_pred_poly)</span>
<span id="cb44-46"><a href="#cb44-46" aria-hidden="true" tabindex="-1"></a>class_report_poly <span class="op">=</span> classification_report(y_test, y_pred_poly)</span>
<span id="cb44-47"><a href="#cb44-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-48"><a href="#cb44-48" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'Polynomial Kernel SVM Model Accuracy: </span><span class="sc">{</span>accuracy_poly <span class="op">*</span> <span class="dv">100</span><span class="sc">:.2f}</span><span class="ss">%'</span>)</span>
<span id="cb44-49"><a href="#cb44-49" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Polynomial Kernel SVM Confusion Matrix:'</span>)</span>
<span id="cb44-50"><a href="#cb44-50" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(conf_matrix_poly)</span>
<span id="cb44-51"><a href="#cb44-51" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Polynomial Kernel SVM Classification Report:'</span>)</span>
<span id="cb44-52"><a href="#cb44-52" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(class_report_poly)</span>
<span id="cb44-53"><a href="#cb44-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-54"><a href="#cb44-54" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 4: Train SVM model using RBF Kernel</span></span>
<span id="cb44-55"><a href="#cb44-55" aria-hidden="true" tabindex="-1"></a>rbf_svm <span class="op">=</span> SVC(kernel<span class="op">=</span><span class="st">'rbf'</span>, gamma<span class="op">=</span><span class="st">'scale'</span>, C<span class="op">=</span><span class="fl">1.0</span>)</span>
<span id="cb44-56"><a href="#cb44-56" aria-hidden="true" tabindex="-1"></a>rbf_svm.fit(X_train, y_train)</span>
<span id="cb44-57"><a href="#cb44-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-58"><a href="#cb44-58" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 5: Predict the outcomes on the test set using RBF Kernel</span></span>
<span id="cb44-59"><a href="#cb44-59" aria-hidden="true" tabindex="-1"></a>y_pred_rbf <span class="op">=</span> rbf_svm.predict(X_test)</span>
<span id="cb44-60"><a href="#cb44-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-61"><a href="#cb44-61" aria-hidden="true" tabindex="-1"></a><span class="co"># Evaluate the model using RBF Kernel</span></span>
<span id="cb44-62"><a href="#cb44-62" aria-hidden="true" tabindex="-1"></a>accuracy_rbf <span class="op">=</span> accuracy_score(y_test, y_pred_rbf)</span>
<span id="cb44-63"><a href="#cb44-63" aria-hidden="true" tabindex="-1"></a>conf_matrix_rbf <span class="op">=</span> confusion_matrix(y_test, y_pred_rbf)</span>
<span id="cb44-64"><a href="#cb44-64" aria-hidden="true" tabindex="-1"></a>class_report_rbf <span class="op">=</span> classification_report(y_test, y_pred_rbf)</span>
<span id="cb44-65"><a href="#cb44-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-66"><a href="#cb44-66" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'RBF Kernel SVM Model Accuracy: </span><span class="sc">{</span>accuracy_rbf <span class="op">*</span> <span class="dv">100</span><span class="sc">:.2f}</span><span class="ss">%'</span>)</span>
<span id="cb44-67"><a href="#cb44-67" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'RBF Kernel SVM Confusion Matrix:'</span>)</span>
<span id="cb44-68"><a href="#cb44-68" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(conf_matrix_rbf)</span>
<span id="cb44-69"><a href="#cb44-69" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'RBF Kernel SVM Classification Report:'</span>)</span>
<span id="cb44-70"><a href="#cb44-70" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(class_report_rbf)</span>
<span id="cb44-71"><a href="#cb44-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-72"><a href="#cb44-72" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 6: Classify new data points using Polynomial Kernel SVM</span></span>
<span id="cb44-73"><a href="#cb44-73" aria-hidden="true" tabindex="-1"></a>new_texts <span class="op">=</span> [<span class="st">'Win a free ticket'</span>, <span class="st">'Are you available for a meeting?'</span>]</span>
<span id="cb44-74"><a href="#cb44-74" aria-hidden="true" tabindex="-1"></a>new_X <span class="op">=</span> vectorizer.transform(new_texts).toarray()</span>
<span id="cb44-75"><a href="#cb44-75" aria-hidden="true" tabindex="-1"></a>new_predictions_poly <span class="op">=</span> poly_svm.predict(new_X)</span>
<span id="cb44-76"><a href="#cb44-76" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-77"><a href="#cb44-77" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> text, prediction <span class="kw">in</span> <span class="bu">zip</span>(new_texts, new_predictions_poly):</span>
<span id="cb44-78"><a href="#cb44-78" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f'Text: "</span><span class="sc">{</span>text<span class="sc">}</span><span class="ss">" -&gt; Prediction: </span><span class="sc">{</span><span class="st">"Spam"</span> <span class="cf">if</span> prediction <span class="op">==</span> <span class="dv">1</span> <span class="cf">else</span> <span class="st">"Not Spam"</span><span class="sc">}</span><span class="ss"> (Polynomial Kernel)'</span>)</span>
<span id="cb44-79"><a href="#cb44-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-80"><a href="#cb44-80" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 7: Classify new data points using RBF Kernel SVM</span></span>
<span id="cb44-81"><a href="#cb44-81" aria-hidden="true" tabindex="-1"></a>new_predictions_rbf <span class="op">=</span> rbf_svm.predict(new_X)</span>
<span id="cb44-82"><a href="#cb44-82" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-83"><a href="#cb44-83" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> text, prediction <span class="kw">in</span> <span class="bu">zip</span>(new_texts, new_predictions_rbf):</span>
<span id="cb44-84"><a href="#cb44-84" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f'Text: "</span><span class="sc">{</span>text<span class="sc">}</span><span class="ss">" -&gt; Prediction: </span><span class="sc">{</span><span class="st">"Spam"</span> <span class="cf">if</span> prediction <span class="op">==</span> <span class="dv">1</span> <span class="cf">else</span> <span class="st">"Not Spam"</span><span class="sc">}</span><span class="ss"> (RBF Kernel)'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Polynomial Kernel SVM Model Accuracy: 0.00%
Polynomial Kernel SVM Confusion Matrix:
[[0 0]
 [2 0]]
Polynomial Kernel SVM Classification Report:
              precision    recall  f1-score   support

          -1       0.00      0.00      0.00       0.0
           1       0.00      0.00      0.00       2.0

    accuracy                           0.00       2.0
   macro avg       0.00      0.00      0.00       2.0
weighted avg       0.00      0.00      0.00       2.0

RBF Kernel SVM Model Accuracy: 0.00%
RBF Kernel SVM Confusion Matrix:
[[0 0]
 [2 0]]
RBF Kernel SVM Classification Report:
              precision    recall  f1-score   support

          -1       0.00      0.00      0.00       0.0
           1       0.00      0.00      0.00       2.0

    accuracy                           0.00       2.0
   macro avg       0.00      0.00      0.00       2.0
weighted avg       0.00      0.00      0.00       2.0

Text: "Win a free ticket" -&gt; Prediction: Not Spam (Polynomial Kernel)
Text: "Are you available for a meeting?" -&gt; Prediction: Not Spam (Polynomial Kernel)
Text: "Win a free ticket" -&gt; Prediction: Not Spam (RBF Kernel)
Text: "Are you available for a meeting?" -&gt; Prediction: Not Spam (RBF Kernel)</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>/Users/ravishankar/miniforge3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning:

Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.

/Users/ravishankar/miniforge3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning:

Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.

/Users/ravishankar/miniforge3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning:

Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.

/Users/ravishankar/miniforge3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning:

Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.

/Users/ravishankar/miniforge3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning:

Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.

/Users/ravishankar/miniforge3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning:

Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.

/Users/ravishankar/miniforge3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning:

Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.

/Users/ravishankar/miniforge3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning:

Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.

/Users/ravishankar/miniforge3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning:

Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.

/Users/ravishankar/miniforge3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning:

Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.

/Users/ravishankar/miniforge3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning:

Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.

/Users/ravishankar/miniforge3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning:

Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
</code></pre>
</div>
</div>
</section>
</section>
<section id="soft-margin-svm" class="level2">
<h2 class="anchored" data-anchor-id="soft-margin-svm">4.6.3 Soft Margin SVM</h2>
<p>Soft margin SVM allows some misclassifications in the training data to enable the model to generalize better to unseen data. It introduces a penalty for misclassified points.</p>
<ul>
<li><p><strong>Objective:</strong> Balance maximizing the margin and minimizing the classification error.</p></li>
<li><p><strong>Optimization Problem:</strong> <span class="math display">\[
\min_{\mathbf{w}, b, \xi} \frac{1}{2} \|\mathbf{w}\|^2 + C \sum_{i=1}^{n} \xi_i
\]</span> subject to <span class="math display">\[
y_i (\mathbf{w} \cdot \mathbf{x_i} + b) \geq 1 - \xi_i, \quad \xi_i \geq 0, \quad \forall i
\]</span> where <span class="math inline">\(\xi_i\)</span> are slack variables that allow for misclassification, and <span class="math inline">\(C\)</span> is a regularization parameter that controls the trade-off between maximizing the margin and minimizing the error.</p></li>
<li><p><strong>Example:</strong> Classifying images of handwritten digits, allowing for some misclassified training examples to improve generalization.</p></li>
</ul>
<section id="steps-5" class="level4">
<h4 class="anchored" data-anchor-id="steps-5">Steps:</h4>
<ol type="1">
<li><p><strong>Introduce slack variables:</strong> Allow some training points to be inside the margin or misclassified.</p></li>
<li><p><strong>Solve the optimization problem:</strong> Find <span class="math inline">\(\mathbf{w}\)</span>, <span class="math inline">\(b\)</span>, and <span class="math inline">\(\xi_i\)</span> that minimize the objective function.</p></li>
<li><p><strong>Classify new data points:</strong> Use the computed hyperplane to predict the class of new data points.</p></li>
</ol>
</section>
<section id="detailed-steps-for-soft-margin-svm" class="level4">
<h4 class="anchored" data-anchor-id="detailed-steps-for-soft-margin-svm">Detailed Steps for Soft Margin SVM:</h4>
<ol type="1">
<li><p><strong>Formulate the primal problem:</strong> Set up the optimization problem with slack variables to allow for misclassification.</p></li>
<li><p><strong>Convert to dual form:</strong> Use Lagrange multipliers to convert the primal problem into a dual problem.</p></li>
<li><p><strong>Solve the dual problem:</strong> Use QP solvers to find the optimal values of the Lagrange multipliers.</p></li>
<li><p><strong>Recover the primal solution:</strong> Use the optimal Lagrange multipliers to compute the weight vector, bias term, and slack variables.</p></li>
<li><p><strong>Tune the regularization parameter <span class="math inline">\(C\)</span>:</strong> Use cross-validation to find the optimal value of <span class="math inline">\(C\)</span> that balances margin maximization and error minimization.</p></li>
</ol>
<div id="28b903e9" class="cell" data-execution_count="19">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb47"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb47-2"><a href="#cb47-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> load_digits</span>
<span id="cb47-3"><a href="#cb47-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split, GridSearchCV</span>
<span id="cb47-4"><a href="#cb47-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> StandardScaler</span>
<span id="cb47-5"><a href="#cb47-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> accuracy_score</span>
<span id="cb47-6"><a href="#cb47-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.svm <span class="im">import</span> SVC</span>
<span id="cb47-7"><a href="#cb47-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-8"><a href="#cb47-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Load dataset</span></span>
<span id="cb47-9"><a href="#cb47-9" aria-hidden="true" tabindex="-1"></a>digits <span class="op">=</span> load_digits()</span>
<span id="cb47-10"><a href="#cb47-10" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> digits.data, digits.target</span>
<span id="cb47-11"><a href="#cb47-11" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> np.where(y <span class="op">==</span> <span class="dv">0</span>, <span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>)  <span class="co"># Convert to binary classification problem (0 vs non-0)</span></span>
<span id="cb47-12"><a href="#cb47-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-13"><a href="#cb47-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Split the dataset into training and test sets</span></span>
<span id="cb47-14"><a href="#cb47-14" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, y, test_size<span class="op">=</span><span class="fl">0.3</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb47-15"><a href="#cb47-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-16"><a href="#cb47-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Standardize the features</span></span>
<span id="cb47-17"><a href="#cb47-17" aria-hidden="true" tabindex="-1"></a>scaler <span class="op">=</span> StandardScaler()</span>
<span id="cb47-18"><a href="#cb47-18" aria-hidden="true" tabindex="-1"></a>X_train <span class="op">=</span> scaler.fit_transform(X_train)</span>
<span id="cb47-19"><a href="#cb47-19" aria-hidden="true" tabindex="-1"></a>X_test <span class="op">=</span> scaler.transform(X_test)</span>
<span id="cb47-20"><a href="#cb47-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-21"><a href="#cb47-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the Soft Margin SVM with cross-validation to find the best regularization parameter</span></span>
<span id="cb47-22"><a href="#cb47-22" aria-hidden="true" tabindex="-1"></a>param_grid <span class="op">=</span> {<span class="st">'C'</span>: [<span class="fl">0.1</span>, <span class="dv">1</span>, <span class="dv">10</span>, <span class="dv">100</span>]}</span>
<span id="cb47-23"><a href="#cb47-23" aria-hidden="true" tabindex="-1"></a>svm <span class="op">=</span> SVC(kernel<span class="op">=</span><span class="st">'linear'</span>)</span>
<span id="cb47-24"><a href="#cb47-24" aria-hidden="true" tabindex="-1"></a>grid_search <span class="op">=</span> GridSearchCV(svm, param_grid, cv<span class="op">=</span><span class="dv">5</span>)</span>
<span id="cb47-25"><a href="#cb47-25" aria-hidden="true" tabindex="-1"></a>grid_search.fit(X_train, y_train)</span>
<span id="cb47-26"><a href="#cb47-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-27"><a href="#cb47-27" aria-hidden="true" tabindex="-1"></a><span class="co"># Best regularization parameter</span></span>
<span id="cb47-28"><a href="#cb47-28" aria-hidden="true" tabindex="-1"></a>best_C <span class="op">=</span> grid_search.best_params_[<span class="st">'C'</span>]</span>
<span id="cb47-29"><a href="#cb47-29" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'Best C: </span><span class="sc">{</span>best_C<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb47-30"><a href="#cb47-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-31"><a href="#cb47-31" aria-hidden="true" tabindex="-1"></a><span class="co"># Train the final model with the best C</span></span>
<span id="cb47-32"><a href="#cb47-32" aria-hidden="true" tabindex="-1"></a>final_svm <span class="op">=</span> SVC(kernel<span class="op">=</span><span class="st">'linear'</span>, C<span class="op">=</span>best_C)</span>
<span id="cb47-33"><a href="#cb47-33" aria-hidden="true" tabindex="-1"></a>final_svm.fit(X_train, y_train)</span>
<span id="cb47-34"><a href="#cb47-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-35"><a href="#cb47-35" aria-hidden="true" tabindex="-1"></a><span class="co"># Predict the class of the test set with the final model</span></span>
<span id="cb47-36"><a href="#cb47-36" aria-hidden="true" tabindex="-1"></a>final_y_pred <span class="op">=</span> final_svm.predict(X_test)</span>
<span id="cb47-37"><a href="#cb47-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-38"><a href="#cb47-38" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate the accuracy of the final model</span></span>
<span id="cb47-39"><a href="#cb47-39" aria-hidden="true" tabindex="-1"></a>final_accuracy <span class="op">=</span> accuracy_score(y_test, final_y_pred)</span>
<span id="cb47-40"><a href="#cb47-40" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'Final Accuracy: </span><span class="sc">{</span>final_accuracy <span class="op">*</span> <span class="dv">100</span><span class="sc">:.2f}</span><span class="ss">%'</span>)</span>
<span id="cb47-41"><a href="#cb47-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-42"><a href="#cb47-42" aria-hidden="true" tabindex="-1"></a><span class="co"># Example usage: Classifying new images of handwritten digits</span></span>
<span id="cb47-43"><a href="#cb47-43" aria-hidden="true" tabindex="-1"></a>new_data <span class="op">=</span> scaler.transform(digits.data[:<span class="dv">5</span>])  <span class="co"># Example new data points</span></span>
<span id="cb47-44"><a href="#cb47-44" aria-hidden="true" tabindex="-1"></a>predictions <span class="op">=</span> final_svm.predict(new_data)</span>
<span id="cb47-45"><a href="#cb47-45" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'Predictions: </span><span class="sc">{</span>predictions<span class="sc">}</span><span class="ss">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Best C: 0.1
Final Accuracy: 99.81%
Predictions: [ 1 -1 -1 -1 -1]</code></pre>
</div>
</div>
</section>
</section>
<section id="svm-for-regression-svr" class="level2">
<h2 class="anchored" data-anchor-id="svm-for-regression-svr">4.6.4 SVM for Regression (SVR)</h2>
<p>Support Vector Regression (SVR) extends SVM to regression tasks by finding a function that deviates from the actual target values by a value no greater than <span class="math inline">\(\epsilon\)</span> for all training data, while also being as flat as possible.</p>
<ul>
<li><p><strong>Objective:</strong> Minimize the prediction error within a certain tolerance <span class="math inline">\(\epsilon\)</span>.</p></li>
<li><p><strong>Optimization Problem:</strong> <span class="math display">\[
\min_{\mathbf{w}, b, \xi, \xi^*} \frac{1}{2} \|\mathbf{w}\|^2 + C \sum_{i=1}^{n} (\xi_i + \xi_i^*)
\]</span> subject to <span class="math display">\[
y_i - (\mathbf{w} \cdot \mathbf{x_i} + b) \leq \epsilon + \xi_i
\]</span> <span class="math display">\[
(\mathbf{w} \cdot \mathbf{x_i} + b) - y_i \leq \epsilon + \xi_i^*
\]</span> <span class="math display">\[
\xi_i, \xi_i^* \geq 0
\]</span> where <span class="math inline">\(\xi_i\)</span> and <span class="math inline">\(\xi_i^*\)</span> are slack variables that allow for errors, and <span class="math inline">\(C\)</span> is a regularization parameter.</p></li>
<li><p><strong>Example:</strong> Predicting housing prices based on features like size, location, and number of rooms.</p></li>
</ul>
<section id="steps-6" class="level4">
<h4 class="anchored" data-anchor-id="steps-6">Steps:</h4>
<ol type="1">
<li><p><strong>Introduce slack variables:</strong> Allow some deviations from the target values within the margin <span class="math inline">\(\epsilon\)</span>.</p></li>
<li><p><strong>Solve the optimization problem:</strong> Find <span class="math inline">\(\mathbf{w}\)</span>, <span class="math inline">\(b\)</span>, <span class="math inline">\(\xi_i\)</span>, and <span class="math inline">\(\xi_i^*\)</span> that minimize the objective function.</p></li>
<li><p><strong>Predict new values:</strong> Use the computed function to predict the target values for new data points.</p></li>
</ol>
</section>
<section id="detailed-steps-for-svr" class="level4">
<h4 class="anchored" data-anchor-id="detailed-steps-for-svr">Detailed Steps for SVR:</h4>
<ol type="1">
<li><p><strong>Formulate the primal problem:</strong> Set up the optimization problem with slack variables to allow for prediction errors.</p></li>
<li><p><strong>Convert to dual form:</strong> Use Lagrange multipliers to convert the primal problem into a dual problem.</p></li>
<li><p><strong>Solve the dual problem:</strong> Use QP solvers to find the optimal values of the Lagrange multipliers.</p></li>
<li><p><strong>Recover the primal solution:</strong> Use the optimal Lagrange multipliers to compute the weight vector, bias term, and slack variables.</p></li>
<li><p><strong>Tune the regularization parameter <span class="math inline">\(C\)</span> and <span class="math inline">\(\epsilon\)</span>:</strong> Use cross-validation to find the optimal values of <span class="math inline">\(C\)</span> and <span class="math inline">\(\epsilon\)</span> that balance flatness and prediction error.</p></li>
</ol>
</section>
<section id="kernelized-svr" class="level4">
<h4 class="anchored" data-anchor-id="kernelized-svr">Kernelized SVR:</h4>
<ol type="1">
<li><p><strong>Choose a kernel function:</strong> Select an appropriate kernel function (e.g., linear, polynomial, RBF) based on the data characteristics.</p></li>
<li><p><strong>Compute the kernel matrix:</strong> Calculate the kernel matrix <span class="math inline">\(K\)</span> where each element <span class="math inline">\(K_{ij}\)</span> is the kernel function applied to data points <span class="math inline">\(\mathbf{x_i}\)</span> and <span class="math inline">\(\mathbf{x_j}\)</span>.</p></li>
<li><p><strong>Solve the dual problem:</strong> Use the kernel matrix in the dual formulation of the SVR optimization problem.</p></li>
<li><p><strong>Make predictions:</strong> Use the kernel function to compute the regression function for new data points.</p></li>
</ol>
<p>Advanced considerations in SVM include:</p>
<ul>
<li><p><strong>Hyperparameter Tuning:</strong> Use techniques like grid search or random search to find the best hyperparameters (e.g., <span class="math inline">\(C\)</span>, <span class="math inline">\(\epsilon\)</span>, kernel parameters) for the SVM model.</p></li>
<li><p><strong>Feature Scaling:</strong> Standardize features to have zero mean and unit variance, as SVM is sensitive to the scale of the input data.</p></li>
<li><p><strong>Kernel Selection:</strong> Choose an appropriate kernel function (linear, polynomial, RBF, etc.) based on the complexity and nature of the data.</p></li>
<li><p><strong>Model Interpretability:</strong> While SVMs with non-linear kernels can produce highly accurate models, they are less interpretable compared to linear SVMs. Use methods like LIME or SHAP for interpretability.</p></li>
<li><p><strong>Outlier Sensitivity:</strong> Be aware that SVM can be sensitive to outliers. Consider using robust techniques or pre-processing steps to handle outliers.</p></li>
<li><p><strong>Computational Complexity:</strong> For large datasets, consider using approximate methods or specialized algorithms like Sequential Minimal Optimization (SMO) to improve computational efficiency.</p></li>
</ul>
<p>By following these detailed steps and considerations, SVMs can be effectively utilized for various machine learning tasks, providing powerful predictive models for both classification and regression.</p>
</section>
</section>
</section>
</div>



</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>