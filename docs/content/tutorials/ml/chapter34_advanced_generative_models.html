<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.47">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Ravi Shankar">

<title>chapter34_advanced_generative_models – Data Down To Earth</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../">
<link href="../../../logo.png" rel="icon" type="image/png">
<script src="../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../../styles.css">
<link rel="stylesheet" href="../../content.css">
<meta property="og:title" content="– Data Down To Earth">
<meta property="og:site_name" content="Data Down To Earth">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="../../../index.html" class="navbar-brand navbar-brand-logo">
    <img src="../../../logo.png" alt="" class="navbar-logo">
    </a>
    <a class="navbar-brand" href="../../../index.html">
    <span class="navbar-title">Data Down To Earth</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-projects" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Projects</span>
    </a>
    <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="nav-menu-projects">    
        <li>
    <a class="dropdown-item" href="../../../projects/dsml_projects.html">
 <span class="dropdown-text">Data Science and ML Projects</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../projects/genai_projects.html">
 <span class="dropdown-text">Generative AI Projects</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-tutorials" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Tutorials</span>
    </a>
    <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="nav-menu-tutorials">    
        <li>
    <a class="dropdown-item" href="../../../topics/statistics.html">
 <span class="dropdown-text">Statistics &amp; Probability</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../topics/machine_learning.html">
 <span class="dropdown-text">Machine Learning</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../topics/de.html">
 <span class="dropdown-text">Data Engineering</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../topics/generative_ai.html">
 <span class="dropdown-text">Generative AI</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../topics/public_health.html">
 <span class="dropdown-text">Public Health</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../topics/product_sense.html">
 <span class="dropdown-text">Product Sense</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item">
    <a class="nav-link" href="../../../notes/notes.html"> 
<span class="menu-text">Notes &amp; Research</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/iamrsps"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/iamravishankar/"> <i class="bi bi-linkedin" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content column-page" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
</div>



<div class="quarto-title-meta column-page">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Ravi Shankar </p>
          </div>
  </div>
    
  
    
  </div>
  


</header>


<div class="text-content">
<section id="flow-based-models" class="level4">
<h4 class="anchored" data-anchor-id="flow-based-models">34.1. Flow-based Models</h4>
<p>Flow-based models use invertible transformations to map complex data distributions to simple distributions (like Gaussian distributions) and vice versa. This property allows for exact likelihood computation and efficient sampling.</p>
</section>
<section id="normalizing-flows" class="level4">
<h4 class="anchored" data-anchor-id="normalizing-flows">34.1.1. Normalizing Flows</h4>
<p>Normalizing flows are a family of generative models that transform a simple base distribution into a more complex target distribution through a series of invertible and differentiable transformations.</p>
<ul>
<li><p><strong>Key Concepts:</strong></p>
<ul>
<li><strong>Base Distribution:</strong> A simple distribution, typically Gaussian, denoted as <span class="math inline">\(p_Z(z)\)</span>.</li>
<li><strong>Transformations:</strong> A sequence of invertible functions <span class="math inline">\(f_1, f_2, \ldots, f_K\)</span> that map the base distribution to the target distribution.</li>
</ul></li>
<li><p><strong>Mathematical Formulation:</strong></p>
<ul>
<li><strong>Transformation:</strong> <span class="math display">\[
z = f_K \circ f_{K-1} \circ \cdots \circ f_1(x)
\]</span></li>
<li><strong>Change of Variables Formula:</strong> The likelihood of the data <span class="math inline">\(x\)</span> can be computed using: <span class="math display">\[
p_X(x) = p_Z(f(x)) \left| \det \left( \frac{\partial f(x)}{\partial x} \right) \right|
\]</span> where <span class="math inline">\(\det \left( \frac{\partial f(x)}{\partial x} \right)\)</span> is the determinant of the Jacobian matrix of the transformation.</li>
</ul></li>
<li><p><strong>Training:</strong> Maximize the log-likelihood of the data: <span class="math display">\[
\log p_X(x) = \log p_Z(f(x)) + \log \left| \det \left( \frac{\partial f(x)}{\partial x} \right) \right|
\]</span></p></li>
<li><p><strong>Advantages:</strong></p>
<ul>
<li>Exact likelihood computation.</li>
<li>Efficient sampling and inversion.</li>
</ul></li>
<li><p><strong>Disadvantages:</strong></p>
<ul>
<li>Designing invertible transformations with efficient Jacobian computation can be challenging.</li>
</ul></li>
<li><p><strong>Applications:</strong></p>
<ul>
<li>Density estimation.</li>
<li>Anomaly detection.</li>
<li>Image synthesis.</li>
</ul></li>
</ul>
</section>
<section id="real-nvp" class="level4">
<h4 class="anchored" data-anchor-id="real-nvp">34.1.2. Real NVP</h4>
<p>Real-valued Non-Volume Preserving (Real NVP) is a specific type of normalizing flow that uses a coupling layer to achieve invertibility and efficient Jacobian computation.</p>
<ul>
<li><p><strong>Key Concepts:</strong></p>
<ul>
<li><strong>Coupling Layer:</strong> Splits the input <span class="math inline">\(x\)</span> into two parts <span class="math inline">\((x_1, x_2)\)</span> and applies an affine transformation to one part conditioned on the other. <span class="math display">\[
y_1 = x_1
\]</span> <span class="math display">\[
y_2 = x_2 \odot \exp(s(x_1)) + t(x_1)
\]</span> where <span class="math inline">\(s\)</span> and <span class="math inline">\(t\)</span> are scaling and translation functions, and <span class="math inline">\(\odot\)</span> denotes element-wise multiplication.</li>
</ul></li>
<li><p><strong>Jacobian Determinant:</strong> The determinant of the Jacobian for the coupling layer is easy to compute: <span class="math display">\[
\left| \det \left( \frac{\partial y}{\partial x} \right) \right| = \exp \left( \sum s(x_1) \right)
\]</span></p></li>
<li><p><strong>Advantages:</strong></p>
<ul>
<li>Efficient and scalable transformations.</li>
<li>Simple and exact Jacobian determinant computation.</li>
</ul></li>
<li><p><strong>Disadvantages:</strong></p>
<ul>
<li>The expressiveness is limited by the form of coupling layers.</li>
</ul></li>
<li><p><strong>Applications:</strong></p>
<ul>
<li>Image synthesis.</li>
<li>Data augmentation.</li>
<li>Density estimation.</li>
</ul></li>
</ul>
</section>
<section id="glow" class="level4">
<h4 class="anchored" data-anchor-id="glow">34.1.3. Glow</h4>
<p>Glow extends the ideas from Real NVP and introduces additional features like invertible <span class="math inline">\(1 \times 1\)</span> convolutions to improve the flexibility and expressiveness of the model.</p>
<ul>
<li><p><strong>Key Concepts:</strong></p>
<ul>
<li><strong>Invertible <span class="math inline">\(1 \times 1\)</span> Convolutions:</strong> Used to permute the channels of the input tensor, enhancing the coupling layer’s expressiveness. <span class="math display">\[
y = W x
\]</span> where <span class="math inline">\(W\)</span> is a learnable <span class="math inline">\(1 \times 1\)</span> convolutional weight matrix.</li>
</ul></li>
<li><p><strong>ActNorm Layer:</strong> Normalizes activations to stabilize training.</p>
<ul>
<li><strong>Affine Transformation:</strong> <span class="math display">\[
y = s \odot x + t
\]</span> where <span class="math inline">\(s\)</span> and <span class="math inline">\(t\)</span> are learnable scale and translation parameters initialized to ensure zero mean and unit variance of activations.</li>
</ul></li>
<li><p><strong>Multi-scale Architecture:</strong> Splits the data into multiple scales and processes each scale separately, enhancing the model’s ability to capture hierarchical features.</p></li>
<li><p><strong>Training:</strong> Similar to Real NVP, maximize the log-likelihood of the data.</p></li>
<li><p><strong>Advantages:</strong></p>
<ul>
<li>Improved expressiveness and flexibility.</li>
<li>Efficient and scalable for high-dimensional data.</li>
</ul></li>
<li><p><strong>Disadvantages:</strong></p>
<ul>
<li>More complex architecture and training process.</li>
</ul></li>
<li><p><strong>Applications:</strong></p>
<ul>
<li>High-quality image synthesis.</li>
<li>Video generation.</li>
<li>Density estimation and anomaly detection.</li>
</ul></li>
</ul>
<p>By understanding and utilizing flow-based models like Normalizing Flows, Real NVP, and Glow, researchers and practitioners can develop powerful generative models capable of capturing complex data distributions with high accuracy and efficiency. These models open new possibilities in various applications, from image and video generation to data augmentation and anomaly detection.</p>
</section>
<section id="diffusion-models" class="level3">
<h3 class="anchored" data-anchor-id="diffusion-models">34.2. Diffusion Models</h3>
<p>Diffusion models are a class of generative models that learn to reverse a diffusion process to generate data. These models are particularly effective for tasks like image and audio generation.</p>
<section id="denoising-diffusion-probabilistic-models-ddpm" class="level4">
<h4 class="anchored" data-anchor-id="denoising-diffusion-probabilistic-models-ddpm">34.2.1. Denoising Diffusion Probabilistic Models (DDPM)</h4>
<p>Denoising Diffusion Probabilistic Models (DDPMs) generate data by reversing a gradual noise addition process, learning to denoise the noisy data step by step.</p>
<ul>
<li><p><strong>Key Concepts:</strong></p>
<ul>
<li><strong>Forward Diffusion Process:</strong> Gradually adds Gaussian noise to the data over <span class="math inline">\(T\)</span> time steps, converting the data distribution into a standard Gaussian distribution. <span class="math display">\[
q(\mathbf{x}_t | \mathbf{x}_{t-1}) = \mathcal{N}(\mathbf{x}_t; \sqrt{1 - \beta_t} \mathbf{x}_{t-1}, \beta_t \mathbf{I})
\]</span> where <span class="math inline">\(\beta_t\)</span> are small positive constants controlling the noise schedule.</li>
<li><strong>Reverse Diffusion Process:</strong> The model learns to reverse the noise addition, generating data from Gaussian noise. <span class="math display">\[
p_\theta(\mathbf{x}_{t-1} | \mathbf{x}_t) = \mathcal{N}(\mathbf{x}_{t-1}; \mu_\theta(\mathbf{x}_t, t), \sigma_\theta^2(t) \mathbf{I})
\]</span></li>
</ul></li>
<li><p><strong>Training Objective:</strong></p>
<ul>
<li><strong>Variational Lower Bound (VLB):</strong> Minimize the negative log-likelihood by optimizing the variational lower bound. <span class="math display">\[
L = \mathbb{E}_{q} \left[ \sum_{t=1}^T D_{KL} \left( q(\mathbf{x}_{t-1} | \mathbf{x}_t, \mathbf{x}_0) \| p_\theta(\mathbf{x}_{t-1} | \mathbf{x}_t) \right) - \log p_\theta(\mathbf{x}_T) \right]
\]</span></li>
</ul></li>
<li><p><strong>Denoising Objective:</strong> Train the model to predict the original data from noisy data. <span class="math display">\[
\mathcal{L}_{\text{simple}} = \mathbb{E}_{t, \mathbf{x}_0, \epsilon} \left[ \| \epsilon - \epsilon_\theta(\mathbf{x}_t, t) \|^2 \right]
\]</span> where <span class="math inline">\(\epsilon \sim \mathcal{N}(0, \mathbf{I})\)</span>.</p></li>
<li><p><strong>Advantages:</strong></p>
<ul>
<li>Generates high-quality samples.</li>
<li>Stable training process.</li>
</ul></li>
<li><p><strong>Disadvantages:</strong></p>
<ul>
<li>Slow sampling process due to the large number of denoising steps.</li>
<li>Computationally intensive.</li>
</ul></li>
<li><p><strong>Applications:</strong></p>
<ul>
<li>Image generation.</li>
<li>Audio synthesis.</li>
<li>Video generation.</li>
</ul></li>
</ul>
</section>
<section id="score-based-generative-models" class="level4">
<h4 class="anchored" data-anchor-id="score-based-generative-models">34.2.2. Score-based Generative Models</h4>
<p>Score-based generative models use the concept of score matching to learn the gradient of the data distribution, allowing for efficient sampling and high-quality generation.</p>
<ul>
<li><p><strong>Key Concepts:</strong></p>
<ul>
<li><strong>Score Function:</strong> The gradient of the log-density of the data distribution, denoted as <span class="math inline">\(\nabla_{\mathbf{x}} \log p(\mathbf{x})\)</span>.</li>
<li><strong>Stein’s Identity:</strong> Provides a way to estimate the score function from samples. <span class="math display">\[
\mathbb{E}_{p(\mathbf{x})} [f(\mathbf{x}) \nabla_{\mathbf{x}} \log p(\mathbf{x})] = \mathbb{E}_{p(\mathbf{x})} [\nabla_{\mathbf{x}} f(\mathbf{x})]
\]</span></li>
</ul></li>
<li><p><strong>Denoising Score Matching:</strong> Train a neural network to estimate the score function by denoising noisy data. <span class="math display">\[
\mathcal{L}_{\text{DSM}} = \mathbb{E}_{q(\mathbf{x}, \mathbf{y})} \left[ \| s_\theta(\mathbf{y}) - \nabla_{\mathbf{y}} \log q(\mathbf{y}|\mathbf{x}) \|^2 \right]
\]</span> where <span class="math inline">\(\mathbf{y} = \mathbf{x} + \sigma \epsilon\)</span> is the noisy data, and <span class="math inline">\(\epsilon \sim \mathcal{N}(0, \mathbf{I})\)</span>.</p></li>
<li><p><strong>Annealed Langevin Dynamics:</strong> Sample from the data distribution using the estimated score function and Langevin dynamics. <span class="math display">\[
\mathbf{x}_{t+1} = \mathbf{x}_t + \frac{\eta_t}{2} s_\theta(\mathbf{x}_t) + \sqrt{\eta_t} \epsilon_t
\]</span> where <span class="math inline">\(\eta_t\)</span> is the step size and <span class="math inline">\(\epsilon_t \sim \mathcal{N}(0, \mathbf{I})\)</span>.</p></li>
<li><p><strong>Advantages:</strong></p>
<ul>
<li>Can generate high-quality samples with fewer steps compared to DDPMs.</li>
<li>Flexible framework for various data types.</li>
</ul></li>
<li><p><strong>Disadvantages:</strong></p>
<ul>
<li>Requires careful tuning of the noise schedule and sampling process.</li>
<li>Computationally demanding training process.</li>
</ul></li>
<li><p><strong>Applications:</strong></p>
<ul>
<li>Image synthesis.</li>
<li>Audio generation.</li>
<li>3D shape generation.</li>
</ul></li>
</ul>
</section>
<section id="applications-in-image-and-audio-generation" class="level4">
<h4 class="anchored" data-anchor-id="applications-in-image-and-audio-generation">34.2.3. Applications in Image and Audio Generation</h4>
<p>Diffusion models have demonstrated remarkable success in generating high-fidelity images and audio, pushing the boundaries of generative modeling.</p>
<ul>
<li><strong>Image Generation:</strong>
<ul>
<li><strong>High-resolution Image Synthesis:</strong> Diffusion models generate detailed and high-resolution images, capturing complex textures and structures.</li>
<li><strong>Denoising Tasks:</strong> Apply diffusion models to tasks like super-resolution, inpainting, and image restoration by leveraging their denoising capabilities.</li>
</ul></li>
<li><strong>Audio Generation:</strong>
<ul>
<li><strong>Speech Synthesis:</strong> Use diffusion models to generate high-quality speech, capturing the nuances of human voice and prosody.</li>
<li><strong>Music Generation:</strong> Generate realistic music compositions with coherent temporal structures using diffusion processes.</li>
</ul></li>
<li><strong>Advantages:</strong>
<ul>
<li>High-quality and realistic generation.</li>
<li>Versatile applications across various data modalities.</li>
</ul></li>
<li><strong>Disadvantages:</strong>
<ul>
<li>High computational cost for training and sampling.</li>
<li>Requires large datasets for optimal performance.</li>
</ul></li>
<li><strong>Applications:</strong>
<ul>
<li>Art and entertainment: Creating synthetic media content.</li>
<li>Data augmentation: Enhancing training datasets with synthetic data.</li>
<li>Restoration and enhancement: Improving the quality of degraded or incomplete data.</li>
</ul></li>
</ul>
<p>By leveraging diffusion models such as DDPMs and score-based generative models, researchers and practitioners can achieve state-of-the-art performance in generative tasks, enabling the creation of highly realistic and detailed synthetic data across various domains.</p>
</section>
</section>
<section id="energy-based-models" class="level3">
<h3 class="anchored" data-anchor-id="energy-based-models">34.3. Energy-based Models</h3>
<p>Energy-based models (EBMs) are a class of generative models that define an energy function over the input data space. The energy function assigns lower energies to more likely configurations and higher energies to less likely ones. EBMs are powerful for capturing complex dependencies in data.</p>
<section id="contrastive-divergence" class="level4">
<h4 class="anchored" data-anchor-id="contrastive-divergence">34.3.1. Contrastive Divergence</h4>
<p>Contrastive Divergence (CD) is a training algorithm for energy-based models, particularly Restricted Boltzmann Machines (RBMs). It approximates the gradient of the log-likelihood by contrasting the data distribution with the model distribution.</p>
<ul>
<li><strong>Restricted Boltzmann Machines (RBMs):</strong>
<ul>
<li><strong>Structure:</strong> RBMs are composed of visible units <span class="math inline">\(v\)</span> and hidden units <span class="math inline">\(h\)</span>, with an energy function defined as: <span class="math display">\[
E(v, h) = -b^T v - c^T h - v^T W h
\]</span> where <span class="math inline">\(b\)</span> and <span class="math inline">\(c\)</span> are bias terms for the visible and hidden units, respectively, and <span class="math inline">\(W\)</span> is the weight matrix.</li>
<li><strong>Probability Distributions:</strong> The joint probability distribution over <span class="math inline">\(v\)</span> and <span class="math inline">\(h\)</span> is given by: <span class="math display">\[
P(v, h) = \frac{e^{-E(v, h)}}{Z}
\]</span> where <span class="math inline">\(Z\)</span> is the partition function.</li>
</ul></li>
<li><strong>Contrastive Divergence (CD-k):</strong>
<ul>
<li><strong>Algorithm:</strong>
<ol type="1">
<li><strong>Initialization:</strong> Start with the data sample <span class="math inline">\(v_0\)</span>.</li>
<li><strong>Gibbs Sampling:</strong> Perform <span class="math inline">\(k\)</span> steps of Gibbs sampling to obtain a sample <span class="math inline">\(\tilde{v}\)</span> from the model distribution.</li>
<li><strong>Parameter Update:</strong> Update the parameters using the difference between data-dependent and model-dependent expectations. <span class="math display">\[
  \Delta W = \langle v_0 h_0^T \rangle - \langle \tilde{v} \tilde{h}^T \rangle
  \]</span> where <span class="math inline">\(\langle \cdot \rangle\)</span> denotes expectation, and <span class="math inline">\(h_0\)</span>, <span class="math inline">\(\tilde{h}\)</span> are the hidden states corresponding to <span class="math inline">\(v_0\)</span> and <span class="math inline">\(\tilde{v}\)</span>.</li>
</ol></li>
</ul></li>
<li><strong>Advantages:</strong>
<ul>
<li>Efficient and simple approximation of the log-likelihood gradient.</li>
<li>Scalable to large datasets with the use of mini-batches.</li>
</ul></li>
<li><strong>Disadvantages:</strong>
<ul>
<li>Approximation can be biased, especially with small <span class="math inline">\(k\)</span>.</li>
<li>Requires careful tuning of hyperparameters and learning rate.</li>
</ul></li>
<li><strong>Applications:</strong>
<ul>
<li>Feature learning and dimensionality reduction.</li>
<li>Collaborative filtering and recommendation systems.</li>
<li>Pretraining deep networks.</li>
</ul></li>
</ul>
</section>
<section id="noise-contrastive-estimation" class="level4">
<h4 class="anchored" data-anchor-id="noise-contrastive-estimation">34.3.2. Noise-Contrastive Estimation</h4>
<p>Noise-Contrastive Estimation (NCE) is a method for estimating the parameters of unnormalized statistical models by using a noise distribution for comparison.</p>
<ul>
<li><strong>Key Concepts:</strong>
<ul>
<li><strong>Energy Function:</strong> Defines the unnormalized probability of a data point <span class="math inline">\(x\)</span>. <span class="math display">\[
P(x; \theta) = \frac{e^{-E(x; \theta)}}{Z(\theta)}
\]</span> where <span class="math inline">\(E(x; \theta)\)</span> is the energy function and <span class="math inline">\(Z(\theta)\)</span> is the partition function.</li>
<li><strong>Noise Distribution:</strong> A known distribution <span class="math inline">\(P_n(x)\)</span> used as a reference to contrast against the model distribution.</li>
</ul></li>
<li><strong>Noise-Contrastive Estimation (NCE):</strong>
<ul>
<li><strong>Training Objective:</strong> Formulate the training as a binary classification problem to distinguish data samples from noise samples. <span class="math display">\[
D = \left\{ (x_i, 1) \right\}_{i=1}^{N} \cup \left\{ (y_j, 0) \right\}_{j=1}^{M}
\]</span> where <span class="math inline">\(x_i\)</span> are data samples and <span class="math inline">\(y_j\)</span> are noise samples.</li>
<li><strong>Logistic Regression:</strong> Fit a logistic regression model to classify data and noise. <span class="math display">\[
\log \frac{P(x)}{P_n(x)} = E_n(x) - E(x)
\]</span></li>
<li><strong>Loss Function:</strong> Minimize the negative log-likelihood of the logistic regression. <span class="math display">\[
\mathcal{L}(\theta) = -\sum_{i=1}^N \log \sigma(E_n(x_i) - E(x_i)) - \sum_{j=1}^M \log \sigma(E(y_j) - E_n(y_j))
\]</span> where <span class="math inline">\(\sigma(\cdot)\)</span> is the sigmoid function.</li>
</ul></li>
<li><strong>Advantages:</strong>
<ul>
<li>Avoids the need to compute the partition function.</li>
<li>Can be applied to any unnormalized model.</li>
<li>Scalable and efficient for large datasets.</li>
</ul></li>
<li><strong>Disadvantages:</strong>
<ul>
<li>Performance depends on the choice of noise distribution.</li>
<li>Requires a large number of noise samples for accurate estimation.</li>
</ul></li>
<li><strong>Applications:</strong>
<ul>
<li>Natural language processing, especially word embeddings.</li>
<li>Density estimation in high-dimensional spaces.</li>
<li>Training generative models without explicit normalization.</li>
</ul></li>
</ul>
<p>By leveraging Contrastive Divergence and Noise-Contrastive Estimation, energy-based models can effectively learn complex data distributions, making them powerful tools for various applications in generative modeling, feature learning, and beyond.</p>
</section>
</section>
<section id="neural-radiance-fields-nerf" class="level3">
<h3 class="anchored" data-anchor-id="neural-radiance-fields-nerf">34.4. Neural Radiance Fields (NeRF)</h3>
<p>Neural Radiance Fields (NeRF) are a groundbreaking approach for synthesizing novel views of complex 3D scenes. NeRFs use a fully connected deep neural network to model the volumetric scene representation and render highly realistic images from arbitrary viewpoints.</p>
<section id="nerf-for-novel-view-synthesis" class="level4">
<h4 class="anchored" data-anchor-id="nerf-for-novel-view-synthesis">34.4.1. NeRF for Novel View Synthesis</h4>
<p>NeRFs can generate novel views of a scene by learning the 3D scene structure and appearance from a set of 2D images.</p>
<ul>
<li><p><strong>Key Concepts:</strong></p>
<ul>
<li><strong>Radiance Field:</strong> A continuous function that represents the color and density of a scene at any point in space. <span class="math display">\[
F_\theta: (\mathbf{x}, \mathbf{d}) \rightarrow (\mathbf{c}, \sigma)
\]</span> where <span class="math inline">\(\mathbf{x}\)</span> is a 3D point, <span class="math inline">\(\mathbf{d}\)</span> is a 2D viewing direction, <span class="math inline">\(\mathbf{c}\)</span> is the RGB color, and <span class="math inline">\(\sigma\)</span> is the volume density.</li>
</ul></li>
<li><p><strong>Ray Marching:</strong> Integrate the radiance field along camera rays to compute the pixel values.</p>
<ul>
<li><strong>Volume Rendering:</strong> Compute the color <span class="math inline">\(C(\mathbf{r})\)</span> of a ray <span class="math inline">\(\mathbf{r}(t) = \mathbf{o} + t\mathbf{d}\)</span> using: <span class="math display">\[
C(\mathbf{r}) = \int_{t_n}^{t_f} T(t) \sigma(\mathbf{r}(t)) \mathbf{c}(\mathbf{r}(t), \mathbf{d}) dt
\]</span> where <span class="math inline">\(T(t) = \exp \left( -\int_{t_n}^t \sigma(\mathbf{r}(s)) ds \right)\)</span> is the accumulated transmittance.</li>
</ul></li>
<li><p><strong>Training Objective:</strong> Minimize the difference between the rendered pixel values and the ground truth images. <span class="math display">\[
\mathcal{L} = \sum_{\mathbf{r} \in \mathcal{R}} \|C(\mathbf{r}) - \hat{C}(\mathbf{r})\|^2
\]</span></p></li>
<li><p><strong>Advantages:</strong></p>
<ul>
<li>Produces highly realistic and detailed novel views.</li>
<li>Captures fine geometric and appearance details of the scene.</li>
</ul></li>
<li><p><strong>Disadvantages:</strong></p>
<ul>
<li>Computationally expensive due to the high-resolution volumetric rendering.</li>
<li>Requires a dense set of input images for training.</li>
</ul></li>
<li><p><strong>Applications:</strong></p>
<ul>
<li>3D reconstruction and virtual reality.</li>
<li>Visual effects and game development.</li>
<li>Robotics and autonomous systems.</li>
</ul></li>
</ul>
</section>
<section id="dynamic-nerf" class="level4">
<h4 class="anchored" data-anchor-id="dynamic-nerf">34.4.2. Dynamic NeRF</h4>
<p>Dynamic NeRFs extend the original NeRF framework to handle dynamic scenes with temporal changes.</p>
<ul>
<li><p><strong>Key Concepts:</strong></p>
<ul>
<li><strong>Time-dependent Radiance Field:</strong> Introduce time as an additional input to the radiance field function. <span class="math display">\[
F_\theta: (\mathbf{x}, \mathbf{d}, t) \rightarrow (\mathbf{c}, \sigma)
\]</span> where <span class="math inline">\(t\)</span> represents time.</li>
</ul></li>
<li><p><strong>Handling Motion:</strong> Model the motion of objects or the entire scene by incorporating temporal information.</p>
<ul>
<li><strong>Deformation Fields:</strong> Learn a deformation field that maps points from the canonical space to the current space at time <span class="math inline">\(t\)</span>. <span class="math display">\[
\mathbf{x}_t = \mathbf{x} + \mathbf{d}_\theta(\mathbf{x}, t)
\]</span></li>
</ul></li>
<li><p><strong>Training Objective:</strong> Extend the original loss function to account for temporal changes. <span class="math display">\[
\mathcal{L} = \sum_{\mathbf{r} \in \mathcal{R}, t \in \mathcal{T}} \|C(\mathbf{r}, t) - \hat{C}(\mathbf{r}, t)\|^2
\]</span></p></li>
<li><p><strong>Advantages:</strong></p>
<ul>
<li>Captures complex temporal dynamics in scenes.</li>
<li>Enables rendering of dynamic scenes with high fidelity.</li>
</ul></li>
<li><p><strong>Disadvantages:</strong></p>
<ul>
<li>Increased computational complexity due to the additional temporal dimension.</li>
<li>Requires temporal consistency in input data.</li>
</ul></li>
<li><p><strong>Applications:</strong></p>
<ul>
<li>Animation and special effects in films.</li>
<li>Simulation of time-varying phenomena.</li>
<li>Time-lapse photography and video generation.</li>
</ul></li>
</ul>
</section>
<section id="generalizable-nerf" class="level4">
<h4 class="anchored" data-anchor-id="generalizable-nerf">34.4.3. Generalizable NeRF</h4>
<p>Generalizable NeRFs aim to extend the original NeRF framework to handle new, unseen scenes without retraining.</p>
<ul>
<li><p><strong>Key Concepts:</strong></p>
<ul>
<li><strong>Meta-learning:</strong> Train a model that can quickly adapt to new scenes with a few samples. <span class="math display">\[
\theta^* = \arg\min_\theta \sum_{i} \mathcal{L}_i(F_\theta)
\]</span> where <span class="math inline">\(\mathcal{L}_i\)</span> is the loss for scene <span class="math inline">\(i\)</span>.</li>
</ul></li>
<li><p><strong>Feature Embeddings:</strong> Encode scene-specific information into a latent space, which the radiance field can condition on.</p>
<ul>
<li><strong>Scene Embeddings:</strong> Learn a scene-specific embedding <span class="math inline">\(\mathbf{z}_i\)</span> for each scene. <span class="math display">\[
F_\theta: (\mathbf{x}, \mathbf{d}, \mathbf{z}_i) \rightarrow (\mathbf{c}, \sigma)
\]</span></li>
</ul></li>
<li><p><strong>Training Objective:</strong> Minimize the loss over multiple scenes to enable generalization. <span class="math display">\[
\mathcal{L} = \sum_{i} \sum_{\mathbf{r} \in \mathcal{R}_i} \|C(\mathbf{r}; \mathbf{z}_i) - \hat{C}(\mathbf{r})\|^2
\]</span></p></li>
<li><p><strong>Advantages:</strong></p>
<ul>
<li>Can quickly adapt to new scenes with few examples.</li>
<li>Reduces the need for extensive retraining for each new scene.</li>
</ul></li>
<li><p><strong>Disadvantages:</strong></p>
<ul>
<li>May require a large and diverse training dataset to achieve good generalization.</li>
<li>Potentially lower fidelity compared to scene-specific NeRFs.</li>
</ul></li>
<li><p><strong>Applications:</strong></p>
<ul>
<li>Augmented reality and mixed reality applications.</li>
<li>Interactive 3D content creation.</li>
<li>Rapid prototyping and visualization.</li>
</ul></li>
</ul>
<p>By understanding and utilizing NeRFs, including novel view synthesis, dynamic NeRFs, and generalizable NeRFs, researchers and practitioners can push the boundaries of 3D scene representation and rendering, enabling highly realistic and versatile applications in various fields.</p>
</section>
</section>
<section id="neural-radiance-fields-nerf-1" class="level3">
<h3 class="anchored" data-anchor-id="neural-radiance-fields-nerf-1">34.4. Neural Radiance Fields (NeRF)</h3>
<p>Neural Radiance Fields (NeRF) are a groundbreaking approach for synthesizing novel views of complex 3D scenes. NeRFs use a fully connected deep neural network to model the volumetric scene representation and render highly realistic images from arbitrary viewpoints.</p>
<section id="nerf-for-novel-view-synthesis-1" class="level4">
<h4 class="anchored" data-anchor-id="nerf-for-novel-view-synthesis-1">34.4.1. NeRF for Novel View Synthesis</h4>
<p>NeRFs can generate novel views of a scene by learning the 3D scene structure and appearance from a set of 2D images.</p>
<ul>
<li><p><strong>Key Concepts:</strong></p>
<ul>
<li><strong>Radiance Field:</strong> A continuous function that represents the color and density of a scene at any point in space. <span class="math display">\[
F_\theta: (\mathbf{x}, \mathbf{d}) \rightarrow (\mathbf{c}, \sigma)
\]</span> where <span class="math inline">\(\mathbf{x}\)</span> is a 3D point, <span class="math inline">\(\mathbf{d}\)</span> is a 2D viewing direction, <span class="math inline">\(\mathbf{c}\)</span> is the RGB color, and <span class="math inline">\(\sigma\)</span> is the volume density.</li>
</ul></li>
<li><p><strong>Ray Marching:</strong> Integrate the radiance field along camera rays to compute the pixel values.</p>
<ul>
<li><strong>Volume Rendering:</strong> Compute the color <span class="math inline">\(C(\mathbf{r})\)</span> of a ray <span class="math inline">\(\mathbf{r}(t) = \mathbf{o} + t\mathbf{d}\)</span> using: <span class="math display">\[
C(\mathbf{r}) = \int_{t_n}^{t_f} T(t) \sigma(\mathbf{r}(t)) \mathbf{c}(\mathbf{r}(t), \mathbf{d}) dt
\]</span> where <span class="math inline">\(T(t) = \exp \left( -\int_{t_n}^t \sigma(\mathbf{r}(s)) ds \right)\)</span> is the accumulated transmittance.</li>
</ul></li>
<li><p><strong>Training Objective:</strong> Minimize the difference between the rendered pixel values and the ground truth images. <span class="math display">\[
\mathcal{L} = \sum_{\mathbf{r} \in \mathcal{R}} \|C(\mathbf{r}) - \hat{C}(\mathbf{r})\|^2
\]</span></p></li>
<li><p><strong>Advantages:</strong></p>
<ul>
<li>Produces highly realistic and detailed novel views.</li>
<li>Captures fine geometric and appearance details of the scene.</li>
</ul></li>
<li><p><strong>Disadvantages:</strong></p>
<ul>
<li>Computationally expensive due to the high-resolution volumetric rendering.</li>
<li>Requires a dense set of input images for training.</li>
</ul></li>
<li><p><strong>Applications:</strong></p>
<ul>
<li>3D reconstruction and virtual reality.</li>
<li>Visual effects and game development.</li>
<li>Robotics and autonomous systems.</li>
</ul></li>
</ul>
</section>
<section id="dynamic-nerf-1" class="level4">
<h4 class="anchored" data-anchor-id="dynamic-nerf-1">34.4.2. Dynamic NeRF</h4>
<p>Dynamic NeRFs extend the original NeRF framework to handle dynamic scenes with temporal changes.</p>
<ul>
<li><p><strong>Key Concepts:</strong></p>
<ul>
<li><strong>Time-dependent Radiance Field:</strong> Introduce time as an additional input to the radiance field function. <span class="math display">\[
F_\theta: (\mathbf{x}, \mathbf{d}, t) \rightarrow (\mathbf{c}, \sigma)
\]</span> where <span class="math inline">\(t\)</span> represents time.</li>
</ul></li>
<li><p><strong>Handling Motion:</strong> Model the motion of objects or the entire scene by incorporating temporal information.</p>
<ul>
<li><strong>Deformation Fields:</strong> Learn a deformation field that maps points from the canonical space to the current space at time <span class="math inline">\(t\)</span>. <span class="math display">\[
\mathbf{x}_t = \mathbf{x} + \mathbf{d}_\theta(\mathbf{x}, t)
\]</span></li>
</ul></li>
<li><p><strong>Training Objective:</strong> Extend the original loss function to account for temporal changes. <span class="math display">\[
\mathcal{L} = \sum_{\mathbf{r} \in \mathcal{R}, t \in \mathcal{T}} \|C(\mathbf{r}, t) - \hat{C}(\mathbf{r}, t)\|^2
\]</span></p></li>
<li><p><strong>Advantages:</strong></p>
<ul>
<li>Captures complex temporal dynamics in scenes.</li>
<li>Enables rendering of dynamic scenes with high fidelity.</li>
</ul></li>
<li><p><strong>Disadvantages:</strong></p>
<ul>
<li>Increased computational complexity due to the additional temporal dimension.</li>
<li>Requires temporal consistency in input data.</li>
</ul></li>
<li><p><strong>Applications:</strong></p>
<ul>
<li>Animation and special effects in films.</li>
<li>Simulation of time-varying phenomena.</li>
<li>Time-lapse photography and video generation.</li>
</ul></li>
</ul>
</section>
<section id="generalizable-nerf-1" class="level4">
<h4 class="anchored" data-anchor-id="generalizable-nerf-1">34.4.3. Generalizable NeRF</h4>
<p>Generalizable NeRFs aim to extend the original NeRF framework to handle new, unseen scenes without retraining.</p>
<ul>
<li><p><strong>Key Concepts:</strong></p>
<ul>
<li><strong>Meta-learning:</strong> Train a model that can quickly adapt to new scenes with a few samples. <span class="math display">\[
\theta^* = \arg\min_\theta \sum_{i} \mathcal{L}_i(F_\theta)
\]</span> where <span class="math inline">\(\mathcal{L}_i\)</span> is the loss for scene <span class="math inline">\(i\)</span>.</li>
</ul></li>
<li><p><strong>Feature Embeddings:</strong> Encode scene-specific information into a latent space, which the radiance field can condition on.</p>
<ul>
<li><strong>Scene Embeddings:</strong> Learn a scene-specific embedding <span class="math inline">\(\mathbf{z}_i\)</span> for each scene. <span class="math display">\[
F_\theta: (\mathbf{x}, \mathbf{d}, \mathbf{z}_i) \rightarrow (\mathbf{c}, \sigma)
\]</span></li>
</ul></li>
<li><p><strong>Training Objective:</strong> Minimize the loss over multiple scenes to enable generalization. <span class="math display">\[
\mathcal{L} = \sum_{i} \sum_{\mathbf{r} \in \mathcal{R}_i} \|C(\mathbf{r}; \mathbf{z}_i) - \hat{C}(\mathbf{r})\|^2
\]</span></p></li>
<li><p><strong>Advantages:</strong></p>
<ul>
<li>Can quickly adapt to new scenes with few examples.</li>
<li>Reduces the need for extensive retraining for each new scene.</li>
</ul></li>
<li><p><strong>Disadvantages:</strong></p>
<ul>
<li>May require a large and diverse training dataset to achieve good generalization.</li>
<li>Potentially lower fidelity compared to scene-specific NeRFs.</li>
</ul></li>
<li><p><strong>Applications:</strong></p>
<ul>
<li>Augmented reality and mixed reality applications.</li>
<li>Interactive 3D content creation.</li>
<li>Rapid prototyping and visualization.</li>
</ul></li>
</ul>
<p>By understanding and utilizing NeRFs, including novel view synthesis, dynamic NeRFs, and generalizable NeRFs, researchers and practitioners can push the boundaries of 3D scene representation and rendering, enabling highly realistic and versatile applications in various fields.</p>
</section>
</section>
<section id="implicit-neural-representations" class="level3">
<h3 class="anchored" data-anchor-id="implicit-neural-representations">34.5. Implicit Neural Representations</h3>
<p>Implicit neural representations (INRs) use neural networks to represent continuous signals and data structures, such as images, audio, and 3D shapes, without relying on explicit grid-based data structures. INRs offer a flexible and powerful approach for high-fidelity data representation and reconstruction.</p>
<section id="key-concepts" class="level4">
<h4 class="anchored" data-anchor-id="key-concepts">Key Concepts</h4>
<ul>
<li><p><strong>Implicit Function:</strong> An implicit function represents data as a continuous function parameterized by a neural network. <span class="math display">\[
f_\theta: \mathbf{x} \rightarrow \mathbf{y}
\]</span> where <span class="math inline">\(\mathbf{x}\)</span> is the input coordinate (e.g., a spatial location), <span class="math inline">\(\mathbf{y}\)</span> is the output value (e.g., color or occupancy), and <span class="math inline">\(\theta\)</span> denotes the neural network parameters.</p></li>
<li><p><strong>Advantages:</strong></p>
<ul>
<li><strong>Continuous Representation:</strong> Provides smooth and continuous representation of data, capturing fine details and complex structures.</li>
<li><strong>Compactness:</strong> Requires less memory compared to explicit representations, especially for high-resolution data.</li>
<li><strong>Scalability:</strong> Easily scalable to higher dimensions without a significant increase in memory usage.</li>
</ul></li>
<li><p><strong>Disadvantages:</strong></p>
<ul>
<li><strong>Training Complexity:</strong> Training implicit neural representations can be computationally intensive and require large amounts of data.</li>
<li><strong>Inference Speed:</strong> Evaluating the function at numerous points can be slow, affecting real-time applications.</li>
</ul></li>
</ul>
</section>
<section id="applications" class="level4">
<h4 class="anchored" data-anchor-id="applications">Applications</h4>
<ul>
<li><strong>3D Shape Representation:</strong>
<ul>
<li><p><strong>Occupancy Networks:</strong> Model the occupancy probability of points in 3D space, representing 3D shapes implicitly. <span class="math display">\[
f_\theta: \mathbf{x} \rightarrow p(\mathbf{x})
\]</span> where <span class="math inline">\(p(\mathbf{x})\)</span> is the probability that point <span class="math inline">\(\mathbf{x}\)</span> is inside the shape.</p></li>
<li><p><strong>Signed Distance Functions (SDFs):</strong> Represent 3D shapes by learning the signed distance from any point in space to the surface of the shape. <span class="math display">\[
f_\theta: \mathbf{x} \rightarrow d(\mathbf{x})
\]</span> where <span class="math inline">\(d(\mathbf{x})\)</span> is the signed distance to the surface.</p></li>
</ul></li>
<li><strong>Image Representation:</strong>
<ul>
<li><strong>Implicit Neural Representations for Images:</strong> Model the pixel values of an image as a continuous function of 2D coordinates. <span class="math display">\[
f_\theta: (u, v) \rightarrow \mathbf{c}
\]</span> where <span class="math inline">\((u, v)\)</span> are the 2D coordinates and <span class="math inline">\(\mathbf{c}\)</span> is the RGB color.</li>
</ul></li>
<li><strong>Audio Representation:</strong>
<ul>
<li><strong>Neural Audio Representation:</strong> Represent audio signals as continuous functions of time, capturing high-fidelity audio with neural networks. <span class="math display">\[
f_\theta: t \rightarrow a(t)
\]</span> where <span class="math inline">\(t\)</span> is the time and <span class="math inline">\(a(t)\)</span> is the audio amplitude.</li>
</ul></li>
</ul>
</section>
<section id="advanced-techniques" class="level4">
<h4 class="anchored" data-anchor-id="advanced-techniques">Advanced Techniques</h4>
<ul>
<li><p><strong>Fourier Features:</strong> Enhance the capacity of implicit neural representations to capture high-frequency details by encoding inputs with Fourier features. <span class="math display">\[
\gamma(\mathbf{x}) = [\sin(2\pi \mathbf{B} \mathbf{x}), \cos(2\pi \mathbf{B} \mathbf{x})]
\]</span> where <span class="math inline">\(\mathbf{B}\)</span> is a fixed, randomly initialized matrix.</p></li>
<li><p><strong>Meta-learning for Implicit Representations:</strong> Train a meta-model that can quickly adapt to new data, improving the generalization capability of implicit neural representations.</p>
<ul>
<li><strong>Meta-SDF:</strong> Learn a meta-model to represent signed distance functions of various shapes. <span class="math display">\[
f_{\theta, \phi}: (\mathbf{x}, \mathbf{z}) \rightarrow d(\mathbf{x})
\]</span> where <span class="math inline">\(\mathbf{z}\)</span> is a shape-specific latent code, and <span class="math inline">\(\theta, \phi\)</span> are the meta-model parameters.</li>
</ul></li>
</ul>
</section>
<section id="training-objectives" class="level4">
<h4 class="anchored" data-anchor-id="training-objectives">Training Objectives</h4>
<ul>
<li><p><strong>Reconstruction Loss:</strong> Minimize the difference between the predicted and ground truth values. <span class="math display">\[
\mathcal{L}_{\text{recon}} = \sum_{i} \| f_\theta(\mathbf{x}_i) - \mathbf{y}_i \|^2
\]</span></p></li>
<li><p><strong>Regularization:</strong> Apply regularization techniques to ensure smoothness and avoid overfitting.</p>
<ul>
<li><strong>Gradient Regularization:</strong> Penalize large gradients to enforce smoothness in the representation. <span class="math display">\[
\mathcal{L}_{\text{grad}} = \sum_{i} \| \nabla f_\theta(\mathbf{x}_i) \|^2
\]</span></li>
</ul></li>
</ul>
<p>By leveraging implicit neural representations, researchers and practitioners can achieve high-fidelity and memory-efficient representations of complex data structures, enabling advancements in various fields such as computer graphics, signal processing, and beyond.</p>
</section>
</section>
<section id="adversarial-generative-models" class="level3">
<h3 class="anchored" data-anchor-id="adversarial-generative-models">34.6. Adversarial Generative Models</h3>
<p>Adversarial generative models, primarily Generative Adversarial Networks (GANs), utilize a two-network framework to generate realistic data samples. The generator creates data samples, while the discriminator evaluates them, fostering an adversarial training process that improves the quality of generated data.</p>
<section id="stylegan-and-stylegan2" class="level4">
<h4 class="anchored" data-anchor-id="stylegan-and-stylegan2">34.6.1. StyleGAN and StyleGAN2</h4>
<p>StyleGAN and its successor, StyleGAN2, are advanced GAN architectures that achieve high-quality image synthesis by incorporating style-based generators.</p>
<ul>
<li><strong>Key Concepts:</strong>
<ul>
<li><strong>Mapping Network:</strong> Projects the input latent code <span class="math inline">\(z\)</span> to an intermediate latent space <span class="math inline">\(w\)</span>, allowing for more disentangled representations. <span class="math display">\[
w = f(z)
\]</span></li>
<li><strong>Style Modulation:</strong> Modulates the intermediate features at each layer using the latent code <span class="math inline">\(w\)</span>. <span class="math display">\[
\text{AdaIN}(x, w) = \gamma(w) \frac{x - \mu(x)}{\sigma(x)} + \beta(w)
\]</span> where <span class="math inline">\(\gamma(w)\)</span> and <span class="math inline">\(\beta(w)\)</span> are learned affine transformations.</li>
<li><strong>Progressive Growing:</strong> Trains the generator and discriminator progressively, starting from low resolution and gradually increasing to higher resolutions.</li>
</ul></li>
<li><strong>StyleGAN2 Improvements:</strong>
<ul>
<li><strong>Revised AdaIN:</strong> Eliminates artifacts by removing normalization and improving the style modulation.</li>
<li><strong>Path Length Regularization:</strong> Ensures that the generator’s outputs are stable by regularizing the path length in the latent space.</li>
<li><strong>Weight Demodulation:</strong> Prevents artifacts by normalizing the weights in each convolutional layer.</li>
</ul></li>
<li><strong>Advantages:</strong>
<ul>
<li>Generates high-resolution and high-quality images.</li>
<li>Allows for fine control over generated images through style modulation.</li>
</ul></li>
<li><strong>Disadvantages:</strong>
<ul>
<li>Computationally intensive training process.</li>
<li>Requires large amounts of data for optimal performance.</li>
</ul></li>
<li><strong>Applications:</strong>
<ul>
<li>High-quality image synthesis.</li>
<li>Facial attribute editing and manipulation.</li>
<li>Art and content generation.</li>
</ul></li>
</ul>
</section>
<section id="biggan" class="level4">
<h4 class="anchored" data-anchor-id="biggan">34.6.2. BigGAN</h4>
<p>BigGAN extends the GAN framework to generate high-fidelity images at large scales by incorporating architectural and training enhancements.</p>
<ul>
<li><strong>Key Concepts:</strong>
<ul>
<li><strong>Large Batch Training:</strong> Uses large batch sizes to stabilize training and improve the quality of generated images.</li>
<li><strong>Orthogonal Regularization:</strong> Applies orthogonal regularization to the weights of the generator to promote diversity in the generated samples.</li>
<li><strong>Class-Conditional Generation:</strong> Conditions the generator on class labels to produce class-specific images, improving diversity and quality. <span class="math display">\[
\mathbf{h} = \text{concat}(z, y)
\]</span> where <span class="math inline">\(z\)</span> is the noise vector and <span class="math inline">\(y\)</span> is the class label.</li>
</ul></li>
<li><strong>Architecture Enhancements:</strong>
<ul>
<li><strong>Self-Attention Mechanism:</strong> Incorporates self-attention layers to capture long-range dependencies in the generated images.</li>
<li><strong>Spectral Normalization:</strong> Normalizes the weights in both the generator and discriminator to stabilize training.</li>
</ul></li>
<li><strong>Advantages:</strong>
<ul>
<li>Produces high-resolution images with fine details.</li>
<li>Scalable to large and complex datasets.</li>
</ul></li>
<li><strong>Disadvantages:</strong>
<ul>
<li>Requires significant computational resources.</li>
<li>Training can be challenging and sensitive to hyperparameters.</li>
</ul></li>
<li><strong>Applications:</strong>
<ul>
<li>Image generation for diverse classes.</li>
<li>Data augmentation for training robust models.</li>
<li>High-resolution art and media content creation.</li>
</ul></li>
</ul>
</section>
</section>
<section id="transformer-based-generative-models" class="level3">
<h3 class="anchored" data-anchor-id="transformer-based-generative-models">34.7. Transformer-based Generative Models</h3>
<p>Transformer-based generative models leverage the self-attention mechanism of transformers to model complex dependencies in sequential data, making them suitable for tasks like text, image, and audio generation.</p>
<section id="image-gpt" class="level4">
<h4 class="anchored" data-anchor-id="image-gpt">34.7.1. Image GPT</h4>
<p>Image GPT adapts the GPT (Generative Pre-trained Transformer) framework to image generation, treating images as sequences of pixels.</p>
<ul>
<li><strong>Key Concepts:</strong>
<ul>
<li><strong>Pixel Sequence:</strong> Flattens the 2D image into a 1D sequence of pixels and models the distribution using a transformer. <span class="math display">\[
\mathbf{x} = [x_1, x_2, \ldots, x_n]
\]</span></li>
<li><strong>Autoregressive Generation:</strong> Generates images pixel by pixel, conditioned on previously generated pixels. <span class="math display">\[
p(\mathbf{x}) = \prod_{i=1}^{n} p(x_i | x_1, x_2, \ldots, x_{i-1})
\]</span></li>
</ul></li>
<li><strong>Training Objective:</strong>
<ul>
<li><strong>Maximize Likelihood:</strong> Train the model to maximize the likelihood of the pixel sequences. <span class="math display">\[
\mathcal{L} = -\sum_{i=1}^{n} \log p(x_i | x_1, x_2, \ldots, x_{i-1})
\]</span></li>
</ul></li>
<li><strong>Advantages:</strong>
<ul>
<li>Captures complex dependencies in pixel sequences.</li>
<li>Scalable to high-dimensional data.</li>
</ul></li>
<li><strong>Disadvantages:</strong>
<ul>
<li>Computationally intensive due to sequential processing.</li>
<li>High memory requirements for large images.</li>
</ul></li>
<li><strong>Applications:</strong>
<ul>
<li>High-quality image synthesis.</li>
<li>Image inpainting and completion.</li>
<li>Artistic content generation.</li>
</ul></li>
</ul>
</section>
<section id="dall-e-and-dall-e-2" class="level4">
<h4 class="anchored" data-anchor-id="dall-e-and-dall-e-2">34.7.2. DALL-E and DALL-E 2</h4>
<p>DALL-E and DALL-E 2 are transformer-based models designed for generating images from textual descriptions, demonstrating the power of combining language and vision.</p>
<ul>
<li><strong>Key Concepts:</strong>
<ul>
<li><strong>Text-to-Image Generation:</strong> Conditions the image generation process on textual descriptions, enabling the creation of images that match the given text. <span class="math display">\[
p(\mathbf{x} | \mathbf{t}) = \prod_{i=1}^{n} p(x_i | x_1, x_2, \ldots, x_{i-1}, \mathbf{t})
\]</span> where <span class="math inline">\(\mathbf{t}\)</span> is the textual description.</li>
</ul></li>
<li><strong>DALL-E Architecture:</strong>
<ul>
<li><strong>Discrete VAE (dVAE):</strong> Encodes images into discrete latent variables, which are then decoded by the transformer.</li>
<li><strong>Transformer Decoder:</strong> Generates sequences of discrete latent variables conditioned on text.</li>
</ul></li>
<li><strong>DALL-E 2 Improvements:</strong>
<ul>
<li><strong>CLIP Embeddings:</strong> Uses CLIP (Contrastive Language-Image Pre-training) embeddings to better align text and image representations.</li>
<li><strong>Diffusion Models:</strong> Integrates diffusion models for high-quality image generation with improved coherence and diversity.</li>
</ul></li>
<li><strong>Advantages:</strong>
<ul>
<li>Generates coherent and diverse images from textual descriptions.</li>
<li>Integrates language and vision effectively.</li>
</ul></li>
<li><strong>Disadvantages:</strong>
<ul>
<li>Requires large-scale datasets for training.</li>
<li>Computationally demanding and resource-intensive.</li>
</ul></li>
<li><strong>Applications:</strong>
<ul>
<li>Creative and artistic content generation.</li>
<li>Visualizing concepts from textual descriptions.</li>
<li>Generating illustrations for storytelling and education.</li>
</ul></li>
</ul>
<p>By leveraging adversarial generative models and transformer-based generative models, researchers and practitioners can push the boundaries of generative modeling, enabling the creation of highly realistic and versatile data across various domains. These models open new possibilities in creative industries, data augmentation, and beyond.</p>
</section>
</section>
<section id="generative-models-for-3d-data" class="level3">
<h3 class="anchored" data-anchor-id="generative-models-for-3d-data">34.8. Generative Models for 3D Data</h3>
<p>Generative models for 3D data are designed to create, reconstruct, and manipulate three-dimensional objects and scenes. These models are essential in fields such as computer graphics, virtual reality, augmented reality, and 3D printing.</p>
<section id="key-concepts-1" class="level4">
<h4 class="anchored" data-anchor-id="key-concepts-1">Key Concepts</h4>
<ul>
<li><strong>3D Representations:</strong> 3D data can be represented in various forms, including voxels, point clouds, meshes, and implicit functions. Each representation has its strengths and trade-offs in terms of detail, memory usage, and computational complexity.</li>
</ul>
</section>
<section id="voxel-based-generative-models" class="level4">
<h4 class="anchored" data-anchor-id="voxel-based-generative-models">34.8.1. Voxel-based Generative Models</h4>
<p>Voxel-based models represent 3D objects as a grid of discrete units, each containing information about the object’s presence and properties at that location.</p>
<ul>
<li><strong>3D Convolutional Neural Networks (3D-CNNs):</strong> Extend 2D convolutional layers to 3D, enabling the processing of voxel grids.
<ul>
<li><strong>Architecture:</strong> <span class="math display">\[
V' = \text{3D-CNN}(V)
\]</span> where <span class="math inline">\(V\)</span> is the input voxel grid and <span class="math inline">\(V'\)</span> is the output.</li>
<li><strong>Applications:</strong>
<ul>
<li>3D object generation.</li>
<li>Medical imaging (e.g., MRI, CT scans).</li>
<li>3D scene reconstruction.</li>
</ul></li>
</ul></li>
</ul>
</section>
<section id="point-cloud-generative-models" class="level4">
<h4 class="anchored" data-anchor-id="point-cloud-generative-models">34.8.2. Point Cloud Generative Models</h4>
<p>Point clouds represent 3D objects as a set of discrete points in space, each with its coordinates and possibly additional features (e.g., color, normal vectors).</p>
<ul>
<li><strong>PointNet and PointNet++:</strong> Neural networks designed to handle unordered point sets, capturing both global and local features.
<ul>
<li><strong>Architecture:</strong> <span class="math display">\[
P' = \text{PointNet}(P)
\]</span> where <span class="math inline">\(P\)</span> is the input point cloud and <span class="math inline">\(P'\)</span> is the generated point cloud.</li>
<li><strong>Applications:</strong>
<ul>
<li>3D object recognition.</li>
<li>Autonomous driving (e.g., LiDAR data processing).</li>
<li>Environmental mapping and reconstruction.</li>
</ul></li>
</ul></li>
</ul>
</section>
<section id="mesh-based-generative-models" class="level4">
<h4 class="anchored" data-anchor-id="mesh-based-generative-models">34.8.3. Mesh-based Generative Models</h4>
<p>Meshes represent 3D objects using vertices, edges, and faces, providing a compact and detailed representation of surface geometry.</p>
<ul>
<li><strong>Generative Adversarial Networks (GANs) for Meshes:</strong> Extend GANs to generate 3D meshes by learning vertex positions and connectivity.
<ul>
<li><strong>Architecture:</strong> <span class="math display">\[
M' = \text{MeshGAN}(M)
\]</span> where <span class="math inline">\(M\)</span> is the input mesh and <span class="math inline">\(M'\)</span> is the generated mesh.</li>
<li><strong>Applications:</strong>
<ul>
<li>Character modeling in animation and games.</li>
<li>3D printing.</li>
<li>Architectural visualization.</li>
</ul></li>
</ul></li>
</ul>
</section>
<section id="implicit-function-based-models" class="level4">
<h4 class="anchored" data-anchor-id="implicit-function-based-models">34.8.4. Implicit Function-based Models</h4>
<p>Implicit function-based models represent 3D objects as continuous functions that define the shape, such as occupancy functions or signed distance functions (SDFs).</p>
<ul>
<li><strong>DeepSDF (Signed Distance Function):</strong> Uses a neural network to learn the signed distance function of a 3D shape.
<ul>
<li><strong>Architecture:</strong> <span class="math display">\[
d(\mathbf{x}) = f_\theta(\mathbf{x})
\]</span> where <span class="math inline">\(\mathbf{x}\)</span> is a 3D point and <span class="math inline">\(d(\mathbf{x})\)</span> is the signed distance to the surface.</li>
<li><strong>Applications:</strong>
<ul>
<li>High-fidelity 3D shape representation.</li>
<li>Shape interpolation and morphing.</li>
<li>Robust to varying resolutions and scales.</li>
</ul></li>
</ul></li>
</ul>
</section>
<section id="hybrid-models" class="level4">
<h4 class="anchored" data-anchor-id="hybrid-models">34.8.5. Hybrid Models</h4>
<p>Hybrid models combine different representations to leverage their strengths, such as combining point clouds and meshes or using both voxels and implicit functions.</p>
<ul>
<li><p><strong>Architecture:</strong> <span class="math display">\[
H = \text{HybridModel}(V, P, M)
\]</span> where <span class="math inline">\(V\)</span>, <span class="math inline">\(P\)</span>, and <span class="math inline">\(M\)</span> are voxel, point cloud, and mesh representations, respectively, and <span class="math inline">\(H\)</span> is the hybrid model output.</p></li>
<li><p><strong>Applications:</strong></p>
<ul>
<li>Complex scene reconstruction.</li>
<li>Multi-modal 3D data fusion.</li>
<li>Enhanced 3D object generation and manipulation.</li>
</ul></li>
</ul>
</section>
<section id="training-and-evaluation" class="level4">
<h4 class="anchored" data-anchor-id="training-and-evaluation">Training and Evaluation</h4>
<ul>
<li><strong>Training Objectives:</strong> Typically involve minimizing reconstruction loss, adversarial loss, or a combination of both.
<ul>
<li><strong>Reconstruction Loss:</strong> Measures the difference between the generated and ground truth 3D data. <span class="math display">\[
\mathcal{L}_{\text{recon}} = \sum_{i} \| G_\theta(z_i) - x_i \|^2
\]</span> where <span class="math inline">\(G_\theta\)</span> is the generative model, <span class="math inline">\(z_i\)</span> is the latent code, and <span class="math inline">\(x_i\)</span> is the ground truth data.</li>
<li><strong>Adversarial Loss:</strong> Used in GANs to improve the realism of generated 3D data. <span class="math display">\[
\mathcal{L}_{\text{adv}} = \mathbb{E}_{x \sim p_\text{data}}[\log D(x)] + \mathbb{E}_{z \sim p_z}[\log (1 - D(G(z)))]
\]</span> where <span class="math inline">\(D\)</span> is the discriminator.</li>
</ul></li>
<li><strong>Evaluation Metrics:</strong> Common metrics include Intersection over Union (IoU), Chamfer Distance, Earth Mover’s Distance (EMD), and visual inspection for quality and realism.</li>
</ul>
<p>By leveraging advanced generative models for 3D data, researchers and practitioners can achieve high-fidelity 3D object and scene generation, reconstruction, and manipulation, enabling new possibilities in computer graphics, virtual reality, augmented reality, and beyond.</p>
</section>
</section>
<section id="controllable-generation" class="level3">
<h3 class="anchored" data-anchor-id="controllable-generation">34.9. Controllable Generation</h3>
<p>Controllable generation refers to the capability of generative models to allow users to influence the output according to specified attributes or conditions. This is essential in applications requiring user input or constraints, such as personalized content creation, style transfer, and targeted data augmentation.</p>
<section id="key-concepts-2" class="level4">
<h4 class="anchored" data-anchor-id="key-concepts-2">Key Concepts</h4>
<ul>
<li><p><strong>Conditional Generative Models:</strong> Models that generate data based on input conditions or attributes. <span class="math display">\[
p(x|c)
\]</span> where <span class="math inline">\(x\)</span> is the generated data and <span class="math inline">\(c\)</span> is the condition or attribute.</p></li>
<li><p><strong>Attribute Manipulation:</strong> Changing specific attributes of the generated data while keeping other aspects unchanged.</p></li>
</ul>
</section>
<section id="conditional-generative-adversarial-networks-cgans" class="level4">
<h4 class="anchored" data-anchor-id="conditional-generative-adversarial-networks-cgans">34.9.1. Conditional Generative Adversarial Networks (cGANs)</h4>
<p>Conditional GANs extend the GAN framework to incorporate conditional information, enabling controlled generation based on input conditions.</p>
<ul>
<li><strong>Architecture:</strong>
<ul>
<li><strong>Generator:</strong> <span class="math display">\[
G(z, c)
\]</span> where <span class="math inline">\(z\)</span> is the noise vector and <span class="math inline">\(c\)</span> is the conditional vector.</li>
<li><strong>Discriminator:</strong> <span class="math display">\[
D(x, c)
\]</span> where <span class="math inline">\(x\)</span> is the real or generated data and <span class="math inline">\(c\)</span> is the condition.</li>
</ul></li>
<li><strong>Training Objective:</strong>
<ul>
<li><strong>Generator Loss:</strong> <span class="math display">\[
\mathcal{L}_G = \mathbb{E}_{z \sim p_z, c \sim p_c} [\log (1 - D(G(z, c), c))]
\]</span></li>
<li><strong>Discriminator Loss:</strong> <span class="math display">\[
\mathcal{L}_D = \mathbb{E}_{x \sim p_{\text{data}}, c \sim p_c} [\log D(x, c)] + \mathbb{E}_{z \sim p_z, c \sim p_c} [\log (1 - D(G(z, c), c))]
\]</span></li>
</ul></li>
<li><strong>Applications:</strong>
<ul>
<li><strong>Image Synthesis:</strong> Generate images conditioned on labels, such as generating specific objects or scenes.</li>
<li><strong>Style Transfer:</strong> Transfer specific styles to images while preserving content.</li>
<li><strong>Data Augmentation:</strong> Create variations of data based on specified conditions.</li>
</ul></li>
</ul>
</section>
<section id="controllable-variational-autoencoders-cvae" class="level4">
<h4 class="anchored" data-anchor-id="controllable-variational-autoencoders-cvae">34.9.2. Controllable Variational Autoencoders (CVAE)</h4>
<p>CVAE extends the VAE framework to include conditional information, enabling the generation of data conditioned on attributes.</p>
<ul>
<li><strong>Architecture:</strong>
<ul>
<li><strong>Encoder:</strong> <span class="math display">\[
q_\phi(z|x, c)
\]</span> where <span class="math inline">\(x\)</span> is the input data and <span class="math inline">\(c\)</span> is the condition.</li>
<li><strong>Decoder:</strong> <span class="math display">\[
p_\theta(x|z, c)
\]</span> where <span class="math inline">\(z\)</span> is the latent representation and <span class="math inline">\(c\)</span> is the condition.</li>
</ul></li>
<li><strong>Training Objective:</strong>
<ul>
<li><strong>ELBO:</strong> <span class="math display">\[
\mathcal{L}_{\text{ELBO}} = \mathbb{E}_{q_\phi(z|x, c)} [\log p_\theta(x|z, c)] - D_{\text{KL}}(q_\phi(z|x, c) \| p(z))
\]</span></li>
</ul></li>
<li><strong>Applications:</strong>
<ul>
<li><strong>Text Generation:</strong> Generate text with specific sentiment or style.</li>
<li><strong>Speech Synthesis:</strong> Create speech samples with specified characteristics, such as accent or emotion.</li>
<li><strong>Personalized Content Creation:</strong> Generate content tailored to user preferences.</li>
</ul></li>
</ul>
</section>
<section id="disentangled-representations" class="level4">
<h4 class="anchored" data-anchor-id="disentangled-representations">34.9.3. Disentangled Representations</h4>
<p>Disentangled representations separate the underlying factors of variation in data, enabling control over specific attributes.</p>
<ul>
<li><strong>Key Concepts:</strong>
<ul>
<li><strong>Factor Disentanglement:</strong> Each latent variable represents a distinct factor of variation.</li>
<li><strong>Factor Control:</strong> Manipulate individual factors without affecting others.</li>
</ul></li>
<li><strong>Training Approaches:</strong>
<ul>
<li><strong><span class="math inline">\(\beta\)</span>-VAE:</strong> Introduces a hyperparameter <span class="math inline">\(\beta\)</span> to control the degree of disentanglement. <span class="math display">\[
\mathcal{L}_{\beta\text{VAE}} = \mathbb{E}_{q_\phi(z|x)} [\log p_\theta(x|z)] - \beta D_{\text{KL}}(q_\phi(z|x) \| p(z))
\]</span></li>
<li><strong>InfoGAN:</strong> Maximizes mutual information between latent variables and their corresponding observations. <span class="math display">\[
\mathcal{L}_{\text{InfoGAN}} = \mathcal{L}_{\text{GAN}} - \lambda I(c; G(z, c))
\]</span> where <span class="math inline">\(I(c; G(z, c))\)</span> is the mutual information between conditions <span class="math inline">\(c\)</span> and generated data <span class="math inline">\(G(z, c)\)</span>.</li>
</ul></li>
<li><strong>Applications:</strong>
<ul>
<li><strong>Face Editing:</strong> Modify specific attributes like hair color, expression, or age.</li>
<li><strong>3D Object Generation:</strong> Control aspects like shape, size, or pose.</li>
<li><strong>Artistic Creation:</strong> Vary styles or elements within generated artworks.</li>
</ul></li>
</ul>
</section>
<section id="attention-mechanisms-for-control" class="level4">
<h4 class="anchored" data-anchor-id="attention-mechanisms-for-control">34.9.4. Attention Mechanisms for Control</h4>
<p>Attention mechanisms can be integrated into generative models to focus on specific parts of the input or condition, enhancing controllability.</p>
<ul>
<li><strong>Key Concepts:</strong>
<ul>
<li><strong>Attention Weights:</strong> Determine the importance of different parts of the input or condition. <span class="math display">\[
\alpha_i = \frac{\exp(e_i)}{\sum_{j} \exp(e_j)}
\]</span> where <span class="math inline">\(e_i\)</span> are the attention scores.</li>
<li><strong>Weighted Sum:</strong> Compute a weighted sum of input features based on attention weights. <span class="math display">\[
\mathbf{h} = \sum_{i} \alpha_i \mathbf{x}_i
\]</span></li>
</ul></li>
<li><strong>Applications:</strong>
<ul>
<li><strong>Image Inpainting:</strong> Focus on relevant regions for filling in missing parts.</li>
<li><strong>Text Generation:</strong> Attend to specific words or phrases to maintain context.</li>
<li><strong>Speech Synthesis:</strong> Emphasize certain phonemes or syllables for natural prosody.</li>
</ul></li>
</ul>
</section>
<section id="evaluation-metrics-for-controllability" class="level4">
<h4 class="anchored" data-anchor-id="evaluation-metrics-for-controllability">34.9.5. Evaluation Metrics for Controllability</h4>
<p>Evaluating controllability involves measuring how well the generated data adheres to specified conditions and the diversity of the generated outputs.</p>
<ul>
<li><strong>Key Metrics:</strong>
<ul>
<li><strong>Conditional Accuracy:</strong> The accuracy of generated samples matching the specified conditions.</li>
<li><strong>Diversity:</strong> The variability of generated samples within the specified condition.</li>
<li><strong>Mutual Information:</strong> The mutual information between conditions and generated data.</li>
</ul></li>
</ul>
<p>By leveraging these techniques and concepts, controllable generation enables the creation of highly customizable and user-driven data, enhancing applications across various domains, from creative industries to data-driven sciences.</p>
</section>
</section>
<section id="evaluation-metrics-for-generative-models" class="level3">
<h3 class="anchored" data-anchor-id="evaluation-metrics-for-generative-models">34.10. Evaluation Metrics for Generative Models</h3>
<p>Evaluating generative models is crucial to understanding their performance and the quality of the generated data. Various metrics can be employed to assess different aspects of generative models, including the fidelity, diversity, and adherence to specific conditions or attributes.</p>
<section id="fidelity-metrics" class="level4">
<h4 class="anchored" data-anchor-id="fidelity-metrics">34.10.1. Fidelity Metrics</h4>
<p>Fidelity metrics assess how closely the generated data matches the real data in terms of quality and realism.</p>
<ul>
<li><strong>Inception Score (IS):</strong>
<ul>
<li>Measures the quality and diversity of generated images using a pre-trained Inception network. <span class="math display">\[
IS(G) = \exp \left( \mathbb{E}_{\mathbf{x} \sim p_g} D_{\text{KL}}(p(y|\mathbf{x}) \| p(y)) \right)
\]</span> where <span class="math inline">\(p(y|\mathbf{x})\)</span> is the conditional label distribution predicted by the Inception model, and <span class="math inline">\(p(y)\)</span> is the marginal distribution over all generated samples.</li>
</ul></li>
<li><strong>Frechet Inception Distance (FID):</strong>
<ul>
<li>Compares the statistics of real and generated images using the activations of a pre-trained Inception network. <span class="math display">\[
\text{FID} = \| \mu_r - \mu_g \|^2 + \text{Tr}(\Sigma_r + \Sigma_g - 2(\Sigma_r \Sigma_g)^{1/2})
\]</span> where <span class="math inline">\((\mu_r, \Sigma_r)\)</span> and <span class="math inline">\((\mu_g, \Sigma_g)\)</span> are the mean and covariance of the real and generated data feature activations, respectively.</li>
</ul></li>
<li><strong>Kernel Inception Distance (KID):</strong>
<ul>
<li>A kernel-based alternative to FID that uses polynomial kernels to measure the similarity between real and generated data distributions. <span class="math display">\[
\text{KID} = \mathbb{E}_{x, x' \sim p_r} [k(x, x')] + \mathbb{E}_{y, y' \sim p_g} [k(y, y')] - 2\mathbb{E}_{x \sim p_r, y \sim p_g} [k(x, y)]
\]</span> where <span class="math inline">\(k\)</span> is the polynomial kernel function.</li>
</ul></li>
</ul>
</section>
<section id="diversity-metrics" class="level4">
<h4 class="anchored" data-anchor-id="diversity-metrics">34.10.2. Diversity Metrics</h4>
<p>Diversity metrics assess the variability within the generated data to ensure it captures the full range of possible outputs.</p>
<ul>
<li><strong>Mode Score:</strong>
<ul>
<li>Extends the Inception Score to account for both fidelity and diversity by incorporating the KL divergence between the real and generated label distributions. <span class="math display">\[
\text{Mode Score} = \exp \left( \mathbb{E}_{\mathbf{x} \sim p_g} D_{\text{KL}}(p(y|\mathbf{x}) \| p(y)) \right) \cdot \exp \left( D_{\text{KL}}(p(y) \| q(y)) \right)
\]</span> where <span class="math inline">\(q(y)\)</span> is the empirical distribution of labels in the real data.</li>
</ul></li>
<li><strong>Multi-scale Structural Similarity Index Measure (MS-SSIM):</strong>
<ul>
<li>Evaluates the structural similarity between pairs of images, with higher values indicating greater diversity. <span class="math display">\[
\text{MS-SSIM}(x, y) = \prod_{j=1}^{M} \text{SSIM}(x_j, y_j)
\]</span> where <span class="math inline">\(x_j\)</span> and <span class="math inline">\(y_j\)</span> are the images at scale <span class="math inline">\(j\)</span>, and <span class="math inline">\(M\)</span> is the total number of scales.</li>
</ul></li>
</ul>
</section>
<section id="conditional-metrics" class="level4">
<h4 class="anchored" data-anchor-id="conditional-metrics">34.10.3. Conditional Metrics</h4>
<p>Conditional metrics evaluate how well the generated data adheres to specified conditions or attributes.</p>
<ul>
<li><strong>Conditional Inception Score (CIS):</strong>
<ul>
<li>Extends the Inception Score by conditioning on attributes or classes. <span class="math display">\[
\text{CIS} = \exp \left( \mathbb{E}_{\mathbf{x} \sim p_g} \mathbb{E}_{c \sim p(c|\mathbf{x})} D_{\text{KL}}(p(y|\mathbf{x}, c) \| p(y|c)) \right)
\]</span> where <span class="math inline">\(p(c|\mathbf{x})\)</span> is the conditional distribution over attributes.</li>
</ul></li>
<li><strong>Frechet Classifier Distance (FCD):</strong>
<ul>
<li>Similar to FID but uses class-specific feature statistics. <span class="math display">\[
\text{FCD} = \frac{1}{C} \sum_{c=1}^{C} \left( \| \mu_r^c - \mu_g^c \|^2 + \text{Tr}(\Sigma_r^c + \Sigma_g^c - 2(\Sigma_r^c \Sigma_g^c)^{1/2}) \right)
\]</span> where <span class="math inline">\((\mu_r^c, \Sigma_r^c)\)</span> and <span class="math inline">\((\mu_g^c, \Sigma_g^c)\)</span> are the class-specific mean and covariance of the real and generated data feature activations, respectively.</li>
</ul></li>
</ul>
</section>
<section id="human-evaluation-metrics" class="level4">
<h4 class="anchored" data-anchor-id="human-evaluation-metrics">34.10.4. Human Evaluation Metrics</h4>
<p>Human evaluation involves subjective assessment by human evaluators to judge the quality and realism of generated data.</p>
<ul>
<li><strong>Mean Opinion Score (MOS):</strong>
<ul>
<li>Human raters provide scores for generated samples on a scale (e.g., 1 to 5), with higher scores indicating better quality. <span class="math display">\[
\text{MOS} = \frac{1}{N} \sum_{i=1}^{N} \text{score}_i
\]</span></li>
</ul></li>
<li><strong>Pairwise Comparison:</strong>
<ul>
<li>Human raters compare pairs of generated samples and select the one they find more realistic or higher quality.</li>
<li><strong>Win Rate:</strong> The proportion of times a sample from a particular model is chosen as the better sample.</li>
</ul></li>
</ul>
</section>
<section id="application-specific-metrics" class="level4">
<h4 class="anchored" data-anchor-id="application-specific-metrics">34.10.5. Application-specific Metrics</h4>
<p>Different applications may require specific metrics tailored to the nature of the data and the desired outcomes.</p>
<ul>
<li><strong>Perceptual Path Length (PPL):</strong>
<ul>
<li>Measures the smoothness of the latent space by evaluating the perceptual differences along linear interpolations in the latent space. <span class="math display">\[
\text{PPL} = \mathbb{E}_{z_1, z_2} \left[ \frac{1}{\epsilon^2} d(G(z_1), G(z_2)) \right]
\]</span> where <span class="math inline">\(d\)</span> is a perceptual distance metric, and <span class="math inline">\(\epsilon\)</span> is a small step size.</li>
</ul></li>
<li><strong>Intersection over Union (IoU) for 3D Data:</strong>
<ul>
<li>Evaluates the overlap between generated and ground truth 3D shapes. <span class="math display">\[
\text{IoU} = \frac{|V_{\text{gen}} \cap V_{\text{real}}|}{|V_{\text{gen}} \cup V_{\text{real}}|}
\]</span> where <span class="math inline">\(V_{\text{gen}}\)</span> and <span class="math inline">\(V_{\text{real}}\)</span> are the voxel representations of the generated and real shapes.</li>
</ul></li>
</ul>
<p>By employing these evaluation metrics, researchers and practitioners can comprehensively assess the performance of generative models, ensuring they produce high-quality, diverse, and conditionally accurate outputs suitable for various applications.</p>
</section>
</section>
</div>



</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>