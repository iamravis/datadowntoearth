<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.47">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Ravi Shankar">

<title>chapter10_unsupervised_learning – Data Down To Earth</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../">
<link href="../../../logo.png" rel="icon" type="image/png">
<script src="../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../../styles.css">
<link rel="stylesheet" href="../../content.css">
<meta property="og:title" content="– Data Down To Earth">
<meta property="og:description" content="">
<meta property="og:site_name" content="Data Down To Earth">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="../../../index.html" class="navbar-brand navbar-brand-logo">
    <img src="../../../logo.png" alt="" class="navbar-logo">
    </a>
    <a class="navbar-brand" href="../../../index.html">
    <span class="navbar-title">Data Down To Earth</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-projects" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Projects</span>
    </a>
    <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="nav-menu-projects">    
        <li>
    <a class="dropdown-item" href="../../../projects/dsml_projects.html">
 <span class="dropdown-text">Data Science and ML Projects</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../projects/genai_projects.html">
 <span class="dropdown-text">Generative AI Projects</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-tutorials" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Tutorials</span>
    </a>
    <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="nav-menu-tutorials">    
        <li>
    <a class="dropdown-item" href="../../../topics/statistics.html">
 <span class="dropdown-text">Statistics &amp; Probability</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../topics/machine_learning.html">
 <span class="dropdown-text">Machine Learning</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../topics/de.html">
 <span class="dropdown-text">Data Engineering</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../topics/generative_ai.html">
 <span class="dropdown-text">Generative AI</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../topics/public_health.html">
 <span class="dropdown-text">Public Health</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../topics/product_sense.html">
 <span class="dropdown-text">Product Sense</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item">
    <a class="nav-link" href="../../../blogs/blogs.html"> 
<span class="menu-text">Blog</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/iamrsps"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/iamravishankar/"> <i class="bi bi-linkedin" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content column-page" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
</div>



<div class="quarto-title-meta column-page">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Ravi Shankar </p>
          </div>
  </div>
    
  
    
  </div>
  


</header>


<section id="chapter-10.-clustering-techniques" class="level1 text-content">
<h1>Chapter 10. Clustering Techniques</h1>
<p>Clustering is an unsupervised learning technique used to group similar data points together. The goal is to partition the dataset into distinct clusters where data points within each cluster are more similar to each other than to those in other clusters.</p>
<section id="k-means-clustering" class="level2">
<h2 class="anchored" data-anchor-id="k-means-clustering">10.1. K-means Clustering</h2>
<p>K-means is one of the most popular clustering algorithms. It aims to partition the dataset into K clusters by minimizing the within-cluster variance.</p>
<section id="algorithm-details-and-implementation" class="level3">
<h3 class="anchored" data-anchor-id="algorithm-details-and-implementation">10.1.1. Algorithm Details and Implementation</h3>
<p>The K-means algorithm follows these steps:</p>
<ol type="1">
<li><strong>Initialization:</strong> Choose K initial cluster centroids randomly or using a specific method such as K-means++.</li>
<li><strong>Assignment Step:</strong> Assign each data point to the nearest centroid based on Euclidean distance.</li>
<li><strong>Update Step:</strong> Recalculate the centroids as the mean of all data points assigned to each cluster.</li>
<li><strong>Repeat:</strong> Repeat the assignment and update steps until the centroids no longer change or the maximum number of iterations is reached.</li>
</ol>
<ul>
<li><strong>Detailed Steps:</strong>
<ul>
<li><strong>Initialization:</strong>
<ul>
<li>Randomly select K points from the dataset as initial centroids.</li>
<li>K-means++ can be used to select initial centroids to speed up convergence by spreading out the initial points.</li>
</ul></li>
<li><strong>Assignment Step:</strong>
<ul>
<li>For each data point, compute the distance to each centroid.</li>
<li>Assign the data point to the cluster with the nearest centroid.</li>
</ul></li>
<li><strong>Update Step:</strong>
<ul>
<li>For each cluster, calculate the new centroid by taking the mean of all data points assigned to that cluster.</li>
</ul></li>
<li><strong>Convergence Check:</strong>
<ul>
<li>Check if the centroids have changed. If not, the algorithm has converged.</li>
<li>If the centroids have changed, repeat the assignment and update steps.</li>
</ul></li>
</ul></li>
</ul>
</section>
<section id="choosing-the-optimal-k" class="level3">
<h3 class="anchored" data-anchor-id="choosing-the-optimal-k">10.1.2. Choosing the Optimal K</h3>
<p>Choosing the optimal number of clusters (K) is crucial for effective clustering. Several methods can be used to determine the best K.</p>
<section id="elbow-method" class="level4">
<h4 class="anchored" data-anchor-id="elbow-method">10.1.2.1. Elbow Method</h4>
<p>The Elbow Method involves plotting the within-cluster sum of squares (WCSS) against the number of clusters and looking for an “elbow point” where the WCSS begins to decrease at a slower rate.</p>
<ul>
<li><strong>Steps:</strong>
<ol type="1">
<li>Run K-means with different values of K.</li>
<li>Calculate the WCSS for each K.</li>
<li>Plot WCSS against K and look for the point where the decrease in WCSS slows down, indicating the optimal K.</li>
</ol></li>
<li><strong>Advantages:</strong>
<ul>
<li>Simple and intuitive to use.</li>
<li>Provides a visual representation of the trade-off between the number of clusters and WCSS.</li>
</ul></li>
<li><strong>Disadvantages:</strong>
<ul>
<li>The “elbow point” can be subjective and not always clear.</li>
<li>May not always provide a definitive optimal K.</li>
</ul></li>
</ul>
</section>
<section id="silhouette-analysis" class="level4">
<h4 class="anchored" data-anchor-id="silhouette-analysis">10.1.2.2. Silhouette Analysis</h4>
<p>Silhouette analysis measures how similar a data point is to its own cluster compared to other clusters. The silhouette score ranges from -1 to 1, where a high value indicates that the data point is well matched to its own cluster and poorly matched to neighboring clusters.</p>
<ul>
<li><strong>Steps:</strong>
<ol type="1">
<li>Calculate the silhouette coefficient for each data point.</li>
<li>Average the silhouette coefficients to get the overall silhouette score for different values of K.</li>
<li>Choose the K with the highest average silhouette score.</li>
</ol></li>
<li><strong>Advantages:</strong>
<ul>
<li>Provides a quantitative measure of clustering quality.</li>
<li>Helps in selecting the number of clusters that best fits the data.</li>
</ul></li>
<li><strong>Disadvantages:</strong>
<ul>
<li>Computationally intensive for large datasets.</li>
<li>Sensitive to noise and outliers.</li>
</ul></li>
</ul>
</section>
<section id="gap-statistic" class="level4">
<h4 class="anchored" data-anchor-id="gap-statistic">10.1.2.3. Gap Statistic</h4>
<p>The Gap Statistic compares the total within-cluster variation for different numbers of clusters with their expected values under null reference distribution of the data.</p>
<ul>
<li><strong>Steps:</strong>
<ol type="1">
<li>Run K-means for different values of K and compute the WCSS for each K.</li>
<li>Generate B reference datasets by sampling from a uniform distribution over the range of the data.</li>
<li>Run K-means on each reference dataset for each value of K and compute the WCSS.</li>
<li>Calculate the gap statistic for each K: <span class="math display">\[
\text{Gap}(K) = \frac{1}{B} \sum_{b=1}^{B} \log(\text{WCSS}_{b}(K)) - \log(\text{WCSS}(K))
\]</span></li>
<li>Choose the K that maximizes the Gap statistic.</li>
</ol></li>
<li><strong>Advantages:</strong>
<ul>
<li>Provides a statistically grounded method for selecting the number of clusters.</li>
<li>Can be more reliable than visual methods like the Elbow Method.</li>
</ul></li>
<li><strong>Disadvantages:</strong>
<ul>
<li>Computationally expensive, especially for large datasets and many reference datasets.</li>
<li>Requires careful implementation to ensure accurate results.</li>
</ul></li>
</ul>
</section>
</section>
<section id="k-means" class="level3">
<h3 class="anchored" data-anchor-id="k-means">10.1.3. K-means++</h3>
<p>K-means++ is an initialization algorithm for K-means that aims to improve the clustering result by spreading out the initial cluster centroids.</p>
<ul>
<li><strong>Steps:</strong>
<ol type="1">
<li>Choose the first centroid randomly from the data points.</li>
<li>For each data point, compute its distance to the nearest chosen centroid.</li>
<li>Select the next centroid with a probability proportional to the square of the distance to the nearest existing centroid.</li>
<li>Repeat steps 2-3 until K centroids have been chosen.</li>
</ol></li>
<li><strong>Advantages:</strong>
<ul>
<li>Leads to faster convergence and better final clustering results compared to random initialization.</li>
<li>Reduces the likelihood of poor clustering results due to bad initial centroids.</li>
</ul></li>
<li><strong>Disadvantages:</strong>
<ul>
<li>Slightly more computationally intensive than random initialization due to the distance calculations.</li>
</ul></li>
</ul>
</section>
<section id="mini-batch-k-means" class="level3">
<h3 class="anchored" data-anchor-id="mini-batch-k-means">10.1.4. Mini-batch K-means</h3>
<p>Mini-batch K-means is a variation of the K-means algorithm that uses small, random subsets of the data (mini-batches) to update the cluster centroids, making the algorithm more scalable and faster on large datasets.</p>
<ul>
<li><strong>Steps:</strong>
<ol type="1">
<li>Initialize K centroids randomly or using K-means++.</li>
<li>Select a random mini-batch of data points.</li>
<li>Assign each data point in the mini-batch to the nearest centroid.</li>
<li>Update the centroids based on the mini-batch assignments.</li>
<li>Repeat steps 2-4 until convergence or for a fixed number of iterations.</li>
</ol></li>
<li><strong>Advantages:</strong>
<ul>
<li>Significantly faster than standard K-means, especially on large datasets.</li>
<li>Reduces memory requirements as only a mini-batch needs to be in memory at a time.</li>
</ul></li>
<li><strong>Disadvantages:</strong>
<ul>
<li>May lead to less accurate clustering compared to standard K-means due to the approximation introduced by mini-batches.</li>
<li>Requires careful tuning of the mini-batch size.</li>
</ul></li>
</ul>
<p>By understanding and applying these methods and enhancements, you can effectively use K-means clustering for various data analysis tasks, ensuring that you choose the optimal number of clusters and achieve high-quality clustering results.</p>
</section>
</section>
<section id="hierarchical-clustering" class="level2">
<h2 class="anchored" data-anchor-id="hierarchical-clustering">10.2. Hierarchical Clustering</h2>
<p>Hierarchical clustering builds a hierarchy of clusters by either merging smaller clusters into larger ones (agglomerative) or splitting larger clusters into smaller ones (divisive). It does not require specifying the number of clusters in advance and provides a dendrogram to visualize the cluster structure.</p>
<section id="agglomerative-clustering" class="level3">
<h3 class="anchored" data-anchor-id="agglomerative-clustering">10.2.1. Agglomerative Clustering</h3>
<p>Agglomerative clustering, also known as bottom-up clustering, starts with each data point as its own cluster and merges the closest pairs of clusters iteratively until all points are in a single cluster or a stopping criterion is met.</p>
<ul>
<li><p><strong>Steps:</strong></p>
<ol type="1">
<li><strong>Initialize</strong>: Start with each data point as a separate cluster.</li>
<li><strong>Merge Closest Clusters</strong>: Find the pair of clusters with the smallest distance between them and merge them into a single cluster.</li>
<li><strong>Update Distances</strong>: Recalculate the distances between the new cluster and all other clusters.</li>
<li><strong>Repeat</strong>: Repeat steps 2 and 3 until a single cluster remains or the desired number of clusters is achieved.</li>
</ol></li>
<li><p><strong>Advantages</strong>: Does not require specifying the number of clusters in advance. Creates a comprehensive hierarchy of clusters.</p></li>
<li><p><strong>Disadvantages</strong>: Computationally expensive for large datasets. Sensitive to noise and outliers.</p></li>
</ul>
</section>
<section id="divisive-clustering" class="level3">
<h3 class="anchored" data-anchor-id="divisive-clustering">10.2.2. Divisive Clustering</h3>
<p>Divisive clustering, also known as top-down clustering, starts with all data points in a single cluster and splits them into smaller clusters iteratively until each data point is its own cluster or a stopping criterion is met.</p>
<ul>
<li><p><strong>Steps:</strong></p>
<ol type="1">
<li><strong>Initialize</strong>: Start with all data points in a single cluster.</li>
<li><strong>Split Cluster</strong>: Identify the cluster that is most dissimilar and split it into two smaller clusters.</li>
<li><strong>Update Distances</strong>: Recalculate the distances between the new clusters and all other clusters.</li>
<li><strong>Repeat</strong>: Repeat steps 2 and 3 until each data point is in its own cluster or the desired number of clusters is achieved.</li>
</ol></li>
<li><p><strong>Advantages</strong>: Can provide more accurate clusters for certain datasets. Provides a comprehensive hierarchy of clusters.</p></li>
<li><p><strong>Disadvantages</strong>: Computationally intensive, especially for large datasets. Selecting the right split can be challenging.</p></li>
</ul>
</section>
<section id="linkage-methods" class="level3">
<h3 class="anchored" data-anchor-id="linkage-methods">10.2.3. Linkage Methods</h3>
<p>Linkage methods determine how the distance between clusters is calculated during the merging or splitting process in hierarchical clustering. Different linkage methods can lead to different clustering results.</p>
<section id="single-linkage" class="level4">
<h4 class="anchored" data-anchor-id="single-linkage">10.2.3.1. Single Linkage</h4>
<ul>
<li><p><strong>Single Linkage</strong>: Also known as the nearest neighbor method, it defines the distance between two clusters as the minimum distance between any single pair of points in the two clusters.</p>
<ul>
<li><p><strong>Formula</strong>: <span class="math display">\[
d(C_i, C_j) = \min_{x \in C_i, y \in C_j} d(x, y)
\]</span></p></li>
<li><p><strong>Advantages</strong>: Simple to implement. Can handle non-elliptical shapes of clusters.</p></li>
<li><p><strong>Disadvantages</strong>: Can result in “chaining” where clusters can form long, snake-like structures.</p></li>
</ul></li>
</ul>
</section>
<section id="complete-linkage" class="level4">
<h4 class="anchored" data-anchor-id="complete-linkage">10.2.3.2. Complete Linkage</h4>
<ul>
<li><p><strong>Complete Linkage</strong>: Also known as the farthest neighbor method, it defines the distance between two clusters as the maximum distance between any single pair of points in the two clusters.</p>
<ul>
<li><p><strong>Formula</strong>: <span class="math display">\[
d(C_i, C_j) = \max_{x \in C_i, y \in C_j} d(x, y)
\]</span></p></li>
<li><p><strong>Advantages</strong>: Tends to create more compact clusters. Less sensitive to noise and outliers compared to single linkage.</p></li>
<li><p><strong>Disadvantages</strong>: Can be computationally expensive. May break up large clusters.</p></li>
</ul></li>
</ul>
</section>
<section id="average-linkage" class="level4">
<h4 class="anchored" data-anchor-id="average-linkage">10.2.3.3. Average Linkage</h4>
<ul>
<li><p><strong>Average Linkage</strong>: Defines the distance between two clusters as the average distance between all pairs of points in the two clusters.</p>
<ul>
<li><p><strong>Formula</strong>: <span class="math display">\[
d(C_i, C_j) = \frac{1}{|C_i| |C_j|} \sum_{x \in C_i} \sum_{y \in C_j} d(x, y)
\]</span></p></li>
<li><p><strong>Advantages</strong>: Balances between single and complete linkage. Produces clusters with similar variance.</p></li>
<li><p><strong>Disadvantages</strong>: Computationally intensive for large datasets. Sensitive to the definition of distance.</p></li>
</ul></li>
</ul>
</section>
<section id="wards-method" class="level4">
<h4 class="anchored" data-anchor-id="wards-method">10.2.3.4. Ward’s Method</h4>
<ul>
<li><p><strong>Ward’s Method</strong>: Minimizes the total within-cluster variance. At each step, it merges the pair of clusters that leads to the minimum increase in total within-cluster variance after merging.</p>
<ul>
<li><p><strong>Formula</strong>: <span class="math display">\[
d(C_i, C_j) = \sum_{x \in C_i \cup C_j} (x - \mu_{C_i \cup C_j})^2 - \sum_{x \in C_i} (x - \mu_{C_i})^2 - \sum_{x \in C_j} (x - \mu_{C_j})^2
\]</span></p></li>
<li><p><strong>Advantages</strong>: Tends to produce compact and spherical clusters. Handles outliers better than other methods.</p></li>
<li><p><strong>Disadvantages</strong>: Computationally expensive. Assumes clusters have similar sizes.</p></li>
</ul></li>
</ul>
</section>
</section>
<section id="dendrograms-and-cluster-interpretation" class="level3">
<h3 class="anchored" data-anchor-id="dendrograms-and-cluster-interpretation">10.2.4. Dendrograms and Cluster Interpretation</h3>
<ul>
<li><p><strong>Dendrograms</strong>: A tree-like diagram that records the sequences of merges or splits in hierarchical clustering. Each branch represents a cluster, and the height of the branch represents the distance at which the clusters are merged or split.</p>
<ul>
<li><strong>Steps to Create a Dendrogram</strong>:
<ol type="1">
<li>Perform hierarchical clustering on the dataset using a chosen linkage method.</li>
<li>Plot the dendrogram, where the x-axis represents the data points and the y-axis represents the distance or dissimilarity.</li>
</ol></li>
<li><strong>Interpreting Dendrograms</strong>:
<ul>
<li>The height of the branches indicates the dissimilarity between merged clusters.</li>
<li>The number of branches at a certain height can be used to determine the number of clusters.</li>
<li>Cutting the dendrogram at a chosen height provides a specific clustering solution.</li>
</ul></li>
<li><strong>Advantages</strong>:
<ul>
<li>Provides a visual summary of the clustering process.</li>
<li>Helps in deciding the number of clusters by visual inspection.</li>
</ul></li>
<li><strong>Disadvantages</strong>:
<ul>
<li>Can be difficult to interpret for large datasets.</li>
<li>Sensitive to the choice of linkage method.</li>
</ul></li>
</ul></li>
</ul>
<p>By understanding and applying these hierarchical clustering methods and techniques, you can effectively analyze and interpret complex datasets, uncovering meaningful patterns and relationships within the data.</p>
</section>
</section>
<section id="dbscan-density-based-spatial-clustering-of-applications-with-noise" class="level2">
<h2 class="anchored" data-anchor-id="dbscan-density-based-spatial-clustering-of-applications-with-noise">10.3. DBSCAN (Density-Based Spatial Clustering of Applications with Noise)</h2>
<p>DBSCAN is a density-based clustering algorithm that groups together points that are closely packed together while marking points that lie alone in low-density regions as outliers. It is particularly effective for discovering clusters of arbitrary shape and dealing with noise.</p>
<section id="core-points-border-points-and-noise-points" class="level3">
<h3 class="anchored" data-anchor-id="core-points-border-points-and-noise-points">10.3.1. Core Points, Border Points, and Noise Points</h3>
<ul>
<li><p><strong>Core Points:</strong> A point is a core point if it has at least a minimum number of points (MinPts) within a given distance (<span class="math inline">\(\epsilon\)</span>) from it. These points are at the heart of a cluster.</p>
<ul>
<li><p><strong>Definition:</strong> For a point <span class="math inline">\(p\)</span> in a dataset, <span class="math inline">\(p\)</span> is a core point if there are at least MinPts points within a distance <span class="math inline">\(\epsilon\)</span> from <span class="math inline">\(p\)</span>, including <span class="math inline">\(p\)</span> itself.</p></li>
<li><p><strong>Example:</strong> If <span class="math inline">\(\epsilon = 0.5\)</span> and MinPts = 4, then a point <span class="math inline">\(p\)</span> is a core point if there are at least 4 points (including <span class="math inline">\(p\)</span>) within a radius of 0.5 units from <span class="math inline">\(p\)</span>.</p></li>
</ul></li>
<li><p><strong>Border Points:</strong> A point is a border point if it is not a core point but lies within the <span class="math inline">\(\epsilon\)</span> distance of a core point. These points are on the edge of a cluster.</p>
<ul>
<li><p><strong>Definition:</strong> For a point <span class="math inline">\(p\)</span> in a dataset, <span class="math inline">\(p\)</span> is a border point if it is within the <span class="math inline">\(\epsilon\)</span> distance of a core point but does not have enough neighbors to be a core point itself.</p></li>
<li><p><strong>Example:</strong> If <span class="math inline">\(\epsilon = 0.5\)</span> and MinPts = 4, a point <span class="math inline">\(q\)</span> is a border point if it is within 0.5 units of a core point <span class="math inline">\(p\)</span> but does not have at least 4 neighbors within 0.5 units.</p></li>
</ul></li>
<li><p><strong>Noise Points:</strong> A point is a noise point if it is neither a core point nor a border point. These points are considered outliers.</p>
<ul>
<li><p><strong>Definition:</strong> For a point <span class="math inline">\(p\)</span> in a dataset, <span class="math inline">\(p\)</span> is a noise point if it does not meet the criteria to be a core point or a border point.</p></li>
<li><p><strong>Example:</strong> If <span class="math inline">\(\epsilon = 0.5\)</span> and MinPts = 4, a point <span class="math inline">\(r\)</span> is a noise point if it does not have enough points within 0.5 units to be a core point and is not within 0.5 units of any core point.</p></li>
</ul></li>
</ul>
</section>
<section id="epsilon-and-minpts-parameters" class="level3">
<h3 class="anchored" data-anchor-id="epsilon-and-minpts-parameters">10.3.2. Epsilon and MinPts Parameters</h3>
<ul>
<li><p><strong>Epsilon (<span class="math inline">\(\epsilon\)</span>):</strong> The maximum distance between two points for one to be considered as in the neighborhood of the other. It defines the radius of the neighborhood around a point.</p>
<ul>
<li><p><strong>Choosing <span class="math inline">\(\epsilon\)</span>:</strong> The optimal value for <span class="math inline">\(\epsilon\)</span> depends on the dataset and can be determined using a k-distance graph. The “elbow” point in the graph can suggest a good value for <span class="math inline">\(\epsilon\)</span>.</p></li>
<li><p><strong>Example:</strong> If <span class="math inline">\(\epsilon\)</span> is set too small, many points will be classified as noise. If <span class="math inline">\(\epsilon\)</span> is set too large, clusters may merge and form one large cluster.</p></li>
</ul></li>
<li><p><strong>MinPts:</strong> The minimum number of points required to form a dense region (including the core point itself). It helps to define the density threshold for clusters.</p>
<ul>
<li><p><strong>Choosing MinPts:</strong> A common heuristic is to set MinPts to be at least the dimensionality of the dataset plus one (e.g., for 2D data, MinPts should be at least 3).</p></li>
<li><p><strong>Example:</strong> If MinPts is set too low, noise points may be included in clusters. If MinPts is set too high, many points may be classified as noise.</p></li>
</ul></li>
</ul>
</section>
<section id="optics-ordering-points-to-identify-the-clustering-structure" class="level3">
<h3 class="anchored" data-anchor-id="optics-ordering-points-to-identify-the-clustering-structure">10.3.3. OPTICS (Ordering Points To Identify the Clustering Structure)</h3>
<p>OPTICS is an extension of DBSCAN that addresses its sensitivity to the parameter <span class="math inline">\(\epsilon\)</span> and provides more insight into the clustering structure of the data.</p>
<ul>
<li><p><strong>Overview:</strong> OPTICS orders the points in the dataset to obtain a reachability plot, which represents the clustering structure. It does not explicitly produce a clustering but provides a visualization that can be used to extract clusters.</p></li>
<li><p><strong>Reachability Distance:</strong> The reachability distance of a point <span class="math inline">\(p\)</span> with respect to another point <span class="math inline">\(o\)</span> is the distance from <span class="math inline">\(p\)</span> to <span class="math inline">\(o\)</span> if <span class="math inline">\(p\)</span> is within <span class="math inline">\(\epsilon\)</span> distance of a core point, otherwise it is undefined.</p>
<ul>
<li><strong>Formula:</strong> <span class="math display">\[
\text{Reachability-Dist}(p, o) = \max(\text{Core-Dist}(o), \text{Dist}(p, o))
\]</span></li>
</ul></li>
<li><p><strong>Core Distance:</strong> The core distance of a point <span class="math inline">\(p\)</span> is the minimum radius <span class="math inline">\(\epsilon\)</span> such that there are at least MinPts points within this radius.</p>
<ul>
<li><strong>Formula:</strong> <span class="math display">\[
\text{Core-Dist}(p) = \text{MinPts-Nearest-Dist}(p)
\]</span></li>
</ul></li>
<li><p><strong>Steps:</strong></p>
<ol type="1">
<li><strong>Initialization:</strong> Calculate the core distance for each point in the dataset.</li>
<li><strong>Ordering:</strong> Order the points based on their reachability distance starting from an arbitrary point and expanding the cluster by processing its neighbors.</li>
<li><strong>Plotting:</strong> Generate a reachability plot where the y-axis represents the reachability distance and the x-axis represents the ordering of points.</li>
</ol></li>
<li><p><strong>Advantages:</strong> Handles varying densities and provides a more flexible and informative output than DBSCAN.</p></li>
<li><p><strong>Disadvantages:</strong> More computationally intensive than DBSCAN. Requires interpretation of the reachability plot to extract clusters.</p></li>
</ul>
<p>By understanding and applying DBSCAN and its parameters, as well as leveraging OPTICS for more complex datasets, you can effectively identify clusters of varying shapes and densities while handling noise and outliers in your data.</p>
</section>
</section>
<section id="gaussian-mixture-models" class="level2">
<h2 class="anchored" data-anchor-id="gaussian-mixture-models">10.4. Gaussian Mixture Models</h2>
<p>Gaussian Mixture Models (GMMs) are probabilistic models that assume all the data points are generated from a mixture of several Gaussian distributions with unknown parameters. GMMs are more flexible than K-means as they can model elliptical clusters and handle clusters of different sizes and covariance structures.</p>
<section id="em-algorithm-for-gmm" class="level3">
<h3 class="anchored" data-anchor-id="em-algorithm-for-gmm">10.4.1. EM Algorithm for GMM</h3>
<p>The Expectation-Maximization (EM) algorithm is used to find the parameters of the Gaussian Mixture Model. The algorithm iteratively improves the parameter estimates to maximize the likelihood of the observed data.</p>
<ul>
<li><p><strong>Steps:</strong></p>
<ol type="1">
<li><strong>Initialization:</strong>
<ul>
<li>Initialize the parameters: means (<span class="math inline">\(\mu\)</span>), covariances (<span class="math inline">\(\Sigma\)</span>), and mixing coefficients (<span class="math inline">\(\pi\)</span>) for each Gaussian component.</li>
<li>Typically, K-means clustering is used for initialization.</li>
</ul></li>
<li><strong>Expectation Step (E-step):</strong>
<ul>
<li>Calculate the responsibility that each Gaussian component takes for each data point.</li>
<li>For each data point <span class="math inline">\(x_i\)</span> and each component <span class="math inline">\(k\)</span>, compute the responsibility <span class="math inline">\(\gamma_{ik}\)</span> as: <span class="math display">\[
\gamma_{ik} = \frac{\pi_k \mathcal{N}(x_i | \mu_k, \Sigma_k)}{\sum_{j=1}^{K} \pi_j \mathcal{N}(x_i | \mu_j, \Sigma_j)}
\]</span></li>
<li>Here, <span class="math inline">\(\mathcal{N}(x_i | \mu_k, \Sigma_k)\)</span> is the Gaussian probability density function.</li>
</ul></li>
<li><strong>Maximization Step (M-step):</strong>
<ul>
<li>Update the parameters using the responsibilities calculated in the E-step.</li>
<li>Update the means: <span class="math display">\[
\mu_k = \frac{\sum_{i=1}^{N} \gamma_{ik} x_i}{\sum_{i=1}^{N} \gamma_{ik}}
\]</span></li>
<li>Update the covariances: <span class="math display">\[
\Sigma_k = \frac{\sum_{i=1}^{N} \gamma_{ik} (x_i - \mu_k)(x_i - \mu_k)^T}{\sum_{i=1}^{N} \gamma_{ik}}
\]</span></li>
<li>Update the mixing coefficients: <span class="math display">\[
\pi_k = \frac{1}{N} \sum_{i=1}^{N} \gamma_{ik}
\]</span></li>
</ul></li>
<li><strong>Convergence Check:</strong>
<ul>
<li>Check for convergence by monitoring the change in the log-likelihood or the parameter values.</li>
<li>If convergence criteria are met, stop; otherwise, repeat the E-step and M-step.</li>
</ul></li>
</ol></li>
<li><p><strong>Advantages:</strong></p>
<ul>
<li>Can model clusters of different shapes and sizes.</li>
<li>Provides a probabilistic assignment of data points to clusters.</li>
</ul></li>
<li><p><strong>Disadvantages:</strong></p>
<ul>
<li>Can converge to local optima; sensitive to initialization.</li>
<li>Computationally expensive for large datasets.</li>
</ul></li>
</ul>
</section>
<section id="choosing-the-number-of-components" class="level3">
<h3 class="anchored" data-anchor-id="choosing-the-number-of-components">10.4.2. Choosing the Number of Components</h3>
<p>Choosing the number of components (<span class="math inline">\(K\)</span>) in a GMM is crucial for effective modeling. Several methods can be used to determine the optimal <span class="math inline">\(K\)</span>.</p>
<ul>
<li><strong>Cross-Validation:</strong>
<ul>
<li>Split the data into training and validation sets.</li>
<li>Fit GMMs with different numbers of components on the training set and evaluate their performance on the validation set.</li>
<li>Choose the number of components that minimizes the validation error.</li>
</ul></li>
<li><strong>Elbow Method:</strong>
<ul>
<li>Plot the log-likelihood of the model against the number of components.</li>
<li>Look for an “elbow point” where the increase in log-likelihood slows down.</li>
</ul></li>
<li><strong>Information Criteria:</strong>
<ul>
<li>Use criteria like Bayesian Information Criterion (BIC) or Akaike Information Criterion (AIC) to compare models with different numbers of components.</li>
</ul></li>
</ul>
</section>
<section id="bayesian-information-criterion-bic" class="level3">
<h3 class="anchored" data-anchor-id="bayesian-information-criterion-bic">10.4.3. Bayesian Information Criterion (BIC)</h3>
<p>The Bayesian Information Criterion (BIC) is a model selection criterion that balances the goodness of fit with the complexity of the model. It is widely used to select the number of components in a GMM.</p>
<ul>
<li><strong>Formula:</strong> <span class="math display">\[
\text{BIC} = -2 \log(L) + p \log(N)
\]</span> where:
<ul>
<li><span class="math inline">\(L\)</span> is the likelihood of the model.</li>
<li><span class="math inline">\(p\)</span> is the number of parameters in the model.</li>
<li><span class="math inline">\(N\)</span> is the number of data points.</li>
</ul></li>
<li><strong>Steps:</strong>
<ol type="1">
<li>Fit GMMs with different numbers of components to the data.</li>
<li>Calculate the BIC for each model.</li>
<li>Choose the model with the lowest BIC.</li>
</ol></li>
<li><strong>Advantages:</strong>
<ul>
<li>Penalizes complex models to avoid overfitting.</li>
<li>Easy to compute and interpret.</li>
</ul></li>
<li><strong>Disadvantages:</strong>
<ul>
<li>Assumes that the true model is among the candidates.</li>
<li>May be sensitive to the choice of priors.</li>
</ul></li>
</ul>
</section>
<section id="variational-bayesian-gmm" class="level3">
<h3 class="anchored" data-anchor-id="variational-bayesian-gmm">10.4.4. Variational Bayesian GMM</h3>
<p>Variational Bayesian GMM (VB-GMM) extends GMM by using Bayesian inference to estimate the distribution of the parameters. It provides a probabilistic framework to determine the number of components and avoids overfitting by incorporating priors.</p>
<ul>
<li><strong>Overview:</strong>
<ul>
<li>Uses variational inference to approximate the posterior distribution of the parameters.</li>
<li>Introduces Dirichlet priors on the mixing coefficients and Normal-Wishart priors on the component parameters (means and covariances).</li>
</ul></li>
<li><strong>Steps:</strong>
<ol type="1">
<li><strong>Initialization:</strong> Set initial values for the variational parameters.</li>
<li><strong>E-step:</strong> Compute the expected values of the latent variables given the current parameter estimates.</li>
<li><strong>M-step:</strong> Maximize the expected complete log-likelihood with respect to the variational parameters.</li>
<li><strong>Update Priors:</strong> Update the priors based on the posterior estimates.</li>
<li><strong>Convergence Check:</strong> Monitor the change in the evidence lower bound (ELBO) to check for convergence.</li>
</ol></li>
<li><strong>Advantages:</strong>
<ul>
<li>Provides a fully probabilistic model, accounting for uncertainty in parameter estimates.</li>
<li>Can infer the number of components as part of the model fitting process.</li>
</ul></li>
<li><strong>Disadvantages:</strong>
<ul>
<li>Computationally more intensive than standard GMM.</li>
<li>Requires careful tuning of the priors and variational parameters.</li>
</ul></li>
</ul>
<p>By understanding and applying these techniques, you can effectively use Gaussian Mixture Models to analyze complex datasets, determining the optimal number of components and leveraging advanced methods like the EM algorithm and variational inference for robust clustering results.</p>
</section>
</section>
<section id="gaussian-mixture-models-1" class="level2">
<h2 class="anchored" data-anchor-id="gaussian-mixture-models-1">10.4. Gaussian Mixture Models</h2>
<p>Gaussian Mixture Models (GMMs) are probabilistic models that assume all the data points are generated from a mixture of several Gaussian distributions with unknown parameters. GMMs are more flexible than K-means as they can model elliptical clusters and handle clusters of different sizes and covariance structures.</p>
<section id="em-algorithm-for-gmm-1" class="level3">
<h3 class="anchored" data-anchor-id="em-algorithm-for-gmm-1">10.4.1. EM Algorithm for GMM</h3>
<p>The Expectation-Maximization (EM) algorithm is used to find the parameters of the Gaussian Mixture Model. The algorithm iteratively improves the parameter estimates to maximize the likelihood of the observed data.</p>
<ul>
<li><p><strong>Steps:</strong></p>
<ol type="1">
<li><strong>Initialization:</strong>
<ul>
<li>Initialize the parameters: means (<span class="math inline">\(\mu\)</span>), covariances (<span class="math inline">\(\Sigma\)</span>), and mixing coefficients (<span class="math inline">\(\pi\)</span>) for each Gaussian component.</li>
<li>Typically, K-means clustering is used for initialization to provide a good starting point.</li>
</ul></li>
<li><strong>Expectation Step (E-step):</strong>
<ul>
<li>Calculate the responsibility that each Gaussian component takes for each data point.</li>
<li>For each data point <span class="math inline">\(x_i\)</span> and each component <span class="math inline">\(k\)</span>, compute the responsibility <span class="math inline">\(\gamma_{ik}\)</span> as: <span class="math display">\[
\gamma_{ik} = \frac{\pi_k \mathcal{N}(x_i | \mu_k, \Sigma_k)}{\sum_{j=1}^{K} \pi_j \mathcal{N}(x_i | \mu_j, \Sigma_j)}
\]</span></li>
<li>Here, <span class="math inline">\(\mathcal{N}(x_i | \mu_k, \Sigma_k)\)</span> is the Gaussian probability density function, given by: <span class="math display">\[
\mathcal{N}(x_i | \mu_k, \Sigma_k) = \frac{1}{(2\pi)^{d/2} |\Sigma_k|^{1/2}} \exp \left( -\frac{1}{2} (x_i - \mu_k)^T \Sigma_k^{-1} (x_i - \mu_k) \right)
\]</span></li>
<li><span class="math inline">\(\gamma_{ik}\)</span> represents the probability that data point <span class="math inline">\(x_i\)</span> was generated by Gaussian component <span class="math inline">\(k\)</span>.</li>
</ul></li>
<li><strong>Maximization Step (M-step):</strong>
<ul>
<li>Update the parameters using the responsibilities calculated in the E-step.</li>
<li>Update the means: <span class="math display">\[
\mu_k = \frac{\sum_{i=1}^{N} \gamma_{ik} x_i}{\sum_{i=1}^{N} \gamma_{ik}}
\]</span></li>
<li>Update the covariances: <span class="math display">\[
\Sigma_k = \frac{\sum_{i=1}^{N} \gamma_{ik} (x_i - \mu_k)(x_i - \mu_k)^T}{\sum_{i=1}^{N} \gamma_{ik}}
\]</span></li>
<li>Update the mixing coefficients: <span class="math display">\[
\pi_k = \frac{1}{N} \sum_{i=1}^{N} \gamma_{ik}
\]</span></li>
</ul></li>
<li><strong>Convergence Check:</strong>
<ul>
<li>Check for convergence by monitoring the change in the log-likelihood or the parameter values.</li>
<li>The log-likelihood is given by: <span class="math display">\[
\log L = \sum_{i=1}^{N} \log \left( \sum_{k=1}^{K} \pi_k \mathcal{N}(x_i | \mu_k, \Sigma_k) \right)
\]</span></li>
<li>If convergence criteria are met (e.g., the change in log-likelihood is below a threshold), stop; otherwise, repeat the E-step and M-step.</li>
</ul></li>
</ol></li>
<li><p><strong>Advantages:</strong></p>
<ul>
<li>Can model clusters of different shapes and sizes.</li>
<li>Provides a probabilistic assignment of data points to clusters, which can be useful for understanding cluster membership uncertainty.</li>
</ul></li>
<li><p><strong>Disadvantages:</strong></p>
<ul>
<li>Can converge to local optima; the result depends on the initialization.</li>
<li>Computationally expensive for large datasets due to the iterative nature of the algorithm and the complexity of updating covariance matrices.</li>
</ul></li>
</ul>
</section>
<section id="choosing-the-number-of-components-1" class="level3">
<h3 class="anchored" data-anchor-id="choosing-the-number-of-components-1">10.4.2. Choosing the Number of Components</h3>
<p>Choosing the number of components (<span class="math inline">\(K\)</span>) in a GMM is crucial for effective modeling. Several methods can be used to determine the optimal <span class="math inline">\(K\)</span>.</p>
<ul>
<li><strong>Cross-Validation:</strong>
<ul>
<li>Split the data into training and validation sets.</li>
<li>Fit GMMs with different numbers of components on the training set and evaluate their performance on the validation set.</li>
<li>Choose the number of components that minimizes the validation error.</li>
<li>This method ensures that the chosen model generalizes well to unseen data.</li>
</ul></li>
<li><strong>Elbow Method:</strong>
<ul>
<li>Plot the log-likelihood of the model against the number of components.</li>
<li>Look for an “elbow point” where the increase in log-likelihood slows down, indicating diminishing returns for adding more components.</li>
<li>This visual method helps identify a point where adding more components does not significantly improve the model fit.</li>
</ul></li>
<li><strong>Information Criteria:</strong>
<ul>
<li>Use criteria like Bayesian Information Criterion (BIC) or Akaike Information Criterion (AIC) to compare models with different numbers of components.</li>
<li>BIC and AIC introduce penalties for model complexity, helping to avoid overfitting by selecting models that balance fit and complexity.</li>
</ul></li>
</ul>
</section>
<section id="bayesian-information-criterion-bic-1" class="level3">
<h3 class="anchored" data-anchor-id="bayesian-information-criterion-bic-1">10.4.3. Bayesian Information Criterion (BIC)</h3>
<p>The Bayesian Information Criterion (BIC) is a model selection criterion that balances the goodness of fit with the complexity of the model. It is widely used to select the number of components in a GMM.</p>
<ul>
<li><strong>Formula:</strong> <span class="math display">\[
\text{BIC} = -2 \log(L) + p \log(N)
\]</span> where:
<ul>
<li><span class="math inline">\(L\)</span> is the likelihood of the model.</li>
<li><span class="math inline">\(p\)</span> is the number of parameters in the model.</li>
<li><span class="math inline">\(N\)</span> is the number of data points.</li>
</ul></li>
<li><strong>Steps:</strong>
<ol type="1">
<li>Fit GMMs with different numbers of components to the data.</li>
<li>Calculate the BIC for each model.</li>
<li>Choose the model with the lowest BIC.</li>
</ol></li>
<li><strong>Advantages:</strong>
<ul>
<li>Penalizes complex models to avoid overfitting.</li>
<li>Easy to compute and interpret.</li>
<li>Provides a quantitative criterion for model selection, making the process more objective.</li>
</ul></li>
<li><strong>Disadvantages:</strong>
<ul>
<li>Assumes that the true model is among the candidates.</li>
<li>May be sensitive to the choice of priors and the scale of the data.</li>
</ul></li>
</ul>
</section>
<section id="variational-bayesian-gmm-1" class="level3">
<h3 class="anchored" data-anchor-id="variational-bayesian-gmm-1">10.4.4. Variational Bayesian GMM</h3>
<p>Variational Bayesian GMM (VB-GMM) extends GMM by using Bayesian inference to estimate the distribution of the parameters. It provides a probabilistic framework to determine the number of components and avoids overfitting by incorporating priors.</p>
<ul>
<li><strong>Overview:</strong>
<ul>
<li>Uses variational inference to approximate the posterior distribution of the parameters.</li>
<li>Introduces Dirichlet priors on the mixing coefficients and Normal-Wishart priors on the component parameters (means and covariances).</li>
<li>VB-GMM can automatically determine the number of components by inferring the posterior distribution over the number of components.</li>
</ul></li>
<li><strong>Steps:</strong>
<ol type="1">
<li><strong>Initialization:</strong> Set initial values for the variational parameters.</li>
<li><strong>E-step:</strong> Compute the expected values of the latent variables given the current parameter estimates.
<ul>
<li>Compute the responsibilities using variational parameters.</li>
</ul></li>
<li><strong>M-step:</strong> Maximize the expected complete log-likelihood with respect to the variational parameters.
<ul>
<li>Update the parameters using the expectations computed in the E-step.</li>
</ul></li>
<li><strong>Update Priors:</strong> Update the priors based on the posterior estimates.</li>
<li><strong>Convergence Check:</strong> Monitor the change in the evidence lower bound (ELBO) to check for convergence.
<ul>
<li>The ELBO is given by: <span class="math display">\[
\text{ELBO} = \mathbb{E}_{q} [\log p(X, Z | \Theta)] - \mathbb{E}_{q} [\log q(Z)]
\]</span></li>
<li>If the change in ELBO is below a threshold, stop; otherwise, repeat the E-step and M-step.</li>
</ul></li>
</ol></li>
<li><strong>Advantages:</strong>
<ul>
<li>Provides a fully probabilistic model, accounting for uncertainty in parameter estimates.</li>
<li>Can infer the number of components as part of the model fitting process, avoiding the need for manual selection.</li>
<li>More robust to overfitting due to the incorporation of priors and the Bayesian framework.</li>
</ul></li>
<li><strong>Disadvantages:</strong>
<ul>
<li>Computationally more intensive than standard GMM.</li>
<li>Requires careful tuning of the priors and variational parameters to ensure accurate and stable inference.</li>
<li>The results can be sensitive to the choice of priors, and interpreting the results requires a good understanding of Bayesian inference.</li>
</ul></li>
</ul>
<p>By understanding and applying these techniques, you can effectively use Gaussian Mixture Models to analyze complex datasets, determining the optimal number of components and leveraging advanced methods like the EM algorithm and variational inference for robust clustering results.</p>
</section>
</section>
<section id="principal-component-analysis-pca" class="level2">
<h2 class="anchored" data-anchor-id="principal-component-analysis-pca">10.5. Principal Component Analysis (PCA)</h2>
<p>Principal Component Analysis (PCA) is a dimensionality reduction technique that transforms the data into a new coordinate system where the greatest variance by any projection of the data comes to lie on the first coordinate (called the first principal component), the second greatest variance on the second coordinate, and so on. PCA helps in reducing the dimensionality of the data while retaining most of the variance, making it easier to visualize and process.</p>
<section id="eigenvalues-and-eigenvectors" class="level3">
<h3 class="anchored" data-anchor-id="eigenvalues-and-eigenvectors">10.5.1. Eigenvalues and Eigenvectors</h3>
<p>Eigenvalues and eigenvectors are fundamental in understanding the PCA transformation. They are derived from the covariance matrix of the data.</p>
<ul>
<li><p><strong>Eigenvalues (<span class="math inline">\(\lambda\)</span>):</strong> Scalars that indicate the magnitude of the variance in the direction of its corresponding eigenvector. The eigenvalue associated with an eigenvector indicates the amount of variance captured by that eigenvector.</p></li>
<li><p><strong>Eigenvectors (<span class="math inline">\(v\)</span>):</strong> Directions in which the data varies the most. Each eigenvector is associated with an eigenvalue and represents a principal component.</p></li>
<li><p><strong>Mathematical Definition:</strong></p>
<ul>
<li>Given a covariance matrix <span class="math inline">\(\Sigma\)</span>, an eigenvector <span class="math inline">\(v\)</span> and its corresponding eigenvalue <span class="math inline">\(\lambda\)</span> satisfy the equation: <span class="math display">\[
\Sigma v = \lambda v
\]</span></li>
<li>The eigenvalues and eigenvectors are obtained by solving this equation.</li>
</ul></li>
<li><p><strong>Steps to Compute Eigenvalues and Eigenvectors:</strong></p>
<ol type="1">
<li><strong>Standardize the data</strong>: Subtract the mean of each feature from the dataset.</li>
<li><strong>Compute the covariance matrix</strong> of the standardized data.</li>
<li><strong>Solve the characteristic equation</strong> to obtain eigenvalues and eigenvectors of the covariance matrix.</li>
<li><strong>Sort the eigenvalues and corresponding eigenvectors</strong> in descending order of eigenvalues.</li>
</ol></li>
</ul>
</section>
<section id="explained-variance-ratio" class="level3">
<h3 class="anchored" data-anchor-id="explained-variance-ratio">10.5.2. Explained Variance Ratio</h3>
<p>The explained variance ratio measures the proportion of the dataset’s variance that is captured by each principal component. It helps in understanding how much information (variance) is retained by the principal components.</p>
<ul>
<li><strong>Mathematical Definition:</strong>
<ul>
<li>The explained variance ratio for the <span class="math inline">\(i\)</span>-th principal component is given by: <span class="math display">\[
\text{Explained Variance Ratio}_i = \frac{\lambda_i}{\sum_{j=1}^{d} \lambda_j}
\]</span></li>
<li>Here, <span class="math inline">\(\lambda_i\)</span> is the eigenvalue of the <span class="math inline">\(i\)</span>-th principal component and <span class="math inline">\(d\)</span> is the total number of components.</li>
</ul></li>
<li><strong>Steps to Compute Explained Variance Ratio:</strong>
<ol type="1">
<li><strong>Compute the eigenvalues</strong> of the covariance matrix.</li>
<li><strong>Sort the eigenvalues</strong> in descending order.</li>
<li><strong>Calculate the proportion of variance</strong> each eigenvalue contributes to the total variance.</li>
</ol></li>
<li><strong>Interpreting Explained Variance Ratio:</strong>
<ul>
<li>A higher explained variance ratio indicates that the principal component captures a larger portion of the dataset’s variance.</li>
<li>By summing the explained variance ratios, you can determine how many principal components are needed to retain a desired amount of variance.</li>
</ul></li>
</ul>
</section>
<section id="dimensionality-reduction-with-pca" class="level3">
<h3 class="anchored" data-anchor-id="dimensionality-reduction-with-pca">10.5.3. Dimensionality Reduction with PCA</h3>
<p>PCA reduces the dimensionality of the data by projecting it onto the principal components that capture the most variance. This results in a lower-dimensional representation of the data that retains most of the original variability.</p>
<ul>
<li><strong>Steps for Dimensionality Reduction:</strong>
<ol type="1">
<li><strong>Standardize the data</strong>: Subtract the mean of each feature and divide by the standard deviation.</li>
<li><strong>Compute the covariance matrix</strong> of the standardized data.</li>
<li><strong>Obtain the eigenvalues and eigenvectors</strong> of the covariance matrix.</li>
<li><strong>Sort the eigenvalues and corresponding eigenvectors</strong> in descending order.</li>
<li><strong>Select the top <span class="math inline">\(k\)</span> eigenvectors</strong> to form a <span class="math inline">\(k\)</span>-dimensional subspace.</li>
<li><strong>Project the original data</strong> onto the <span class="math inline">\(k\)</span>-dimensional subspace to obtain the reduced-dimension data.</li>
</ol></li>
<li><strong>Advantages:</strong>
<ul>
<li>Reduces computational cost and memory usage by reducing the number of dimensions.</li>
<li>Helps in visualizing high-dimensional data in 2D or 3D.</li>
<li>Can improve the performance of machine learning algorithms by removing noise and redundant features.</li>
</ul></li>
<li><strong>Disadvantages:</strong>
<ul>
<li>May lose some information, especially if the number of components retained is too small.</li>
<li>Assumes that the principal components with the highest variance are the most important, which may not always be the case.</li>
</ul></li>
</ul>
</section>
<section id="kernel-pca" class="level3">
<h3 class="anchored" data-anchor-id="kernel-pca">10.5.4. Kernel PCA</h3>
<p>Kernel PCA is an extension of PCA that allows for nonlinear dimensionality reduction by using kernel functions. It maps the original data into a higher-dimensional space where it is more likely to be linearly separable and then performs PCA in this new space.</p>
<ul>
<li><strong>Mathematical Formulation:</strong>
<ul>
<li>Kernel PCA uses a kernel function <span class="math inline">\(k(x, y)\)</span> to compute the dot product in the higher-dimensional space without explicitly performing the transformation.</li>
<li>Common kernel functions include the polynomial kernel: <span class="math display">\[
k(x, y) = (x^T y + c)^d
\]</span> and the Gaussian (RBF) kernel: <span class="math display">\[
k(x, y) = \exp \left( -\frac{\| x - y \|^2}{2\sigma^2} \right)
\]</span></li>
</ul></li>
<li><strong>Steps for Kernel PCA:</strong>
<ol type="1">
<li><strong>Choose a kernel function</strong> and compute the kernel matrix <span class="math inline">\(K\)</span> for the data.</li>
<li><strong>Center the kernel matrix</strong> <span class="math inline">\(K\)</span> to ensure it has zero mean.</li>
<li><strong>Compute the eigenvalues and eigenvectors</strong> of the centered kernel matrix.</li>
<li><strong>Select the top <span class="math inline">\(k\)</span> eigenvectors</strong> to form the <span class="math inline">\(k\)</span>-dimensional subspace.</li>
<li><strong>Project the original data</strong> onto the <span class="math inline">\(k\)</span>-dimensional subspace using the selected eigenvectors.</li>
</ol></li>
<li><strong>Advantages:</strong>
<ul>
<li>Captures nonlinear relationships in the data.</li>
<li>Can separate data that is not linearly separable in the original space.</li>
</ul></li>
<li><strong>Disadvantages:</strong>
<ul>
<li>Choosing the right kernel and tuning its parameters can be challenging.</li>
<li>Computationally expensive for large datasets.</li>
</ul></li>
</ul>
</section>
<section id="incremental-pca-for-large-datasets" class="level3">
<h3 class="anchored" data-anchor-id="incremental-pca-for-large-datasets">10.5.5. Incremental PCA for Large Datasets</h3>
<p>Incremental PCA (IPCA) is a variant of PCA that processes data in mini-batches, making it suitable for large datasets that do not fit into memory. It allows for online learning where the model is updated incrementally as new data arrives.</p>
<ul>
<li><strong>Steps for Incremental PCA:</strong>
<ol type="1">
<li><strong>Initialize the mean and covariance estimates</strong> with the first mini-batch of data.</li>
<li><strong>For each subsequent mini-batch:</strong>
<ul>
<li>Update the mean and covariance estimates.</li>
<li>Compute the eigenvalues and eigenvectors of the updated covariance matrix.</li>
</ul></li>
<li><strong>Accumulate the results</strong> to obtain the final principal components.</li>
</ol></li>
<li><strong>Advantages:</strong>
<ul>
<li>Can handle large datasets that do not fit into memory.</li>
<li>Supports online learning, allowing the model to be updated with new data.</li>
</ul></li>
<li><strong>Disadvantages:</strong>
<ul>
<li>May be less accurate than standard PCA due to the approximation introduced by mini-batches.</li>
<li>Requires careful tuning of the mini-batch size.</li>
</ul></li>
</ul>
<p>By understanding and applying these PCA techniques, you can effectively reduce the dimensionality of complex datasets, capture important patterns and structures, and improve the performance of subsequent machine learning tasks.</p>
</section>
</section>
<section id="t-sne-and-umap" class="level2">
<h2 class="anchored" data-anchor-id="t-sne-and-umap">10.6. t-SNE and UMAP</h2>
<p>t-SNE (t-Distributed Stochastic Neighbor Embedding) and UMAP (Uniform Manifold Approximation and Projection) are dimensionality reduction techniques particularly well-suited for visualizing high-dimensional data in 2D or 3D. They capture the local and global structure of the data and are widely used for exploratory data analysis.</p>
<section id="t-sne-algorithm" class="level3">
<h3 class="anchored" data-anchor-id="t-sne-algorithm">10.6.1. t-SNE Algorithm</h3>
<p>t-SNE is a nonlinear dimensionality reduction technique that maps high-dimensional data to a lower-dimensional space in a way that preserves the structure of the data. It is particularly effective at visualizing clusters.</p>
<ul>
<li><strong>Overview of t-SNE:</strong>
<ul>
<li>t-SNE converts the high-dimensional Euclidean distances between data points into conditional probabilities that represent similarities.</li>
<li>It minimizes the Kullback-Leibler divergence between the probability distributions of the high-dimensional data and the low-dimensional map.</li>
</ul></li>
<li><strong>Algorithm Steps:</strong>
<ol type="1">
<li><p><strong>Compute Pairwise Affinities:</strong> Calculate pairwise affinities (similarities) in the high-dimensional space using a Gaussian kernel. The affinity between points <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span> is: <span class="math display">\[
p_{j|i} = \frac{\exp(-\|x_i - x_j\|^2 / 2\sigma_i^2)}{\sum_{k \neq i} \exp(-\|x_i - x_k\|^2 / 2\sigma_i^2)}
\]</span> where <span class="math inline">\(\sigma_i\)</span> is the bandwidth of the Gaussian kernel centered at <span class="math inline">\(x_i\)</span>.</p></li>
<li><p><strong>Symmetrize Affinities:</strong> Symmetrize the affinities by defining: <span class="math display">\[
p_{ij} = \frac{p_{j|i} + p_{i|j}}{2N}
\]</span> where <span class="math inline">\(N\)</span> is the number of data points.</p></li>
<li><p><strong>Initialize Low-Dimensional Map:</strong> Initialize the low-dimensional map randomly or using PCA.</p></li>
<li><p><strong>Compute Low-Dimensional Affinities:</strong> Calculate affinities in the low-dimensional space using a Student’s t-distribution: <span class="math display">\[
q_{ij} = \frac{(1 + \|y_i - y_j\|^2)^{-1}}{\sum_{k \neq l} (1 + \|y_k - y_l\|^2)^{-1}}
\]</span></p></li>
<li><p><strong>Minimize KL Divergence:</strong> Minimize the Kullback-Leibler divergence between the high-dimensional and low-dimensional affinities: <span class="math display">\[
KL(P||Q) = \sum_{i \neq j} p_{ij} \log \frac{p_{ij}}{q_{ij}}
\]</span> using gradient descent.</p></li>
</ol></li>
</ul>
<section id="perplexity-parameter" class="level4">
<h4 class="anchored" data-anchor-id="perplexity-parameter">10.6.1.1. Perplexity Parameter</h4>
<ul>
<li><p><strong>Perplexity:</strong> Perplexity is a hyperparameter in t-SNE that controls the balance between local and global aspects of the data. It can be interpreted as a smooth measure of the effective number of neighbors for each data point.</p>
<ul>
<li><p><strong>Mathematical Definition:</strong> <span class="math display">\[
\text{Perplexity}(P) = 2^{H(P)}
\]</span> where <span class="math inline">\(H(P)\)</span> is the Shannon entropy of the probability distribution <span class="math inline">\(P\)</span>: <span class="math display">\[
H(P) = -\sum_{i} P(i) \log_2 P(i)
\]</span></p></li>
<li><p><strong>Choosing Perplexity:</strong></p>
<ul>
<li>Typical values range from 5 to 50.</li>
<li>Lower perplexity values focus more on local structure, while higher values capture more global structure.</li>
<li>The optimal perplexity depends on the dataset and often requires experimentation.</li>
</ul></li>
<li><p><strong>Impact on t-SNE:</strong></p>
<ul>
<li>Low perplexity values (e.g., 5-10) will result in capturing very local structures, making small clusters more apparent.</li>
<li>High perplexity values (e.g., 40-50) will result in capturing more global structures, showing larger clusters but potentially losing fine details.</li>
</ul></li>
</ul></li>
</ul>
</section>
<section id="early-exaggeration" class="level4">
<h4 class="anchored" data-anchor-id="early-exaggeration">10.6.1.2. Early Exaggeration</h4>
<ul>
<li><p><strong>Early Exaggeration:</strong> Early exaggeration is a phase in the t-SNE optimization process where the attractive forces between points are temporarily increased to better separate clusters before fine-tuning the layout.</p>
<ul>
<li><strong>Purpose:</strong>
<ul>
<li>Helps to form well-defined clusters in the early stages.</li>
<li>Prevents points from collapsing together too quickly, allowing the algorithm to explore the structure of the data more thoroughly.</li>
</ul></li>
<li><strong>Implementation:</strong>
<ul>
<li>The early exaggeration factor typically ranges from 4 to 12.</li>
<li>During this phase, the pairwise similarities in the high-dimensional space are multiplied by the early exaggeration factor.</li>
</ul></li>
<li><strong>Effect:</strong>
<ul>
<li>Encourages points that are relatively close in the high-dimensional space to move further apart in the low-dimensional space.</li>
<li>After a set number of iterations, the exaggeration factor is removed, and the optimization continues normally.</li>
</ul></li>
<li><strong>Mathematical Impact:</strong>
<ul>
<li>During early exaggeration, the affinities <span class="math inline">\(p_{ij}\)</span> are scaled: <span class="math display">\[
p_{ij}^{\text{exaggerated}} = \alpha p_{ij}
\]</span></li>
<li>This scaling increases the gradient magnitude for attractive forces, enhancing the separation of clusters early in the optimization process.</li>
</ul></li>
</ul></li>
</ul>
</section>
</section>
<section id="umap-algorithm" class="level3">
<h3 class="anchored" data-anchor-id="umap-algorithm">10.6.2. UMAP Algorithm</h3>
<p>UMAP is a more recent dimensionality reduction technique that aims to preserve both the local and global structure of the data more effectively than t-SNE. UMAP is based on manifold learning and topological data analysis.</p>
<ul>
<li><strong>Overview of UMAP:</strong>
<ul>
<li>UMAP constructs a high-dimensional graph representing the data and then optimizes a low-dimensional graph to be as structurally similar as possible to the high-dimensional one.</li>
<li>It uses a combination of local neighbor embeddings and global distances to create a faithful representation of the original data.</li>
</ul></li>
<li><strong>Algorithm Steps:</strong>
<ol type="1">
<li><strong>Construct High-Dimensional Graph:</strong> Create a weighted graph where each node represents a data point, and edges represent the connectivity between points based on local distances.</li>
<li><strong>Optimize Low-Dimensional Embedding:</strong> Minimize the cross-entropy between the high-dimensional and low-dimensional representations to preserve the structure.</li>
</ol></li>
</ul>
<section id="topological-foundations" class="level4">
<h4 class="anchored" data-anchor-id="topological-foundations">10.6.2.1. Topological Foundations</h4>
<ul>
<li><p><strong>Topological Foundations:</strong> UMAP leverages concepts from topology and manifold theory to model the underlying structure of the data.</p>
<ul>
<li><strong>Manifold Hypothesis:</strong>
<ul>
<li>Assumes that high-dimensional data lie on a low-dimensional manifold embedded within the higher-dimensional space.</li>
<li>UMAP aims to uncover this manifold structure and project it into a lower-dimensional space.</li>
</ul></li>
<li><strong>Simplicial Complexes:</strong>
<ul>
<li>UMAP constructs a weighted graph (simplicial complex) that represents the local neighborhood relationships in the data.</li>
<li>Each data point is connected to its nearest neighbors, and edges are weighted based on the distance between points.</li>
</ul></li>
<li><strong>Optimization Objective:</strong>
<ul>
<li>UMAP minimizes the cross-entropy between the high-dimensional and low-dimensional representations.</li>
<li>Preserves both local and global data structures by balancing the attraction and repulsion forces in the optimization process.</li>
</ul></li>
<li><strong>Mathematical Formulation:</strong>
<ul>
<li>UMAP constructs a fuzzy topological representation of the high-dimensional data using a fuzzy simplicial set: <span class="math display">\[
\text{fuzzy\_simplicial\_set}(X, n\_neighbors, min\_dist)
\]</span></li>
<li>The optimization process involves minimizing the cross-entropy between the high-dimensional and low-dimensional graphs: <span class="math display">\[
\text{Cross-Entropy}(P, Q) = -\sum_{i \neq j} [ P_{ij} \log Q_{ij} + (1 - P_{ij}) \log (1 - Q_{ij}) ]
\]</span></li>
<li>Here, <span class="math inline">\(P_{ij}\)</span> and <span class="math inline">\(Q_{ij}\)</span> represent the probabilities of point <span class="math inline">\(i\)</span> being connected to point <span class="math inline">\(j\)</span> in the high-dimensional and low-dimensional spaces, respectively.</li>
</ul></li>
</ul></li>
</ul>
</section>
<section id="comparison-with-t-sne" class="level4">
<h4 class="anchored" data-anchor-id="comparison-with-t-sne">10.6.2.2. Comparison with t-SNE</h4>
<ul>
<li><strong>Comparison with t-SNE:</strong>
<ul>
<li><strong>Preservation of Global Structure:</strong>
<ul>
<li>t-SNE is effective at preserving local structures but can struggle with global structures, making it less suitable for capturing large-scale patterns in the data.</li>
<li>UMAP, on the other hand, is designed to preserve both local and global structures, providing a more accurate representation of the data’s overall shape.</li>
</ul></li>
<li><strong>Computational Efficiency:</strong>
<ul>
<li>UMAP is generally faster and more scalable than t-SNE, especially for large datasets.</li>
<li>UMAP’s runtime complexity is linear in the number of data points, while t-SNE’s complexity is quadratic, making UMAP more suitable for big data applications.</li>
</ul></li>
<li><strong>Parameter Sensitivity:</strong>
<ul>
<li>t-SNE requires careful tuning of the perplexity parameter, and its results can be sensitive to this choice.</li>
<li>UMAP has fewer hyperparameters and is generally more robust to their settings. The primary parameters in UMAP are the number of neighbors and the minimum distance, which control the balance between local and global structure preservation.</li>
</ul></li>
<li><strong>Stability and Reproducibility:</strong>
<ul>
<li>t-SNE can produce different results on different runs due to its stochastic nature, requiring multiple runs to ensure stable results.</li>
<li>UMAP tends to produce more consistent and reproducible results across different runs.</li>
</ul></li>
</ul></li>
<li><strong>UMAP Parameters:</strong>
<ul>
<li><strong>n_neighbors:</strong> Controls the local neighborhood size. Smaller values focus on local structure, while larger values capture more global structure.</li>
<li><strong>min_dist:</strong> Controls the minimum distance between points in the low-dimensional space. Smaller values preserve more local detail, while larger values result in a more compressed embedding.</li>
</ul></li>
</ul>
<p>By understanding and applying t-SNE and UMAP, you can effectively reduce the dimensionality of complex datasets, visualize high-dimensional data, and uncover meaningful patterns and structures that might be hidden in the original space.</p>
</section>
</section>
</section>
<section id="self-organizing-maps-som" class="level2">
<h2 class="anchored" data-anchor-id="self-organizing-maps-som">10.7. Self-Organizing Maps (SOM)</h2>
<p>Self-Organizing Maps (SOM) are a type of artificial neural network used for unsupervised learning, particularly for visualizing and clustering high-dimensional data. SOMs reduce the dimensions of data through the use of self-organizing, competitive learning processes, mapping the data to a lower-dimensional (usually 2D) grid of neurons.</p>
<section id="overview-of-som" class="level3">
<h3 class="anchored" data-anchor-id="overview-of-som">Overview of SOM</h3>
<ul>
<li><strong>Structure of SOM:</strong>
<ul>
<li>A typical SOM consists of two layers: an input layer and an output layer (a 2D grid of neurons).</li>
<li>Each neuron in the output grid has an associated weight vector of the same dimension as the input data.</li>
<li>Neurons are arranged in a grid, and each neuron is connected to its neighboring neurons.</li>
</ul></li>
<li><strong>Training Process:</strong>
<ul>
<li><strong>Initialization:</strong> Initialize the weight vectors of all neurons, typically with small random values.</li>
<li><strong>Competitive Learning:</strong> For each input data point, determine the Best Matching Unit (BMU), which is the neuron whose weight vector is closest to the input vector.</li>
<li><strong>Weight Update:</strong> Update the weight vectors of the BMU and its neighboring neurons to move closer to the input vector.</li>
<li><strong>Neighborhood Function:</strong> The degree of weight adjustment decreases with the distance from the BMU, defined by a neighborhood function.</li>
<li><strong>Iteration:</strong> Repeat the process for many iterations, gradually reducing the learning rate and the radius of the neighborhood function.</li>
</ul></li>
</ul>
</section>
<section id="steps-of-the-som-algorithm" class="level3">
<h3 class="anchored" data-anchor-id="steps-of-the-som-algorithm">Steps of the SOM Algorithm</h3>
<ol type="1">
<li><strong>Initialization:</strong>
<ul>
<li>Initialize the weight vectors <span class="math inline">\(w_i\)</span> for all neurons <span class="math inline">\(i\)</span> randomly.</li>
</ul></li>
<li><strong>Training:</strong>
<ul>
<li><strong>For each training iteration:</strong>
<ol type="1">
<li><strong>Select a random input vector</strong> <span class="math inline">\(x\)</span> from the dataset.</li>
<li><strong>Find the Best Matching Unit (BMU):</strong>
<ul>
<li>Calculate the Euclidean distance between the input vector <span class="math inline">\(x\)</span> and all weight vectors <span class="math inline">\(w_i\)</span>.</li>
<li>Identify the neuron with the minimum distance as the BMU: <span class="math display">\[
\text{BMU} = \arg \min_i \| x - w_i \|
\]</span></li>
</ul></li>
<li><strong>Update the weight vectors:</strong>
<ul>
<li>Adjust the weight vector of the BMU and its neighbors using the learning rate <span class="math inline">\(\alpha(t)\)</span> and neighborhood function <span class="math inline">\(h_{ci}(t)\)</span>: <span class="math display">\[
w_i(t+1) = w_i(t) + \alpha(t) h_{ci}(t) (x - w_i(t))
\]</span></li>
<li>Here, <span class="math inline">\(h_{ci}(t)\)</span> is a Gaussian function of the distance between the BMU <span class="math inline">\(c\)</span> and neuron <span class="math inline">\(i\)</span>: <span class="math display">\[
h_{ci}(t) = \exp \left( -\frac{\| r_c - r_i \|^2}{2\sigma(t)^2} \right)
\]</span></li>
<li><span class="math inline">\(\sigma(t)\)</span> is the neighborhood radius that decreases over time.</li>
</ul></li>
</ol></li>
</ul></li>
<li><strong>Convergence:</strong>
<ul>
<li>Repeat the training process for a predefined number of iterations or until the weight vectors stabilize.</li>
</ul></li>
</ol>
</section>
<section id="applications-of-som" class="level3">
<h3 class="anchored" data-anchor-id="applications-of-som">Applications of SOM</h3>
<ul>
<li><strong>Data Visualization:</strong> SOMs are commonly used for visualizing high-dimensional data by projecting it onto a 2D grid, making it easier to identify patterns and clusters.</li>
<li><strong>Clustering:</strong> SOMs can cluster data by mapping similar data points to the same or neighboring neurons in the grid.</li>
<li><strong>Feature Extraction:</strong> SOMs can be used to extract meaningful features from high-dimensional data, reducing its complexity while retaining important information.</li>
<li><strong>Anomaly Detection:</strong> SOMs can be employed to detect anomalies by identifying data points that do not fit well with the learned map.</li>
</ul>
</section>
<section id="advantages-of-som" class="level3">
<h3 class="anchored" data-anchor-id="advantages-of-som">Advantages of SOM</h3>
<ul>
<li><strong>Intuitive Visualization:</strong> SOMs provide an intuitive way to visualize high-dimensional data in a 2D space.</li>
<li><strong>Topological Preservation:</strong> SOMs preserve the topological structure of the data, meaning that similar data points are mapped to nearby neurons.</li>
<li><strong>Unsupervised Learning:</strong> SOMs do not require labeled data, making them suitable for exploratory data analysis and clustering.</li>
</ul>
</section>
<section id="disadvantages-of-som" class="level3">
<h3 class="anchored" data-anchor-id="disadvantages-of-som">Disadvantages of SOM</h3>
<ul>
<li><strong>Computationally Intensive:</strong> The training process can be computationally intensive, especially for large datasets.</li>
<li><strong>Parameter Sensitivity:</strong> The performance of SOMs depends on the choice of parameters such as the learning rate, neighborhood function, and grid size.</li>
<li><strong>Fixed Grid Size:</strong> The size of the output grid must be predefined, which can limit the flexibility of the model.</li>
</ul>
<p>By understanding and applying Self-Organizing Maps, you can effectively visualize and cluster high-dimensional data, uncovering meaningful patterns and structures that might be hidden in the original space. SOMs are powerful tools for exploratory data analysis and can provide valuable insights into complex datasets.</p>
</section>
</section>
<section id="anomaly-detection-techniques" class="level2">
<h2 class="anchored" data-anchor-id="anomaly-detection-techniques">10.8. Anomaly Detection Techniques</h2>
<p>Anomaly detection is the process of identifying data points, events, or observations that deviate significantly from the majority of the data and are often referred to as outliers. Several techniques can be used for anomaly detection, each with its own strengths and weaknesses.</p>
<section id="isolation-forest" class="level3">
<h3 class="anchored" data-anchor-id="isolation-forest">10.8.1. Isolation Forest</h3>
<p>Isolation Forest is an ensemble-based anomaly detection method that isolates observations by randomly selecting a feature and then randomly selecting a split value between the maximum and minimum values of the selected feature. The idea is that anomalies are few and different, so they are easier to isolate.</p>
<ul>
<li><strong>Algorithm Steps:</strong>
<ol type="1">
<li><strong>Create Isolation Trees:</strong>
<ul>
<li>Build multiple isolation trees (iTrees) from random subsets of the data.</li>
<li>For each tree, recursively partition the data by randomly selecting a feature and a split value until all data points are isolated.</li>
</ul></li>
<li><strong>Path Length Calculation:</strong>
<ul>
<li>The number of splits required to isolate a point is equivalent to the path length from the root to the leaf in the iTree.</li>
<li>Anomalies are isolated quickly, resulting in shorter path lengths.</li>
</ul></li>
<li><strong>Anomaly Score Calculation:</strong>
<ul>
<li>Average the path lengths of the data points across all trees.</li>
<li>Calculate the anomaly score: <span class="math display">\[
\text{Score}(x) = 2^{-\frac{E(h(x))}{c(n)}}
\]</span></li>
<li>Here, <span class="math inline">\(E(h(x))\)</span> is the average path length of point <span class="math inline">\(x\)</span> and <span class="math inline">\(c(n)\)</span> is the average path length of unsuccessful searches in a binary search tree: <span class="math display">\[
c(n) = 2H(n-1) - \frac{2(n-1)}{n}
\]</span></li>
<li>where <span class="math inline">\(H(i)\)</span> is the harmonic number.</li>
</ul></li>
</ol></li>
<li><strong>Advantages:</strong>
<ul>
<li>Efficient for large datasets.</li>
<li>No assumptions about the distribution of the data.</li>
<li>Handles high-dimensional data well.</li>
</ul></li>
<li><strong>Disadvantages:</strong>
<ul>
<li>Sensitive to the number of trees and sample size.</li>
<li>Performance can degrade for small datasets.</li>
</ul></li>
</ul>
</section>
<section id="one-class-svm" class="level3">
<h3 class="anchored" data-anchor-id="one-class-svm">10.8.2. One-class SVM</h3>
<p>One-class SVM is a type of Support Vector Machine used for anomaly detection. It attempts to find a decision boundary that encompasses the majority of the data points and classifies points outside this boundary as anomalies.</p>
<ul>
<li><strong>Algorithm Steps:</strong>
<ol type="1">
<li><strong>Training:</strong>
<ul>
<li>Train the SVM on the dataset to learn a decision function <span class="math inline">\(f(x)\)</span> that is positive for most data points and negative for outliers.</li>
<li>Solve the quadratic optimization problem to find the hyperplane that maximizes the margin from the origin: <span class="math display">\[
\min_{\omega, \rho, \xi} \frac{1}{2} \|\omega\|^2 + \frac{1}{\nu n} \sum_{i=1}^{n} \xi_i - \rho
\]</span> subject to: <span class="math display">\[
(\omega \cdot \phi(x_i)) \geq \rho - \xi_i, \quad \xi_i \geq 0
\]</span></li>
<li>Here, <span class="math inline">\(\phi(x_i)\)</span> maps <span class="math inline">\(x_i\)</span> into a higher-dimensional space, <span class="math inline">\(\xi_i\)</span> are slack variables, and <span class="math inline">\(\nu\)</span> is the regularization parameter.</li>
</ul></li>
<li><strong>Prediction:</strong>
<ul>
<li>For a new point <span class="math inline">\(x\)</span>, compute the decision function <span class="math inline">\(f(x)\)</span>. If <span class="math inline">\(f(x) &lt; 0\)</span>, the point is considered an anomaly.</li>
</ul></li>
</ol></li>
<li><strong>Advantages:</strong>
<ul>
<li>Effective for high-dimensional data.</li>
<li>Flexible with kernel functions to capture complex patterns.</li>
</ul></li>
<li><strong>Disadvantages:</strong>
<ul>
<li>Sensitive to the choice of kernel and its parameters.</li>
<li>Computationally intensive for large datasets.</li>
</ul></li>
</ul>
</section>
<section id="local-outlier-factor-lof" class="level3">
<h3 class="anchored" data-anchor-id="local-outlier-factor-lof">10.8.3. Local Outlier Factor (LOF)</h3>
<p>Local Outlier Factor (LOF) is an unsupervised anomaly detection method that measures the local deviation of a data point with respect to its neighbors. It compares the density of a point to the densities of its neighbors.</p>
<ul>
<li><strong>Algorithm Steps:</strong>
<ol type="1">
<li><strong>Calculate k-Distance:</strong>
<ul>
<li>Compute the distance from each point to its <span class="math inline">\(k\)</span>-th nearest neighbor.</li>
</ul></li>
<li><strong>Calculate Local Reachability Density (LRD):</strong>
<ul>
<li>The reachability distance of a point <span class="math inline">\(p\)</span> with respect to a point <span class="math inline">\(o\)</span> is: <span class="math display">\[
\text{Reachability-Dist}(p, o) = \max(\text{k-Dist}(o), \| p - o \|)
\]</span></li>
<li>The LRD of point <span class="math inline">\(p\)</span> is: <span class="math display">\[
\text{LRD}(p) = \left( \frac{\sum_{o \in N_k(p)} \text{Reachability-Dist}(p, o)}{|N_k(p)|} \right)^{-1}
\]</span></li>
</ul></li>
<li><strong>Calculate LOF:</strong>
<ul>
<li>The LOF of point <span class="math inline">\(p\)</span> is: <span class="math display">\[
\text{LOF}(p) = \frac{\sum_{o \in N_k(p)} \text{LRD}(o)}{|N_k(p)| \cdot \text{LRD}(p)}
\]</span></li>
<li>Points with a high LOF value are considered anomalies.</li>
</ul></li>
</ol></li>
<li><strong>Advantages:</strong>
<ul>
<li>Captures local density variations effectively.</li>
<li>No assumptions about the global data distribution.</li>
</ul></li>
<li><strong>Disadvantages:</strong>
<ul>
<li>Sensitive to the choice of <span class="math inline">\(k\)</span> (number of neighbors).</li>
<li>Computationally expensive for large datasets.</li>
</ul></li>
</ul>
</section>
</section>
<section id="association-rule-learning" class="level2">
<h2 class="anchored" data-anchor-id="association-rule-learning">10.9. Association Rule Learning</h2>
<p>Association rule learning is a rule-based machine learning method for discovering interesting relations between variables in large datasets. It is commonly used in market basket analysis to identify sets of products frequently purchased together.</p>
<section id="apriori-algorithm" class="level3">
<h3 class="anchored" data-anchor-id="apriori-algorithm">10.9.1. Apriori Algorithm</h3>
<p>The Apriori algorithm is a classic algorithm for learning association rules. It uses a breadth-first search strategy to count the support of itemsets and employs a candidate generation function that exploits the downward closure property of support.</p>
<ul>
<li><strong>Algorithm Steps:</strong>
<ol type="1">
<li><strong>Generate Frequent Itemsets:</strong>
<ul>
<li>Identify all itemsets that satisfy the minimum support threshold.</li>
<li>Begin with single items and recursively generate larger itemsets.</li>
<li>Use the property that all non-empty subsets of a frequent itemset must also be frequent.</li>
</ul></li>
<li><strong>Generate Association Rules:</strong>
<ul>
<li>For each frequent itemset, generate rules that meet the minimum confidence threshold.</li>
<li>Calculate confidence for each rule: <span class="math display">\[
\text{Confidence}(A \rightarrow B) = \frac{\text{Support}(A \cup B)}{\text{Support}(A)}
\]</span></li>
<li>Here, <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are itemsets.</li>
</ul></li>
</ol></li>
<li><strong>Advantages:</strong>
<ul>
<li>Simple and easy to implement.</li>
<li>Exploits the downward closure property to reduce the search space.</li>
</ul></li>
<li><strong>Disadvantages:</strong>
<ul>
<li>Can be computationally expensive for large datasets.</li>
<li>Generates a large number of candidate itemsets and rules, which may require further pruning.</li>
</ul></li>
</ul>
</section>
<section id="fp-growth-algorithm" class="level3">
<h3 class="anchored" data-anchor-id="fp-growth-algorithm">10.9.2. FP-growth Algorithm</h3>
<p>The FP-growth (Frequent Pattern growth) algorithm is an efficient and scalable alternative to the Apriori algorithm. It uses a divide-and-conquer approach to decompose the problem into smaller parts, avoiding the candidate generation process.</p>
<ul>
<li><strong>Algorithm Steps:</strong>
<ol type="1">
<li><strong>Construct the FP-Tree:</strong>
<ul>
<li>Scan the dataset and calculate the support for each item.</li>
<li>Build the FP-tree by inserting transactions, maintaining the order of items by descending support.</li>
<li>Nodes in the tree represent items, and edges represent the co-occurrence of items.</li>
</ul></li>
<li><strong>Generate Frequent Itemsets:</strong>
<ul>
<li>Use the FP-tree to extract frequent itemsets.</li>
<li>For each item, construct its conditional FP-tree and recursively mine the frequent itemsets.</li>
<li>Traverse the tree to find frequent itemsets and generate association rules.</li>
</ul></li>
</ol></li>
<li><strong>Advantages:</strong>
<ul>
<li>More efficient than Apriori due to the use of the FP-tree structure.</li>
<li>Reduces the number of database scans, making it suitable for large datasets.</li>
</ul></li>
<li><strong>Disadvantages:</strong>
<ul>
<li>Implementation is more complex compared to Apriori.</li>
<li>Performance can degrade with very large datasets if the FP-tree becomes too large to fit in memory.</li>
</ul></li>
<li><strong>FP-tree Structure:</strong>
<ul>
<li>The FP-tree is a compact representation of the dataset.</li>
<li>It maintains the itemset associations in a compressed format.</li>
<li>Each path in the tree represents a set of transactions sharing common items.</li>
</ul></li>
<li><strong>Conditional FP-tree:</strong>
<ul>
<li>A conditional FP-tree is constructed for each item in the FP-tree.</li>
<li>It represents the subset of transactions that contain the item, facilitating the mining of frequent itemsets.</li>
</ul></li>
</ul>
<p>By understanding and applying these anomaly detection techniques and association rule learning algorithms, you can effectively identify outliers and uncover interesting patterns and associations in your data, providing valuable insights for various applications. These techniques are powerful tools for data mining, enabling you to extract meaningful information and make data-driven decisions.</p>
</section>
</section>
</section>



</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>