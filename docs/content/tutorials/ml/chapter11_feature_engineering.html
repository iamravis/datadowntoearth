<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.47">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Ravi Shankar">

<title>chapter11_feature_engineering – Data Down To Earth</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../">
<link href="../../../logo.png" rel="icon" type="image/png">
<script src="../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../../styles.css">
<link rel="stylesheet" href="../../content.css">
<meta property="og:title" content="– Data Down To Earth">
<meta property="og:description" content="">
<meta property="og:site_name" content="Data Down To Earth">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="../../../index.html" class="navbar-brand navbar-brand-logo">
    <img src="../../../logo.png" alt="" class="navbar-logo">
    </a>
    <a class="navbar-brand" href="../../../index.html">
    <span class="navbar-title">Data Down To Earth</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-projects" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Projects</span>
    </a>
    <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="nav-menu-projects">    
        <li>
    <a class="dropdown-item" href="../../../projects/dsml_projects.html">
 <span class="dropdown-text">Data Science and ML Projects</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../projects/genai_projects.html">
 <span class="dropdown-text">Generative AI Projects</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-tutorials" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Tutorials</span>
    </a>
    <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="nav-menu-tutorials">    
        <li>
    <a class="dropdown-item" href="../../../topics/statistics.html">
 <span class="dropdown-text">Statistics &amp; Probability</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../topics/machine_learning.html">
 <span class="dropdown-text">Machine Learning</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../topics/de.html">
 <span class="dropdown-text">Data Engineering</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../topics/generative_ai.html">
 <span class="dropdown-text">Generative AI</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../topics/public_health.html">
 <span class="dropdown-text">Public Health</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../topics/product_sense.html">
 <span class="dropdown-text">Product Sense</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item">
    <a class="nav-link" href="../../../notes/notes.html"> 
<span class="menu-text">Notes</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/iamrsps"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/iamravishankar/"> <i class="bi bi-linkedin" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content column-page" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
</div>



<div class="quarto-title-meta column-page">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Ravi Shankar </p>
          </div>
  </div>
    
  
    
  </div>
  


</header>


<section id="chapter-11.-feature-engineering" class="level1 text-content">
<h1>Chapter 11. Feature Engineering</h1>
<p>Feature engineering is a crucial step in the machine learning pipeline. It involves creating new features from the raw data to improve the performance of machine learning models. This process can significantly enhance the predictive power of the models.</p>
<section id="feature-creation" class="level2">
<h2 class="anchored" data-anchor-id="feature-creation">11.1. Feature Creation</h2>
<p>Creating new features can help models better capture the underlying patterns in the data. This section covers various techniques for generating new features.</p>
<section id="domain-specific-feature-engineering" class="level3">
<h3 class="anchored" data-anchor-id="domain-specific-feature-engineering">11.1.1. Domain-Specific Feature Engineering</h3>
<p>Domain-specific feature engineering involves creating features based on domain knowledge and understanding of the problem. This approach leverages insights from the specific field to create meaningful features that can improve model performance.</p>
<ul>
<li><strong>Example in Finance:</strong> Creating features such as moving averages, return rates, or volatility measures from stock price data.
<ul>
<li><strong>Moving Average:</strong> Smoothens out price data to identify the trend direction. It is calculated as the average of the past <span class="math inline">\(n\)</span> periods. <span class="math display">\[
\text{MA}_n = \frac{1}{n} \sum_{i=0}^{n-1} P_{t-i}
\]</span></li>
<li><strong>Return Rate:</strong> Measures the gain or loss of an investment over a period. <span class="math display">\[
\text{Return Rate} = \frac{P_{t} - P_{t-1}}{P_{t-1}}
\]</span></li>
<li><strong>Volatility:</strong> Statistical measure of the dispersion of returns. It can be calculated as the standard deviation of returns. <span class="math display">\[
\text{Volatility} = \sqrt{\frac{1}{n} \sum_{i=0}^{n-1} (R_i - \bar{R})^2}
\]</span></li>
</ul></li>
<li><strong>Example in Healthcare:</strong> Generating features like BMI (Body Mass Index), age at diagnosis, or lab test ratios from medical records.
<ul>
<li><strong>BMI:</strong> A measure of body fat based on height and weight. <span class="math display">\[
\text{BMI} = \frac{\text{weight (kg)}}{\text{height (m)}^2}
\]</span></li>
<li><strong>Age at Diagnosis:</strong> The age of a patient at the time of diagnosis, which can be derived from the birthdate and the diagnosis date.</li>
<li><strong>Lab Test Ratios:</strong> Ratios of different lab test results to capture specific health conditions (e.g., AST/ALT ratio for liver health).</li>
</ul></li>
</ul>
</section>
<section id="mathematical-transformations" class="level3">
<h3 class="anchored" data-anchor-id="mathematical-transformations">11.1.2. Mathematical Transformations</h3>
<p>Mathematical transformations involve applying mathematical functions to existing features to create new ones. These transformations can help in normalizing data, handling skewed distributions, or capturing nonlinear relationships.</p>
<ul>
<li><strong>Log Transformation:</strong> Used to stabilize variance and make the data more normally distributed. <span class="math display">\[
x' = \log(x + 1)
\]</span>
<ul>
<li><strong>Example:</strong> Transforming income data to reduce skewness.</li>
</ul></li>
<li><strong>Square Root Transformation:</strong> Useful for reducing right skewness. <span class="math display">\[
x' = \sqrt{x}
\]</span>
<ul>
<li><strong>Example:</strong> Transforming count data (e.g., number of visits to a website).</li>
</ul></li>
<li><strong>Box-Cox Transformation:</strong> A family of power transformations that can make data more normally distributed. <span class="math display">\[
x' = \frac{x^\lambda - 1}{\lambda} \quad \text{for} \quad \lambda \neq 0
\]</span>
<ul>
<li><strong>Example:</strong> Transforming data with different degrees of skewness.</li>
</ul></li>
</ul>
</section>
<section id="temporal-features" class="level3">
<h3 class="anchored" data-anchor-id="temporal-features">11.1.3. Temporal Features</h3>
<p>Temporal features capture time-related patterns in the data. These features are particularly useful in time series analysis and forecasting tasks.</p>
<ul>
<li><strong>Day of the Week:</strong> Encoding the day of the week as a feature (e.g., Monday, Tuesday).
<ul>
<li><strong>Example:</strong> Sales data can show different patterns on weekdays and weekends.</li>
</ul></li>
<li><strong>Month of the Year:</strong> Encoding the month of the year as a feature (e.g., January, February).
<ul>
<li><strong>Example:</strong> Seasonal products have different sales trends depending on the month.</li>
</ul></li>
<li><strong>Time Since Last Event:</strong> Calculating the time elapsed since the last significant event (e.g., last purchase).
<ul>
<li><strong>Example:</strong> In customer churn analysis, the time since the last purchase can be a predictor of churn.</li>
</ul></li>
<li><strong>Lag Features:</strong> Values from previous time steps as features for the current time step.
<ul>
<li><strong>Example:</strong> Using past sales figures to predict future sales.</li>
<li><strong>Formula:</strong> For a time series <span class="math inline">\(X_t\)</span>, a lag feature of <span class="math inline">\(k\)</span> time steps is <span class="math inline">\(X_{t-k}\)</span>.</li>
</ul></li>
<li><strong>Rolling Statistics:</strong> Moving average, moving standard deviation, etc.
<ul>
<li><strong>Example:</strong> Smoothing out short-term fluctuations in time series data.</li>
<li><strong>Formula:</strong> For a time series <span class="math inline">\(X_t\)</span>, a rolling mean over a window of size <span class="math inline">\(w\)</span> is: <span class="math display">\[
\text{Rolling Mean}(X_t) = \frac{1}{w} \sum_{i=0}^{w-1} X_{t-i}
\]</span></li>
</ul></li>
</ul>
</section>
<section id="spatial-features" class="level3">
<h3 class="anchored" data-anchor-id="spatial-features">11.1.4. Spatial Features</h3>
<p>Spatial features capture geographical or spatial relationships in the data. These features are essential in applications such as geographic information systems (GIS), urban planning, and environmental modeling.</p>
<ul>
<li><strong>Latitude and Longitude:</strong> Using geographical coordinates as features.
<ul>
<li><strong>Example:</strong> Predicting property prices based on location.</li>
</ul></li>
<li><strong>Distance to a Point of Interest:</strong> Calculating the distance from a location to a specific point of interest (e.g., distance to the nearest hospital).
<ul>
<li><strong>Example:</strong> Impact of proximity to amenities on house prices.</li>
<li><strong>Formula:</strong> For points <span class="math inline">\((x_1, y_1)\)</span> and <span class="math inline">\((x_2, y_2)\)</span>, the Euclidean distance is: <span class="math display">\[
\text{Distance} = \sqrt{(x_2 - x_1)^2 + (y_2 - y_1)^2}
\]</span></li>
</ul></li>
<li><strong>Spatial Density:</strong> Measuring the density of points within a given radius.
<ul>
<li><strong>Example:</strong> Density of restaurants in a neighborhood.</li>
<li><strong>Formula:</strong> For a point <span class="math inline">\(p\)</span> and radius <span class="math inline">\(r\)</span>, spatial density is the number of points within distance <span class="math inline">\(r\)</span> of <span class="math inline">\(p\)</span>.</li>
</ul></li>
<li><strong>Clustering Features:</strong> Features derived from clustering spatial data, such as cluster labels or cluster centroids.
<ul>
<li><strong>Example:</strong> Grouping properties based on similar characteristics and using cluster information as features.</li>
</ul></li>
</ul>
</section>
</section>
<section id="polynomial-features" class="level2">
<h2 class="anchored" data-anchor-id="polynomial-features">11.2. Polynomial Features</h2>
<p>Polynomial features involve creating new features by taking powers of existing features and their interactions. These features can capture nonlinear relationships between variables.</p>
<section id="interaction-terms" class="level3">
<h3 class="anchored" data-anchor-id="interaction-terms">11.2.1. Interaction Terms</h3>
<p>Interaction terms are created by multiplying two or more features together. These terms can capture the combined effect of multiple variables on the target variable.</p>
<ul>
<li><strong>Example:</strong>
<ul>
<li>For features <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span>, an interaction term would be: <span class="math display">\[
x_{\text{interaction}} = x_1 \cdot x_2
\]</span></li>
<li><strong>Application:</strong> If <span class="math inline">\(x_1\)</span> represents the number of rooms and <span class="math inline">\(x_2\)</span> represents the size of the house, the interaction term could capture how the combination of size and number of rooms affects the house price.</li>
</ul></li>
</ul>
</section>
<section id="higher-order-terms" class="level3">
<h3 class="anchored" data-anchor-id="higher-order-terms">11.2.2. Higher-Order Terms</h3>
<p>Higher-order terms are created by taking powers of existing features. These terms can model nonlinear relationships and add complexity to the model.</p>
<ul>
<li><strong>Second-Order Terms:</strong> Also known as quadratic terms.
<ul>
<li>For a feature <span class="math inline">\(x\)</span>, a second-order term (quadratic term) would be: <span class="math display">\[
x_{\text{quadratic}} = x^2
\]</span></li>
<li><strong>Example:</strong> Capturing the effect of increasing returns or costs in economic models.</li>
</ul></li>
<li><strong>Third-Order Terms:</strong> Also known as cubic terms.
<ul>
<li>For a feature <span class="math inline">\(x\)</span>, a third-order term (cubic term) would be: <span class="math display">\[
x_{\text{cubic}} = x^3
\]</span></li>
<li><strong>Example:</strong> Modeling more complex relationships that involve curvature.</li>
</ul></li>
<li><strong>General Higher-Order Terms:</strong> For any feature <span class="math inline">\(x\)</span> and integer <span class="math inline">\(n\)</span>: <span class="math display">\[
x_{\text{higher-order}} = x^n
\]</span>
<ul>
<li><strong>Application:</strong> Useful in polynomial regression where the relationship between the independent and dependent variables is polynomial in nature.</li>
</ul></li>
</ul>
</section>
</section>
<section id="feature-scaling-and-normalization" class="level2">
<h2 class="anchored" data-anchor-id="feature-scaling-and-normalization">11.4. Feature Scaling and Normalization</h2>
<p>Feature scaling and normalization are crucial steps in preprocessing data for machine learning algorithms. They ensure that features are on a similar scale, which can improve the performance and convergence of algorithms.</p>
<section id="standardization" class="level3">
<h3 class="anchored" data-anchor-id="standardization">11.4.1. Standardization</h3>
<p>Standardization transforms the data to have a mean of 0 and a standard deviation of 1. It is also known as Z-score normalization.</p>
<ul>
<li><strong>Formula:</strong> <span class="math display">\[
x' = \frac{x - \mu}{\sigma}
\]</span>
<ul>
<li>Here, <span class="math inline">\(x\)</span> is the original feature, <span class="math inline">\(\mu\)</span> is the mean of the feature, and <span class="math inline">\(\sigma\)</span> is the standard deviation.</li>
</ul></li>
<li><strong>Application:</strong> Useful for algorithms that assume normally distributed data, such as linear regression and logistic regression.</li>
</ul>
</section>
<section id="min-max-scaling" class="level3">
<h3 class="anchored" data-anchor-id="min-max-scaling">11.4.2. Min-Max Scaling</h3>
<p>Min-Max scaling transforms the data to fit within a specific range, typically [0, 1].</p>
<ul>
<li><strong>Formula:</strong> <span class="math display">\[
x' = \frac{x - x_{\text{min}}}{x_{\text{max}} - x_{\text{min}}}
\]</span>
<ul>
<li>Here, <span class="math inline">\(x\)</span> is the original feature, <span class="math inline">\(x_{\text{min}}\)</span> is the minimum value of the feature, and <span class="math inline">\(x_{\text{max}}\)</span> is the maximum value.</li>
</ul></li>
<li><strong>Application:</strong> Useful for algorithms that do not assume any particular distribution, such as neural networks and k-nearest neighbors.</li>
</ul>
</section>
<section id="robust-scaling" class="level3">
<h3 class="anchored" data-anchor-id="robust-scaling">11.4.3. Robust Scaling</h3>
<p>Robust scaling transforms the data using statistics that are robust to outliers, such as the median and the interquartile range (IQR).</p>
<ul>
<li><strong>Formula:</strong> <span class="math display">\[
x' = \frac{x - \text{median}}{\text{IQR}}
\]</span>
<ul>
<li>Here, <span class="math inline">\(x\)</span> is the original feature, and IQR is the interquartile range (the difference between the 75th and 25th percentiles).</li>
</ul></li>
<li><strong>Application:</strong> Useful for datasets with outliers that can skew the standardization or min-max scaling.</li>
</ul>
</section>
<section id="normalization-l1-l2" class="level3">
<h3 class="anchored" data-anchor-id="normalization-l1-l2">11.4.4. Normalization (L1, L2)</h3>
<p>Normalization scales individual samples to have unit norm, which can be either L1 or L2 norm.</p>
<ul>
<li><p><strong>L1 Normalization:</strong> <span class="math display">\[
x' = \frac{x}{\|x\|_1} = \frac{x}{\sum_{i=1}^{n} |x_i|}
\]</span></p></li>
<li><p><strong>L2 Normalization:</strong> <span class="math display">\[
x' = \frac{x}{\|x\|_2} = \frac{x}{\sqrt{\sum_{i=1}^{n} x_i^2}}
\]</span></p></li>
<li><p><strong>Application:</strong> Useful for algorithms that are sensitive to the magnitude of the features, such as support vector machines and nearest neighbor algorithms.</p></li>
</ul>
</section>
</section>
<section id="feature-importance-and-selection-techniques" class="level2">
<h2 class="anchored" data-anchor-id="feature-importance-and-selection-techniques">11.5. Feature Importance and Selection Techniques</h2>
<p>Feature selection involves selecting the most relevant features for a machine learning model. This process can improve model performance, reduce overfitting, and decrease training time.</p>
<section id="filter-methods" class="level3">
<h3 class="anchored" data-anchor-id="filter-methods">11.5.1. Filter Methods</h3>
<p>Filter methods select features based on statistical measures, independent of the learning algorithm.</p>
<section id="correlation-based-feature-selection" class="level4">
<h4 class="anchored" data-anchor-id="correlation-based-feature-selection">11.5.1.1. Correlation-Based Feature Selection</h4>
<p>Correlation-based feature selection evaluates the correlation between each feature and the target variable.</p>
<ul>
<li><strong>Formula:</strong> <span class="math display">\[
\rho(X, Y) = \frac{\text{Cov}(X, Y)}{\sigma_X \sigma_Y}
\]</span>
<ul>
<li>Here, <span class="math inline">\(\rho\)</span> is the Pearson correlation coefficient, <span class="math inline">\(\text{Cov}(X, Y)\)</span> is the covariance between feature <span class="math inline">\(X\)</span> and target <span class="math inline">\(Y\)</span>, and <span class="math inline">\(\sigma_X\)</span>, <span class="math inline">\(\sigma_Y\)</span> are the standard deviations of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>.</li>
</ul></li>
<li><strong>Application:</strong> Useful for identifying linear relationships.</li>
</ul>
</section>
<section id="mutual-information" class="level4">
<h4 class="anchored" data-anchor-id="mutual-information">11.5.1.2. Mutual Information</h4>
<p>Mutual information measures the amount of information obtained about one variable through another variable.</p>
<ul>
<li><strong>Formula:</strong> <span class="math display">\[
I(X; Y) = \sum_{x \in X} \sum_{y \in Y} p(x, y) \log \frac{p(x, y)}{p(x) p(y)}
\]</span>
<ul>
<li>Here, <span class="math inline">\(p(x, y)\)</span> is the joint probability distribution of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, and <span class="math inline">\(p(x)\)</span>, <span class="math inline">\(p(y)\)</span> are the marginal probability distributions.</li>
</ul></li>
<li><strong>Application:</strong> Useful for capturing non-linear relationships between variables.</li>
</ul>
</section>
<section id="chi-squared-test" class="level4">
<h4 class="anchored" data-anchor-id="chi-squared-test">11.5.1.3. Chi-Squared Test</h4>
<p>The chi-squared test measures the association between categorical features and the target variable.</p>
<ul>
<li><strong>Formula:</strong> <span class="math display">\[
\chi^2 = \sum \frac{(O_i - E_i)^2}{E_i}
\]</span>
<ul>
<li>Here, <span class="math inline">\(O_i\)</span> is the observed frequency, and <span class="math inline">\(E_i\)</span> is the expected frequency.</li>
</ul></li>
<li><strong>Application:</strong> Useful for categorical data to test independence between features and the target variable.</li>
</ul>
</section>
</section>
<section id="wrapper-methods" class="level3">
<h3 class="anchored" data-anchor-id="wrapper-methods">11.5.2. Wrapper Methods</h3>
<p>Wrapper methods evaluate feature subsets by training and testing a specific model. These methods are more computationally intensive but can result in better performance.</p>
<section id="recursive-feature-elimination" class="level4">
<h4 class="anchored" data-anchor-id="recursive-feature-elimination">11.5.2.1. Recursive Feature Elimination</h4>
<p>Recursive Feature Elimination (RFE) recursively removes the least important features and builds the model on the remaining features.</p>
<ul>
<li><strong>Algorithm Steps:</strong>
<ol type="1">
<li>Train the model on the full feature set.</li>
<li>Rank features based on their importance.</li>
<li>Remove the least important feature.</li>
<li>Repeat until the desired number of features is reached.</li>
</ol></li>
<li><strong>Application:</strong> Useful for models where feature importance can be easily determined, such as linear regression and support vector machines.</li>
</ul>
</section>
<section id="forwardbackward-selection" class="level4">
<h4 class="anchored" data-anchor-id="forwardbackward-selection">11.5.2.2. Forward/Backward Selection</h4>
<p>Forward and backward selection are stepwise selection techniques to add or remove features.</p>
<ul>
<li><strong>Forward Selection:</strong>
<ol type="1">
<li>Start with no features.</li>
<li>Add the feature that improves the model performance the most.</li>
<li>Repeat until no significant improvement is achieved.</li>
</ol></li>
<li><strong>Backward Selection:</strong>
<ol type="1">
<li>Start with all features.</li>
<li>Remove the feature that decreases the model performance the least.</li>
<li>Repeat until no significant improvement is achieved.</li>
</ol></li>
<li><strong>Application:</strong> Useful for finding a balance between model performance and complexity.</li>
</ul>
</section>
</section>
<section id="embedded-methods" class="level3">
<h3 class="anchored" data-anchor-id="embedded-methods">11.5.3. Embedded Methods</h3>
<p>Embedded methods perform feature selection during the model training process. These methods are less computationally expensive than wrapper methods and can still capture interactions between features and the model.</p>
<section id="lasso-regularization" class="level4">
<h4 class="anchored" data-anchor-id="lasso-regularization">11.5.3.1. Lasso Regularization</h4>
<p>Lasso (Least Absolute Shrinkage and Selection Operator) regularization adds a penalty equal to the absolute value of the magnitude of coefficients to the loss function.</p>
<ul>
<li><strong>Formula:</strong> <span class="math display">\[
\text{Loss} = \sum_{i=1}^{n} (y_i - \hat{y}_i)^2 + \lambda \sum_{j=1}^{p} |w_j|
\]</span>
<ul>
<li>Here, <span class="math inline">\(\lambda\)</span> is the regularization parameter, and <span class="math inline">\(w_j\)</span> are the model coefficients.</li>
</ul></li>
<li><strong>Application:</strong> Useful for reducing the number of features by driving some coefficients to zero.</li>
</ul>
</section>
<section id="random-forest-feature-importance" class="level4">
<h4 class="anchored" data-anchor-id="random-forest-feature-importance">11.5.3.2. Random Forest Feature Importance</h4>
<p>Random forests provide feature importance scores based on the average decrease in impurity or the average increase in accuracy when the feature is used.</p>
<ul>
<li><strong>Algorithm Steps:</strong>
<ol type="1">
<li>Train a random forest model.</li>
<li>Calculate feature importance scores based on the reduction in Gini impurity or increase in accuracy.</li>
<li>Rank features based on their importance scores.</li>
</ol></li>
<li><strong>Application:</strong> Useful for identifying important features in datasets with complex, non-linear relationships.</li>
</ul>
<p>By understanding and applying these feature scaling, normalization, and selection techniques, you can enhance the performance of your machine learning models, improve their interpretability, and reduce overfitting.</p>
</section>
</section>
</section>
<section id="automated-feature-engineering" class="level2">
<h2 class="anchored" data-anchor-id="automated-feature-engineering">11.6. Automated Feature Engineering</h2>
<p>Automated feature engineering uses algorithms and tools to automatically create and select features from the raw data. This approach can save time and improve efficiency in the feature engineering process.</p>
<section id="featuretools-library" class="level3">
<h3 class="anchored" data-anchor-id="featuretools-library">11.6.1. Featuretools Library</h3>
<p>Featuretools is an open-source Python library for automated feature engineering. It simplifies the process of creating complex features from relational datasets.</p>
<ul>
<li><strong>Overview:</strong>
<ul>
<li><strong>Deep Feature Synthesis (DFS):</strong> Featuretools uses DFS to automatically generate features. DFS stacks multiple primitive operations (e.g., aggregations and transformations) to create new features.</li>
<li><strong>Entity Set:</strong> Featuretools organizes data into an entity set, which is a collection of tables (entities) and the relationships between them.</li>
</ul></li>
<li><strong>Key Components:</strong>
<ul>
<li><strong>Entities:</strong> Tables in the dataset (e.g., customers, transactions).</li>
<li><strong>Relationships:</strong> Connections between tables (e.g., a customer can have many transactions).</li>
<li><strong>Primitive Operations:</strong> Basic operations used to generate features, including:
<ul>
<li><strong>Aggregation Primitives:</strong> Operations that summarize data (e.g., sum, mean).</li>
<li><strong>Transformation Primitives:</strong> Operations that transform data (e.g., difference, division).</li>
</ul></li>
</ul></li>
<li><strong>Example Workflow:</strong>
<ol type="1">
<li><strong>Define an Entity Set:</strong> Create an entity set and add entities (tables) and relationships.</li>
<li><strong>Run Deep Feature Synthesis:</strong> Use DFS to generate new features based on the entity set.</li>
<li><strong>Select Features:</strong> Choose the most relevant features for the machine learning model.</li>
</ol></li>
</ul>
</section>
<section id="autofeat" class="level3">
<h3 class="anchored" data-anchor-id="autofeat">11.6.2. AutoFeat</h3>
<p>AutoFeat is a Python library designed to automate the process of feature engineering and selection. It focuses on creating polynomial features and interactions to enhance model performance.</p>
<ul>
<li><strong>Overview:</strong>
<ul>
<li><strong>Feature Generation:</strong> AutoFeat generates polynomial features and interactions based on the input features.</li>
<li><strong>Feature Selection:</strong> It automatically selects the most relevant features, reducing the dimensionality of the dataset and improving model performance.</li>
</ul></li>
<li><strong>Key Features:</strong>
<ul>
<li><strong>Polynomial Features:</strong> AutoFeat creates higher-order terms and interactions between features.</li>
<li><strong>Customizable:</strong> Users can specify the degree of polynomial features and the types of interactions to generate.</li>
<li><strong>Model Agnostic:</strong> AutoFeat works with any machine learning model, making it versatile for various tasks.</li>
</ul></li>
<li><strong>Example Workflow:</strong>
<ol type="1">
<li><strong>Initialize AutoFeat:</strong> Create an AutoFeat object and configure the desired feature generation settings.</li>
<li><strong>Fit and Transform:</strong> Fit the AutoFeat object to the data to generate and select new features.</li>
<li><strong>Integrate with Model:</strong> Use the transformed dataset with the selected features to train the machine learning model.</li>
</ol></li>
</ul>
</section>
<section id="tsfresh-for-time-series-feature-extraction" class="level3">
<h3 class="anchored" data-anchor-id="tsfresh-for-time-series-feature-extraction">11.6.3. tsfresh for Time Series Feature Extraction</h3>
<p>tsfresh is a Python library specifically designed for extracting relevant features from time series data. It automates the process of generating features that can be used for time series classification and regression tasks.</p>
<ul>
<li><strong>Overview:</strong>
<ul>
<li><strong>Feature Extraction:</strong> tsfresh extracts a large number of features from time series data, including statistical, time-based, and frequency-based features.</li>
<li><strong>Feature Selection:</strong> It includes methods for selecting the most relevant features based on their importance.</li>
</ul></li>
<li><strong>Key Components:</strong>
<ul>
<li><strong>Feature Calculators:</strong> Functions that compute various features from time series data (e.g., mean, variance, autocorrelation).</li>
<li><strong>Relevance Table:</strong> A table that lists the extracted features and their relevance scores, helping to identify the most important features.</li>
</ul></li>
<li><strong>Example Workflow:</strong>
<ol type="1">
<li><strong>Extract Features:</strong> Use tsfresh to extract features from time series data.</li>
<li><strong>Select Relevant Features:</strong> Filter the features based on their relevance scores.</li>
<li><strong>Integrate with Model:</strong> Use the selected features to train time series models.</li>
</ol></li>
</ul>
</section>
</section>
<section id="feature-learning" class="level2">
<h2 class="anchored" data-anchor-id="feature-learning">11.7. Feature Learning</h2>
<p>Feature learning involves using machine learning models to automatically discover and learn features from the raw data. This approach can capture complex patterns and representations that are difficult to manually engineer.</p>
<section id="autoencoders-for-feature-learning" class="level3">
<h3 class="anchored" data-anchor-id="autoencoders-for-feature-learning">11.7.1. Autoencoders for Feature Learning</h3>
<p>Autoencoders are a type of neural network used for unsupervised learning of features. They learn to compress data into a lower-dimensional representation and then reconstruct it back to the original input.</p>
<ul>
<li><strong>Architecture:</strong>
<ul>
<li><strong>Encoder:</strong> The part of the network that compresses the input data into a lower-dimensional representation (latent space).</li>
<li><strong>Decoder:</strong> The part of the network that reconstructs the input data from the lower-dimensional representation.</li>
</ul></li>
<li><strong>Training Process:</strong>
<ul>
<li><strong>Objective:</strong> Minimize the reconstruction error between the input and the reconstructed output.</li>
<li><strong>Loss Function:</strong> Typically, the mean squared error (MSE) is used to measure reconstruction error. <span class="math display">\[
\text{MSE} = \frac{1}{n} \sum_{i=1}^{n} (x_i - \hat{x}_i)^2
\]</span></li>
</ul></li>
<li><strong>Applications:</strong>
<ul>
<li><strong>Dimensionality Reduction:</strong> Autoencoders can be used to reduce the dimensionality of data while retaining important features.</li>
<li><strong>Anomaly Detection:</strong> By learning the normal patterns in data, autoencoders can identify anomalies based on high reconstruction error.</li>
</ul></li>
</ul>
</section>
<section id="restricted-boltzmann-machines" class="level3">
<h3 class="anchored" data-anchor-id="restricted-boltzmann-machines">11.7.2. Restricted Boltzmann Machines</h3>
<p>Restricted Boltzmann Machines (RBMs) are stochastic neural networks that can learn a probability distribution over the input data. They are used for feature learning and dimensionality reduction.</p>
<ul>
<li><strong>Architecture:</strong>
<ul>
<li><strong>Visible Layer:</strong> Represents the input data.</li>
<li><strong>Hidden Layer:</strong> Represents the learned features.</li>
</ul></li>
<li><strong>Training Process:</strong>
<ul>
<li><strong>Objective:</strong> Maximize the likelihood of the input data by learning the weights between the visible and hidden layers.</li>
<li><strong>Contrastive Divergence:</strong> An efficient algorithm used to approximate the gradient of the likelihood function.</li>
</ul></li>
<li><strong>Applications:</strong>
<ul>
<li><strong>Pre-training for Deep Networks:</strong> RBMs can be used to pre-train deep neural networks, improving their performance.</li>
<li><strong>Collaborative Filtering:</strong> RBMs are used in recommendation systems to learn user-item interactions.</li>
</ul></li>
</ul>
</section>
</section>
<section id="handling-missing-data" class="level2">
<h2 class="anchored" data-anchor-id="handling-missing-data">11.8. Handling Missing Data</h2>
<p>Handling missing data is a crucial aspect of data preprocessing. Missing values can lead to biased estimates and reduced model performance if not properly addressed.</p>
<section id="imputation-techniques" class="level3">
<h3 class="anchored" data-anchor-id="imputation-techniques">11.8.1. Imputation Techniques</h3>
<p>Imputation involves filling in the missing values with estimated values based on the observed data.</p>
<ul>
<li><strong>Mean/Median Imputation:</strong>
<ul>
<li><strong>Mean Imputation:</strong> Replace missing values with the mean of the observed values. <span class="math display">\[
x_{\text{imputed}} = \frac{\sum_{i=1}^{n} x_i}{n}
\]</span></li>
<li><strong>Median Imputation:</strong> Replace missing values with the median of the observed values. <span class="math display">\[
x_{\text{imputed}} = \text{median}(x)
\]</span></li>
</ul></li>
<li><strong>Mode Imputation:</strong>
<ul>
<li>Replace missing values with the mode (most frequent value) of the observed values. <span class="math display">\[
x_{\text{imputed}} = \text{mode}(x)
\]</span></li>
</ul></li>
<li><strong>K-Nearest Neighbors (KNN) Imputation:</strong>
<ul>
<li>Replace missing values with the average of the nearest neighbors. <span class="math display">\[
x_{\text{imputed}} = \frac{\sum_{i=1}^{k} x_{\text{neighbor}_i}}{k}
\]</span></li>
</ul></li>
<li><strong>Multiple Imputation:</strong>
<ul>
<li>Generate multiple imputations for each missing value and average the results to account for the uncertainty of the imputed values.</li>
</ul></li>
</ul>
</section>
<section id="missing-value-indicators" class="level3">
<h3 class="anchored" data-anchor-id="missing-value-indicators">11.8.2. Missing Value Indicators</h3>
<p>Missing value indicators are binary flags that indicate whether a value was originally missing. This approach preserves information about the presence of missing data.</p>
<ul>
<li><strong>Indicator Variable:</strong>
<ul>
<li>Create a new binary feature for each original feature with missing values. <span class="math display">\[
I(x_i) = \begin{cases}
  1 &amp; \text{if } x_i \text{ is missing} \\
  0 &amp; \text{if } x_i \text{ is not missing}
\end{cases}
\]</span></li>
</ul></li>
<li><strong>Applications:</strong>
<ul>
<li><strong>Modeling:</strong> Including missing value indicators can help models account for the missing data pattern.</li>
<li><strong>Data Analysis:</strong> Indicators can reveal systematic patterns in missing data.</li>
</ul></li>
</ul>
<p>By understanding and applying these advanced techniques for automated feature engineering, feature learning, and handling missing data, you can enhance the robustness and performance of your machine learning models.</p>
</section>
</section>
</section>



</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>