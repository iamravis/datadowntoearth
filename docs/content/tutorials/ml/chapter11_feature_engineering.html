<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.47">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Ravi Shankar">

<title>chapter11_feature_engineering – Data Down To Earth</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../">
<link href="../../../logo.png" rel="icon" type="image/png">
<script src="../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../../styles.css">
<link rel="stylesheet" href="../../content.css">
<meta property="og:title" content="– Data Down To Earth">
<meta property="og:description" content="">
<meta property="og:site_name" content="Data Down To Earth">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="../../../index.html" class="navbar-brand navbar-brand-logo">
    <img src="../../../logo.png" alt="" class="navbar-logo">
    </a>
    <a class="navbar-brand" href="../../../index.html">
    <span class="navbar-title">Data Down To Earth</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-projects" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Projects</span>
    </a>
    <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="nav-menu-projects">    
        <li>
    <a class="dropdown-item" href="../../../projects/dsml_projects.html">
 <span class="dropdown-text">Data Science and ML Projects</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../projects/genai_projects.html">
 <span class="dropdown-text">Generative AI Projects</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-tutorials" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Tutorials</span>
    </a>
    <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="nav-menu-tutorials">    
        <li>
    <a class="dropdown-item" href="../../../topics/statistics.html">
 <span class="dropdown-text">Statistics &amp; Probability</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../topics/machine_learning.html">
 <span class="dropdown-text">Machine Learning</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../topics/de.html">
 <span class="dropdown-text">Data Engineering</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../topics/generative_ai.html">
 <span class="dropdown-text">Generative AI</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../topics/public_health.html">
 <span class="dropdown-text">Public Health</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../topics/product_sense.html">
 <span class="dropdown-text">Product Sense</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item">
    <a class="nav-link" href="../../../notes/notes.html"> 
<span class="menu-text">Notes</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/iamrsps"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/iamravishankar/"> <i class="bi bi-linkedin" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content column-page" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
</div>



<div class="quarto-title-meta column-page">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Ravi Shankar </p>
          </div>
  </div>
    
  
    
  </div>
  


</header>


<section id="chapter-11.-feature-engineering" class="level1 text-content">
<h1>Chapter 11. Feature Engineering</h1>
<p>Feature engineering is a crucial step in the machine learning pipeline. It involves creating new features from the raw data to improve the performance of machine learning models. This process can significantly enhance the predictive power of the models.</p>
<section id="feature-creation" class="level2">
<h2 class="anchored" data-anchor-id="feature-creation">11.1. Feature Creation</h2>
<p>Creating new features can help models better capture the underlying patterns in the data. This section covers various techniques for generating new features.</p>
<section id="domain-specific-feature-engineering" class="level3">
<h3 class="anchored" data-anchor-id="domain-specific-feature-engineering">11.1.1. Domain-Specific Feature Engineering</h3>
<p>Domain-specific feature engineering involves creating features based on domain knowledge and understanding of the problem. This approach leverages insights from the specific field to create meaningful features that can improve model performance.</p>
<ul>
<li><strong>Example in Finance:</strong> Creating features such as moving averages, return rates, or volatility measures from stock price data.
<ul>
<li><strong>Moving Average:</strong> Smoothens out price data to identify the trend direction. It is calculated as the average of the past <span class="math inline">\(n\)</span> periods. <span class="math display">\[
\text{MA}_n = \frac{1}{n} \sum_{i=0}^{n-1} P_{t-i}
\]</span></li>
<li><strong>Return Rate:</strong> Measures the gain or loss of an investment over a period. <span class="math display">\[
\text{Return Rate} = \frac{P_{t} - P_{t-1}}{P_{t-1}}
\]</span></li>
<li><strong>Volatility:</strong> Statistical measure of the dispersion of returns. It can be calculated as the standard deviation of returns. <span class="math display">\[
\text{Volatility} = \sqrt{\frac{1}{n} \sum_{i=0}^{n-1} (R_i - \bar{R})^2}
\]</span></li>
</ul></li>
<li><strong>Example in Healthcare:</strong> Generating features like BMI (Body Mass Index), age at diagnosis, or lab test ratios from medical records.
<ul>
<li><strong>BMI:</strong> A measure of body fat based on height and weight. <span class="math display">\[
\text{BMI} = \frac{\text{weight (kg)}}{\text{height (m)}^2}
\]</span></li>
<li><strong>Age at Diagnosis:</strong> The age of a patient at the time of diagnosis, which can be derived from the birthdate and the diagnosis date.</li>
<li><strong>Lab Test Ratios:</strong> Ratios of different lab test results to capture specific health conditions (e.g., AST/ALT ratio for liver health).</li>
</ul></li>
</ul>
</section>
<section id="mathematical-transformations" class="level3">
<h3 class="anchored" data-anchor-id="mathematical-transformations">11.1.2. Mathematical Transformations</h3>
<p>Mathematical transformations involve applying mathematical functions to existing features to create new ones. These transformations can help in normalizing data, handling skewed distributions, or capturing nonlinear relationships.</p>
<ul>
<li><strong>Log Transformation:</strong> Used to stabilize variance and make the data more normally distributed. <span class="math display">\[
x' = \log(x + 1)
\]</span>
<ul>
<li><strong>Example:</strong> Transforming income data to reduce skewness.</li>
</ul></li>
<li><strong>Square Root Transformation:</strong> Useful for reducing right skewness. <span class="math display">\[
x' = \sqrt{x}
\]</span>
<ul>
<li><strong>Example:</strong> Transforming count data (e.g., number of visits to a website).</li>
</ul></li>
<li><strong>Box-Cox Transformation:</strong> A family of power transformations that can make data more normally distributed. <span class="math display">\[
x' = \frac{x^\lambda - 1}{\lambda} \quad \text{for} \quad \lambda \neq 0
\]</span>
<ul>
<li><strong>Example:</strong> Transforming data with different degrees of skewness.</li>
</ul></li>
</ul>
</section>
<section id="temporal-features" class="level3">
<h3 class="anchored" data-anchor-id="temporal-features">11.1.3. Temporal Features</h3>
<p>Temporal features capture time-related patterns in the data. These features are particularly useful in time series analysis and forecasting tasks.</p>
<ul>
<li><strong>Day of the Week:</strong> Encoding the day of the week as a feature (e.g., Monday, Tuesday).
<ul>
<li><strong>Example:</strong> Sales data can show different patterns on weekdays and weekends.</li>
</ul></li>
<li><strong>Month of the Year:</strong> Encoding the month of the year as a feature (e.g., January, February).
<ul>
<li><strong>Example:</strong> Seasonal products have different sales trends depending on the month.</li>
</ul></li>
<li><strong>Time Since Last Event:</strong> Calculating the time elapsed since the last significant event (e.g., last purchase).
<ul>
<li><strong>Example:</strong> In customer churn analysis, the time since the last purchase can be a predictor of churn.</li>
</ul></li>
<li><strong>Lag Features:</strong> Values from previous time steps as features for the current time step.
<ul>
<li><strong>Example:</strong> Using past sales figures to predict future sales.</li>
<li><strong>Formula:</strong> For a time series <span class="math inline">\(X_t\)</span>, a lag feature of <span class="math inline">\(k\)</span> time steps is <span class="math inline">\(X_{t-k}\)</span>.</li>
</ul></li>
<li><strong>Rolling Statistics:</strong> Moving average, moving standard deviation, etc.
<ul>
<li><strong>Example:</strong> Smoothing out short-term fluctuations in time series data.</li>
<li><strong>Formula:</strong> For a time series <span class="math inline">\(X_t\)</span>, a rolling mean over a window of size <span class="math inline">\(w\)</span> is: <span class="math display">\[
\text{Rolling Mean}(X_t) = \frac{1}{w} \sum_{i=0}^{w-1} X_{t-i}
\]</span></li>
</ul></li>
</ul>
</section>
<section id="spatial-features" class="level3">
<h3 class="anchored" data-anchor-id="spatial-features">11.1.4. Spatial Features</h3>
<p>Spatial features capture geographical or spatial relationships in the data. These features are essential in applications such as geographic information systems (GIS), urban planning, and environmental modeling.</p>
<ul>
<li><strong>Latitude and Longitude:</strong> Using geographical coordinates as features.
<ul>
<li><strong>Example:</strong> Predicting property prices based on location.</li>
</ul></li>
<li><strong>Distance to a Point of Interest:</strong> Calculating the distance from a location to a specific point of interest (e.g., distance to the nearest hospital).
<ul>
<li><strong>Example:</strong> Impact of proximity to amenities on house prices.</li>
<li><strong>Formula:</strong> For points <span class="math inline">\((x_1, y_1)\)</span> and <span class="math inline">\((x_2, y_2)\)</span>, the Euclidean distance is: <span class="math display">\[
\text{Distance} = \sqrt{(x_2 - x_1)^2 + (y_2 - y_1)^2}
\]</span></li>
</ul></li>
<li><strong>Spatial Density:</strong> Measuring the density of points within a given radius.
<ul>
<li><strong>Example:</strong> Density of restaurants in a neighborhood.</li>
<li><strong>Formula:</strong> For a point <span class="math inline">\(p\)</span> and radius <span class="math inline">\(r\)</span>, spatial density is the number of points within distance <span class="math inline">\(r\)</span> of <span class="math inline">\(p\)</span>.</li>
</ul></li>
<li><strong>Clustering Features:</strong> Features derived from clustering spatial data, such as cluster labels or cluster centroids.
<ul>
<li><strong>Example:</strong> Grouping properties based on similar characteristics and using cluster information as features.</li>
</ul></li>
</ul>
</section>
</section>
<section id="polynomial-features" class="level2">
<h2 class="anchored" data-anchor-id="polynomial-features">11.2. Polynomial Features</h2>
<p>Polynomial features involve creating new features by taking powers of existing features and their interactions. These features can capture nonlinear relationships between variables.</p>
<section id="interaction-terms" class="level3">
<h3 class="anchored" data-anchor-id="interaction-terms">11.2.1. Interaction Terms</h3>
<p>Interaction terms are created by multiplying two or more features together. These terms can capture the combined effect of multiple variables on the target variable.</p>
<ul>
<li><strong>Example:</strong>
<ul>
<li>For features <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span>, an interaction term would be: <span class="math display">\[
x_{\text{interaction}} = x_1 \cdot x_2
\]</span></li>
<li><strong>Application:</strong> If <span class="math inline">\(x_1\)</span> represents the number of rooms and <span class="math inline">\(x_2\)</span> represents the size of the house, the interaction term could capture how the combination of size and number of rooms affects the house price.</li>
</ul></li>
</ul>
</section>
<section id="higher-order-terms" class="level3">
<h3 class="anchored" data-anchor-id="higher-order-terms">11.2.2. Higher-Order Terms</h3>
<p>Higher-order terms are created by taking powers of existing features. These terms can model nonlinear relationships and add complexity to the model.</p>
<ul>
<li><strong>Second-Order Terms:</strong> Also known as quadratic terms.
<ul>
<li>For a feature <span class="math inline">\(x\)</span>, a second-order term (quadratic term) would be: <span class="math display">\[
x_{\text{quadratic}} = x^2
\]</span></li>
<li><strong>Example:</strong> Capturing the effect of increasing returns or costs in economic models.</li>
</ul></li>
<li><strong>Third-Order Terms:</strong> Also known as cubic terms.
<ul>
<li>For a feature <span class="math inline">\(x\)</span>, a third-order term (cubic term) would be: <span class="math display">\[
x_{\text{cubic}} = x^3
\]</span></li>
<li><strong>Example:</strong> Modeling more complex relationships that involve curvature.</li>
</ul></li>
<li><strong>General Higher-Order Terms:</strong> For any feature <span class="math inline">\(x\)</span> and integer <span class="math inline">\(n\)</span>: <span class="math display">\[
x_{\text{higher-order}} = x^n
\]</span>
<ul>
<li><strong>Application:</strong> Useful in polynomial regression where the relationship between the independent and dependent variables is polynomial in nature.</li>
</ul></li>
</ul>
</section>
</section>
<section id="feature-scaling-and-normalization" class="level2">
<h2 class="anchored" data-anchor-id="feature-scaling-and-normalization">11.4. Feature Scaling and Normalization</h2>
<p>Feature scaling and normalization are crucial steps in preprocessing data for machine learning algorithms. They ensure that features are on a similar scale, which can improve the performance and convergence of algorithms.</p>
<section id="standardization" class="level3">
<h3 class="anchored" data-anchor-id="standardization">11.4.1. Standardization</h3>
<p>Standardization transforms the data to have a mean of 0 and a standard deviation of 1. It is also known as Z-score normalization.</p>
<ul>
<li><strong>Formula:</strong> <span class="math display">\[
x' = \frac{x - \mu}{\sigma}
\]</span>
<ul>
<li>Here, <span class="math inline">\(x\)</span> is the original feature, <span class="math inline">\(\mu\)</span> is the mean of the feature, and <span class="math inline">\(\sigma\)</span> is the standard deviation.</li>
</ul></li>
<li><strong>Application:</strong> Useful for algorithms that assume normally distributed data, such as linear regression and logistic regression.</li>
</ul>
</section>
<section id="min-max-scaling" class="level3">
<h3 class="anchored" data-anchor-id="min-max-scaling">11.4.2. Min-Max Scaling</h3>
<p>Min-Max scaling transforms the data to fit within a specific range, typically [0, 1].</p>
<ul>
<li><strong>Formula:</strong> <span class="math display">\[
x' = \frac{x - x_{\text{min}}}{x_{\text{max}} - x_{\text{min}}}
\]</span>
<ul>
<li>Here, <span class="math inline">\(x\)</span> is the original feature, <span class="math inline">\(x_{\text{min}}\)</span> is the minimum value of the feature, and <span class="math inline">\(x_{\text{max}}\)</span> is the maximum value.</li>
</ul></li>
<li><strong>Application:</strong> Useful for algorithms that do not assume any particular distribution, such as neural networks and k-nearest neighbors.</li>
</ul>
</section>
<section id="robust-scaling" class="level3">
<h3 class="anchored" data-anchor-id="robust-scaling">11.4.3. Robust Scaling</h3>
<p>Robust scaling transforms the data using statistics that are robust to outliers, such as the median and the interquartile range (IQR).</p>
<ul>
<li><strong>Formula:</strong> <span class="math display">\[
x' = \frac{x - \text{median}}{\text{IQR}}
\]</span>
<ul>
<li>Here, <span class="math inline">\(x\)</span> is the original feature, and IQR is the interquartile range (the difference between the 75th and 25th percentiles).</li>
</ul></li>
<li><strong>Application:</strong> Useful for datasets with outliers that can skew the standardization or min-max scaling.</li>
</ul>
</section>
<section id="normalization-l1-l2" class="level3">
<h3 class="anchored" data-anchor-id="normalization-l1-l2">11.4.4. Normalization (L1, L2)</h3>
<p>Normalization scales individual samples to have unit norm, which can be either L1 or L2 norm.</p>
<ul>
<li><p><strong>L1 Normalization:</strong> <span class="math display">\[
x' = \frac{x}{\|x\|_1} = \frac{x}{\sum_{i=1}^{n} |x_i|}
\]</span></p></li>
<li><p><strong>L2 Normalization:</strong> <span class="math display">\[
x' = \frac{x}{\|x\|_2} = \frac{x}{\sqrt{\sum_{i=1}^{n} x_i^2}}
\]</span></p></li>
<li><p><strong>Application:</strong> Useful for algorithms that are sensitive to the magnitude of the features, such as support vector machines and nearest neighbor algorithms.</p></li>
</ul>
</section>
</section>
<section id="feature-importance-and-selection-techniques" class="level2">
<h2 class="anchored" data-anchor-id="feature-importance-and-selection-techniques">11.5. Feature Importance and Selection Techniques</h2>
<p>Feature selection involves selecting the most relevant features for a machine learning model. This process can improve model performance, reduce overfitting, and decrease training time.</p>
<section id="filter-methods" class="level3">
<h3 class="anchored" data-anchor-id="filter-methods">11.5.1. Filter Methods</h3>
<p>Filter methods select features based on statistical measures, independent of the learning algorithm.</p>
<section id="correlation-based-feature-selection" class="level4">
<h4 class="anchored" data-anchor-id="correlation-based-feature-selection">11.5.1.1. Correlation-Based Feature Selection</h4>
<p>Correlation-based feature selection evaluates the correlation between each feature and the target variable.</p>
<ul>
<li><strong>Formula:</strong> <span class="math display">\[
\rho(X, Y) = \frac{\text{Cov}(X, Y)}{\sigma_X \sigma_Y}
\]</span>
<ul>
<li>Here, <span class="math inline">\(\rho\)</span> is the Pearson correlation coefficient, <span class="math inline">\(\text{Cov}(X, Y)\)</span> is the covariance between feature <span class="math inline">\(X\)</span> and target <span class="math inline">\(Y\)</span>, and <span class="math inline">\(\sigma_X\)</span>, <span class="math inline">\(\sigma_Y\)</span> are the standard deviations of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>.</li>
</ul></li>
<li><strong>Application:</strong> Useful for identifying linear relationships.</li>
</ul>
</section>
<section id="mutual-information" class="level4">
<h4 class="anchored" data-anchor-id="mutual-information">11.5.1.2. Mutual Information</h4>
<p>Mutual information measures the amount of information obtained about one variable through another variable.</p>
<ul>
<li><strong>Formula:</strong> <span class="math display">\[
I(X; Y) = \sum_{x \in X} \sum_{y \in Y} p(x, y) \log \frac{p(x, y)}{p(x) p(y)}
\]</span>
<ul>
<li>Here, <span class="math inline">\(p(x, y)\)</span> is the joint probability distribution of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, and <span class="math inline">\(p(x)\)</span>, <span class="math inline">\(p(y)\)</span> are the marginal probability distributions.</li>
</ul></li>
<li><strong>Application:</strong> Useful for capturing non-linear relationships between variables.</li>
</ul>
</section>
<section id="chi-squared-test" class="level4">
<h4 class="anchored" data-anchor-id="chi-squared-test">11.5.1.3. Chi-Squared Test</h4>
<p>The chi-squared test measures the association between categorical features and the target variable.</p>
<ul>
<li><strong>Formula:</strong> <span class="math display">\[
\chi^2 = \sum \frac{(O_i - E_i)^2}{E_i}
\]</span>
<ul>
<li>Here, <span class="math inline">\(O_i\)</span> is the observed frequency, and <span class="math inline">\(E_i\)</span> is the expected frequency.</li>
</ul></li>
<li><strong>Application:</strong> Useful for categorical data to test independence between features and the target variable.</li>
</ul>
</section>
</section>
<section id="wrapper-methods" class="level3">
<h3 class="anchored" data-anchor-id="wrapper-methods">11.5.2. Wrapper Methods</h3>
<p>Wrapper methods evaluate feature subsets by training and testing a specific model. These methods are more computationally intensive but can result in better performance.</p>
<section id="recursive-feature-elimination" class="level4">
<h4 class="anchored" data-anchor-id="recursive-feature-elimination">11.5.2.1. Recursive Feature Elimination</h4>
<p>Recursive Feature Elimination (RFE) recursively removes the least important features and builds the model on the remaining features.</p>
<ul>
<li><strong>Algorithm Steps:</strong>
<ol type="1">
<li>Train the model on the full feature set.</li>
<li>Rank features based on their importance.</li>
<li>Remove the least important feature.</li>
<li>Repeat until the desired number of features is reached.</li>
</ol></li>
<li><strong>Application:</strong> Useful for models where feature importance can be easily determined, such as linear regression and support vector machines.</li>
</ul>
</section>
<section id="forwardbackward-selection" class="level4">
<h4 class="anchored" data-anchor-id="forwardbackward-selection">11.5.2.2. Forward/Backward Selection</h4>
<p>Forward and backward selection are stepwise selection techniques to add or remove features.</p>
<ul>
<li><strong>Forward Selection:</strong>
<ol type="1">
<li>Start with no features.</li>
<li>Add the feature that improves the model performance the most.</li>
<li>Repeat until no significant improvement is achieved.</li>
</ol></li>
<li><strong>Backward Selection:</strong>
<ol type="1">
<li>Start with all features.</li>
<li>Remove the feature that decreases the model performance the least.</li>
<li>Repeat until no significant improvement is achieved.</li>
</ol></li>
<li><strong>Application:</strong> Useful for finding a balance between model performance and complexity.</li>
</ul>
</section>
</section>
<section id="embedded-methods" class="level3">
<h3 class="anchored" data-anchor-id="embedded-methods">11.5.3. Embedded Methods</h3>
<p>Embedded methods perform feature selection during the model training process. These methods are less computationally expensive than wrapper methods and can still capture interactions between features and the model.</p>
<section id="lasso-regularization" class="level4">
<h4 class="anchored" data-anchor-id="lasso-regularization">11.5.3.1. Lasso Regularization</h4>
<p>Lasso (Least Absolute Shrinkage and Selection Operator) regularization adds a penalty equal to the absolute value of the magnitude of coefficients to the loss function.</p>
<ul>
<li><strong>Formula:</strong> <span class="math display">\[
\text{Loss} = \sum_{i=1}^{n} (y_i - \hat{y}_i)^2 + \lambda \sum_{j=1}^{p} |w_j|
\]</span>
<ul>
<li>Here, <span class="math inline">\(\lambda\)</span> is the regularization parameter, and <span class="math inline">\(w_j\)</span> are the model coefficients.</li>
</ul></li>
<li><strong>Application:</strong> Useful for reducing the number of features by driving some coefficients to zero.</li>
</ul>
</section>
<section id="random-forest-feature-importance" class="level4">
<h4 class="anchored" data-anchor-id="random-forest-feature-importance">11.5.3.2. Random Forest Feature Importance</h4>
<p>Random forests provide feature importance scores based on the average decrease in impurity or the average increase in accuracy when the feature is used.</p>
<ul>
<li><strong>Algorithm Steps:</strong>
<ol type="1">
<li>Train a random forest model.</li>
<li>Calculate feature importance scores based on the reduction in Gini impurity or increase in accuracy.</li>
<li>Rank features based on their importance scores.</li>
</ol></li>
<li><strong>Application:</strong> Useful for identifying important features in datasets with complex, non-linear relationships.</li>
</ul>
<p>By understanding and applying these feature scaling, normalization, and selection techniques, you can enhance the performance of your machine learning models, improve their interpretability, and reduce overfitting.</p>
</section>
</section>
</section>
<section id="automated-feature-engineering" class="level2">
<h2 class="anchored" data-anchor-id="automated-feature-engineering">11.6. Automated Feature Engineering</h2>
<p>Automated feature engineering uses algorithms and tools to automatically create and select features from the raw data. This approach can save time and improve efficiency in the feature engineering process.</p>
<section id="featuretools-library" class="level3">
<h3 class="anchored" data-anchor-id="featuretools-library">11.6.1. Featuretools Library</h3>
<p>Featuretools is an open-source Python library for automated feature engineering. It simplifies the process of creating complex features from relational datasets.</p>
<ul>
<li><strong>Overview:</strong>
<ul>
<li><strong>Deep Feature Synthesis (DFS):</strong> Featuretools uses DFS to automatically generate features. DFS stacks multiple primitive operations (e.g., aggregations and transformations) to create new features.</li>
<li><strong>Entity Set:</strong> Featuretools organizes data into an entity set, which is a collection of tables (entities) and the relationships between them.</li>
</ul></li>
<li><strong>Key Components:</strong>
<ul>
<li><strong>Entities:</strong> Tables in the dataset (e.g., customers, transactions).</li>
<li><strong>Relationships:</strong> Connections between tables (e.g., a customer can have many transactions).</li>
<li><strong>Primitive Operations:</strong> Basic operations used to generate features, including:
<ul>
<li><strong>Aggregation Primitives:</strong> Operations that summarize data (e.g., sum, mean).</li>
<li><strong>Transformation Primitives:</strong> Operations that transform data (e.g., difference, division).</li>
</ul></li>
</ul></li>
<li><strong>Example Workflow:</strong>
<ol type="1">
<li><strong>Define an Entity Set:</strong> Create an entity set and add entities (tables) and relationships.</li>
<li><strong>Run Deep Feature Synthesis:</strong> Use DFS to generate new features based on the entity set.</li>
<li><strong>Select Features:</strong> Choose the most relevant features for the machine learning model.</li>
</ol></li>
</ul>
</section>
<section id="autofeat" class="level3">
<h3 class="anchored" data-anchor-id="autofeat">11.6.2. AutoFeat</h3>
<p>AutoFeat is a Python library designed to automate the process of feature engineering and selection. It focuses on creating polynomial features and interactions to enhance model performance.</p>
<ul>
<li><strong>Overview:</strong>
<ul>
<li><strong>Feature Generation:</strong> AutoFeat generates polynomial features and interactions based on the input features.</li>
<li><strong>Feature Selection:</strong> It automatically selects the most relevant features, reducing the dimensionality of the dataset and improving model performance.</li>
</ul></li>
<li><strong>Key Features:</strong>
<ul>
<li><strong>Polynomial Features:</strong> AutoFeat creates higher-order terms and interactions between features.</li>
<li><strong>Customizable:</strong> Users can specify the degree of polynomial features and the types of interactions to generate.</li>
<li><strong>Model Agnostic:</strong> AutoFeat works with any machine learning model, making it versatile for various tasks.</li>
</ul></li>
<li><strong>Example Workflow:</strong>
<ol type="1">
<li><strong>Initialize AutoFeat:</strong> Create an AutoFeat object and configure the desired feature generation settings.</li>
<li><strong>Fit and Transform:</strong> Fit the AutoFeat object to the data to generate and select new features.</li>
<li><strong>Integrate with Model:</strong> Use the transformed dataset with the selected features to train the machine learning model.</li>
</ol></li>
</ul>
</section>
<section id="tsfresh-for-time-series-feature-extraction" class="level3">
<h3 class="anchored" data-anchor-id="tsfresh-for-time-series-feature-extraction">11.6.3. tsfresh for Time Series Feature Extraction</h3>
<p>tsfresh is a Python library specifically designed for extracting relevant features from time series data. It automates the process of generating features that can be used for time series classification and regression tasks.</p>
<ul>
<li><strong>Overview:</strong>
<ul>
<li><strong>Feature Extraction:</strong> tsfresh extracts a large number of features from time series data, including statistical, time-based, and frequency-based features.</li>
<li><strong>Feature Selection:</strong> It includes methods for selecting the most relevant features based on their importance.</li>
</ul></li>
<li><strong>Key Components:</strong>
<ul>
<li><strong>Feature Calculators:</strong> Functions that compute various features from time series data (e.g., mean, variance, autocorrelation).</li>
<li><strong>Relevance Table:</strong> A table that lists the extracted features and their relevance scores, helping to identify the most important features.</li>
</ul></li>
<li><strong>Example Workflow:</strong>
<ol type="1">
<li><strong>Extract Features:</strong> Use tsfresh to extract features from time series data.</li>
<li><strong>Select Relevant Features:</strong> Filter the features based on their relevance scores.</li>
<li><strong>Integrate with Model:</strong> Use the selected features to train time series models.</li>
</ol></li>
</ul>
</section>
</section>
<section id="feature-learning" class="level2">
<h2 class="anchored" data-anchor-id="feature-learning">11.7. Feature Learning</h2>
<p>Feature learning involves using machine learning models to automatically discover and learn features from the raw data. This approach can capture complex patterns and representations that are difficult to manually engineer.</p>
<section id="autoencoders-for-feature-learning" class="level3">
<h3 class="anchored" data-anchor-id="autoencoders-for-feature-learning">11.7.1. Autoencoders for Feature Learning</h3>
<p>Autoencoders are a type of neural network used for unsupervised learning of features. They learn to compress data into a lower-dimensional representation and then reconstruct it back to the original input.</p>
<ul>
<li><strong>Architecture:</strong>
<ul>
<li><strong>Encoder:</strong> The part of the network that compresses the input data into a lower-dimensional representation (latent space).</li>
<li><strong>Decoder:</strong> The part of the network that reconstructs the input data from the lower-dimensional representation.</li>
</ul></li>
<li><strong>Training Process:</strong>
<ul>
<li><strong>Objective:</strong> Minimize the reconstruction error between the input and the reconstructed output.</li>
<li><strong>Loss Function:</strong> Typically, the mean squared error (MSE) is used to measure reconstruction error. <span class="math display">\[
\text{MSE} = \frac{1}{n} \sum_{i=1}^{n} (x_i - \hat{x}_i)^2
\]</span></li>
</ul></li>
<li><strong>Applications:</strong>
<ul>
<li><strong>Dimensionality Reduction:</strong> Autoencoders can be used to reduce the dimensionality of data while retaining important features.</li>
<li><strong>Anomaly Detection:</strong> By learning the normal patterns in data, autoencoders can identify anomalies based on high reconstruction error.</li>
</ul></li>
</ul>
</section>
<section id="restricted-boltzmann-machines" class="level3">
<h3 class="anchored" data-anchor-id="restricted-boltzmann-machines">11.7.2. Restricted Boltzmann Machines</h3>
<p>Restricted Boltzmann Machines (RBMs) are stochastic neural networks that can learn a probability distribution over the input data. They are used for feature learning and dimensionality reduction.</p>
<ul>
<li><strong>Architecture:</strong>
<ul>
<li><strong>Visible Layer:</strong> Represents the input data.</li>
<li><strong>Hidden Layer:</strong> Represents the learned features.</li>
</ul></li>
<li><strong>Training Process:</strong>
<ul>
<li><strong>Objective:</strong> Maximize the likelihood of the input data by learning the weights between the visible and hidden layers.</li>
<li><strong>Contrastive Divergence:</strong> An efficient algorithm used to approximate the gradient of the likelihood function.</li>
</ul></li>
<li><strong>Applications:</strong>
<ul>
<li><strong>Pre-training for Deep Networks:</strong> RBMs can be used to pre-train deep neural networks, improving their performance.</li>
<li><strong>Collaborative Filtering:</strong> RBMs are used in recommendation systems to learn user-item interactions.</li>
</ul></li>
</ul>
</section>
</section>
<section id="handling-missing-data-in-nhs-electronic-health-records" class="level2">
<h2 class="anchored" data-anchor-id="handling-missing-data-in-nhs-electronic-health-records">11.8. Handling Missing Data in NHS Electronic Health Records</h2>
<p>Handling missing data is crucial in NHS EHR systems, as incomplete records can lead to biased estimates and reduced model performance, potentially affecting patient care.</p>
<section id="imputation-techniques" class="level3">
<h3 class="anchored" data-anchor-id="imputation-techniques">11.8.1. Imputation Techniques</h3>
<p>Imputation involves filling in missing values with estimated values based on observed data. In NHS EHR systems, this is particularly important for ensuring comprehensive patient records.</p>
<ul>
<li><strong>Mean/Median Imputation:</strong>
<ul>
<li><strong>Mean Imputation:</strong> Replace missing values with the mean of observed values. <span class="math display">\[
x_{\text{imputed}} = \frac{\sum_{i=1}^{n} x_i}{n}
\]</span>
<ul>
<li><strong>Example:</strong> If a patient’s blood pressure readings are missing, replace with the mean blood pressure of all patients in the same age group.</li>
</ul></li>
<li><strong>Median Imputation:</strong> Replace missing values with the median of observed values. <span class="math display">\[
x_{\text{imputed}} = \text{median}(x)
\]</span>
<ul>
<li><strong>Example:</strong> For missing BMI values, use the median BMI of patients with similar characteristics (age, gender, health conditions).</li>
</ul></li>
<li><strong>Advantages:</strong>
<ul>
<li>Simple to implement.</li>
<li>Computationally efficient.</li>
</ul></li>
<li><strong>Disadvantages:</strong>
<ul>
<li>Can distort the distribution of the data.</li>
<li>Does not account for relationships between variables.</li>
</ul></li>
</ul></li>
<li><strong>Mode Imputation:</strong>
<ul>
<li>Replace missing values with the mode (most frequent value) of observed values. <span class="math display">\[
x_{\text{imputed}} = \text{mode}(x)
\]</span>
<ul>
<li><strong>Example:</strong> For missing categorical data like smoking status, impute with the most common status among similar patients.</li>
</ul></li>
<li><strong>Advantages:</strong>
<ul>
<li>Simple to implement.</li>
<li>Suitable for categorical data.</li>
</ul></li>
<li><strong>Disadvantages:</strong>
<ul>
<li>Can introduce bias if the mode is not representative.</li>
<li>Not suitable for continuous data.</li>
</ul></li>
</ul></li>
<li><strong>K-Nearest Neighbors (KNN) Imputation:</strong>
<ul>
<li>Replace missing values with the average of the nearest neighbors. <span class="math display">\[
x_{\text{imputed}} = \frac{\sum_{i=1}^{k} x_{\text{neighbor}_i}}{k}
\]</span>
<ul>
<li><strong>Example:</strong> For a missing cholesterol level, find k patients with similar health profiles and use the average of their cholesterol levels.</li>
</ul></li>
<li><strong>Choosing k:</strong>
<ul>
<li><strong>Cross-Validation:</strong> Use cross-validation to determine the optimal value of k. Split the data into training and validation sets, test different values of k, and select the one that minimizes the imputation error.</li>
<li><strong>Domain Knowledge:</strong> Use domain knowledge to choose k. For example, in a clinical setting, a smaller k might be chosen to ensure that the nearest neighbors are very similar to the patient in question.</li>
<li><strong>Rule of Thumb:</strong> A common rule of thumb is to use the square root of the number of observations (n). However, this should be adjusted based on the specific dataset and problem.</li>
</ul></li>
<li><strong>Advantages:</strong>
<ul>
<li>Accounts for relationships between variables.</li>
<li>Can handle both categorical and continuous data.</li>
</ul></li>
<li><strong>Disadvantages:</strong>
<ul>
<li>Computationally intensive.</li>
<li>Sensitive to the choice of k and distance metric.</li>
</ul></li>
</ul></li>
<li><strong>Multiple Imputation:</strong>
<ul>
<li><p>Generate multiple imputations for each missing value and average the results.</p></li>
<li><p><strong>Example:</strong> Create multiple complete datasets with different imputed values for missing lab results, analyze each, and combine the results for a more robust analysis.</p></li>
<li><p><strong>Advantages:</strong></p>
<ul>
<li>Accounts for the uncertainty of the imputed values.</li>
<li>Provides robust statistical analysis.</li>
</ul></li>
<li><p><strong>Disadvantages:</strong></p>
<ul>
<li>Computationally intensive.</li>
<li>Requires specialized software and expertise.</li>
</ul></li>
</ul></li>
<li><strong>Hot Deck Imputation:</strong>
<ul>
<li>A randomly chosen value from an individual in the sample who has similar values on other variables.
<ul>
<li><strong>Example:</strong> For missing data on patient medication adherence, select a similar patient (in terms of age, gender, and health conditions) and use their adherence data.</li>
</ul></li>
<li><strong>Advantages:</strong>
<ul>
<li>Preserves the distribution of the data.</li>
<li>Can handle both categorical and continuous data.</li>
</ul></li>
<li><strong>Disadvantages:</strong>
<ul>
<li>Can introduce bias if the chosen donor is not representative.</li>
<li>Requires a large dataset to find similar individuals.</li>
</ul></li>
</ul></li>
<li><strong>Cold Deck Imputation:</strong>
<ul>
<li>A systematically chosen value from an individual who has similar values on other variables.
<ul>
<li><strong>Example:</strong> If a patient’s follow-up visit data is missing, use the follow-up data from a similar patient in the same treatment group and condition.</li>
</ul></li>
<li><strong>Advantages:</strong>
<ul>
<li>Systematic and reproducible.</li>
<li>Preserves the distribution of the data.</li>
</ul></li>
<li><strong>Disadvantages:</strong>
<ul>
<li>Can introduce bias if the chosen donor is not representative.</li>
<li>Requires careful selection of similar individuals.</li>
</ul></li>
</ul></li>
<li><strong>Regression Imputation:</strong>
<ul>
<li>The predicted value obtained by regressing the missing variable on other variables.
<ul>
<li><strong>Example:</strong> Predict missing HbA1c levels in diabetes patients based on their age, weight, and previous HbA1c readings.</li>
</ul></li>
<li><strong>Advantages:</strong>
<ul>
<li>Accounts for relationships between variables.</li>
<li>Can handle both categorical and continuous data.</li>
</ul></li>
<li><strong>Disadvantages:</strong>
<ul>
<li>Can underestimate variability.</li>
<li>Assumes a linear relationship between variables.</li>
</ul></li>
</ul></li>
<li><strong>Stochastic Regression Imputation:</strong>
<ul>
<li>The predicted value from a regression plus a random residual.
<ul>
<li><strong>Example:</strong> Impute missing blood pressure readings by adding a random error term to the predicted values from a regression model based on other health indicators.</li>
</ul></li>
<li><strong>Advantages:</strong>
<ul>
<li>Preserves variability in the data.</li>
<li>Accounts for relationships between variables.</li>
</ul></li>
<li><strong>Disadvantages:</strong>
<ul>
<li>Computationally intensive.</li>
<li>Assumes a linear relationship between variables.</li>
</ul></li>
</ul></li>
</ul>
</section>
<section id="missing-value-indicators" class="level3">
<h3 class="anchored" data-anchor-id="missing-value-indicators">11.8.2. Missing Value Indicators</h3>
<p>Missing value indicators are binary flags indicating whether a value was originally missing, preserving information about data absence.</p>
<ul>
<li><strong>Indicator Variable:</strong>
<ul>
<li>Create a new binary feature for each original feature with missing values. <span class="math display">\[
I(x_i) = \begin{cases}
  1 &amp; \text{if } x_i \text{ is missing} \\
  0 &amp; \text{if } x_i \text{ is not missing}
\end{cases}
\]</span>
<ul>
<li><strong>Example:</strong> Create an indicator variable for missing HbA1c tests in diabetes patients, which might itself be informative about patient care patterns.</li>
</ul></li>
<li><strong>Advantages:</strong>
<ul>
<li>Simple to implement.</li>
<li>Preserves information about missingness.</li>
</ul></li>
<li><strong>Disadvantages:</strong>
<ul>
<li>Can increase the dimensionality of the dataset.</li>
<li>May not be informative if missingness is random.</li>
</ul></li>
</ul></li>
</ul>
</section>
<section id="missing-data-mechanisms" class="level3">
<h3 class="anchored" data-anchor-id="missing-data-mechanisms">11.8.3. Missing Data Mechanisms</h3>
<p>Understanding why data is missing is crucial for choosing appropriate imputation techniques:</p>
<ul>
<li><strong>Missing Completely at Random (MCAR):</strong>
<ul>
<li>Data is missing by chance, unrelated to any variables.</li>
<li><strong>Example:</strong> A blood test result is missing because the lab equipment malfunctioned randomly.</li>
<li><strong>Implication:</strong> Simple imputation methods can be effective.</li>
<li><strong>Recommended Techniques:</strong>
<ol type="1">
<li>Mean/Median Imputation: Suitable for MCAR as the missing data is unrelated to other variables.</li>
<li>Hot Deck Imputation: Can preserve the distribution of the data.</li>
<li>Multiple Imputation: While not necessary, it can still provide robust results.</li>
</ol></li>
</ul></li>
<li><strong>Missing at Random (MAR):</strong>
<ul>
<li>Missingness is related to other measured variables but not to the missing variable itself.</li>
<li><strong>Example:</strong> Older patients are less likely to have their BMI recorded, but the likelihood of missing BMI is not related to the actual BMI value.</li>
<li><strong>Implication:</strong> More sophisticated techniques are needed to account for the relationship between missingness and other variables.</li>
<li><strong>Recommended Techniques:</strong>
<ol type="1">
<li>Multiple Imputation by Chained Equations (MICE): Ideal for MAR as it can account for relationships between variables.</li>
<li>K-Nearest Neighbors (KNN) Imputation: Can capture complex relationships between variables.</li>
<li>Regression Imputation: Useful when there’s a clear relationship between the missing variable and other variables.</li>
</ol></li>
</ul></li>
<li><strong>Missing Not at Random (MNAR):</strong>
<ul>
<li>Missingness is related to the unobserved variable itself.</li>
<li><strong>Example:</strong> Patients with very high blood pressure are more likely to miss follow-up appointments where blood pressure would be recorded.</li>
<li><strong>Implication:</strong> Requires specialized techniques and careful consideration of potential biases.</li>
<li><strong>Recommended Techniques:</strong>
<ol type="1">
<li>Pattern Mixture Models: Can model the relationship between the missing data mechanism and the missing values.</li>
<li>Selection Models: Useful when the probability of missingness depends on the unobserved data.</li>
<li>Sensitivity Analysis: To assess how different assumptions about the missing data mechanism affect the results.</li>
</ol></li>
</ul></li>
</ul>
<p><strong>Additional Considerations:</strong></p>
<ul>
<li><p><strong>Hybrid Approaches:</strong> In real-world scenarios, data might exhibit a combination of MCAR, MAR, and MNAR patterns. In such cases, a combination of techniques might be necessary.</p></li>
<li><p><strong>Domain Knowledge:</strong> Incorporating domain expertise is crucial, especially for MNAR cases. Healthcare professionals’ insights can guide the choice of imputation method and help interpret results.</p></li>
<li><p><strong>Data Visualization:</strong> Before choosing an imputation method, visualizing the patterns of missingness can provide insights into the underlying mechanism.</p></li>
<li><p><strong>Multiple Imputation:</strong> While especially useful for MAR, multiple imputation can be a robust approach for all types of missing data, as it accounts for the uncertainty in the imputed values.</p></li>
<li><p><strong>Missingness Indicators:</strong> For all types of missing data, creating indicator variables for missingness can be informative, especially if the fact that a value is missing is itself meaningful (e.g., a patient refusing to answer a specific health question).</p></li>
</ul>
</section>
<section id="multiple-imputation-by-chained-equations-mice" class="level3">
<h3 class="anchored" data-anchor-id="multiple-imputation-by-chained-equations-mice">11.8.4. Multiple Imputation by Chained Equations (MICE)</h3>
<p>MICE is particularly useful for handling complex missing data patterns in EHR systems.</p>
<ul>
<li><strong>Process:</strong>
<ol type="1">
<li>Initialize missing values with simple imputation.</li>
<li>For each variable with missing data, regress it on other variables and update missing values.</li>
<li>Repeat iteratively to stabilize imputations.</li>
</ol></li>
<li><strong>Example in NHS EHR:</strong> Suppose you have missing values in variables like age, blood pressure, and medication dosage:
<ol type="1">
<li>Initialize missing values with mean imputation.</li>
<li>Regress age on blood pressure and medication dosage to update missing age values.</li>
<li>Regress blood pressure on age and medication dosage to update missing blood pressure values.</li>
<li>Regress medication dosage on age and blood pressure to update missing dosage values.</li>
<li>Repeat the process iteratively.</li>
</ol></li>
<li><strong>Advantages:</strong>
<ul>
<li>Accounts for uncertainty in imputed values.</li>
<li>Suitable for complex EHR datasets with multiple variables.</li>
<li>Provides multiple datasets with different imputed values for robust analysis.</li>
</ul></li>
<li><strong>Disadvantages:</strong>
<ul>
<li>Computationally intensive.</li>
<li>Requires specialized software and expertise.</li>
</ul></li>
<li><strong>Applications in NHS EHR:</strong>
<ul>
<li><strong>Clinical Research:</strong> Imputing missing lab results in a study on treatment effectiveness.</li>
<li><strong>Patient Risk Stratification:</strong> Handling incomplete patient histories to accurately assess risk factors.</li>
<li><strong>Resource Allocation:</strong> Addressing missing administrative data to predict hospital resource needs.</li>
</ul></li>
</ul>
</section>
</section>
</section>



</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>