<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.47">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Ravi Shankar">

<title>chapter12_dimensionality_reduction – Data Down To Earth</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../">
<link href="../../../logo.png" rel="icon" type="image/png">
<script src="../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../../styles.css">
<link rel="stylesheet" href="../../content.css">
<meta property="og:title" content="– Data Down To Earth">
<meta property="og:description" content="">
<meta property="og:site_name" content="Data Down To Earth">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="../../../index.html" class="navbar-brand navbar-brand-logo">
    <img src="../../../logo.png" alt="" class="navbar-logo">
    </a>
    <a class="navbar-brand" href="../../../index.html">
    <span class="navbar-title">Data Down To Earth</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-projects" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Projects</span>
    </a>
    <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="nav-menu-projects">    
        <li>
    <a class="dropdown-item" href="../../../projects/dsml_projects.html">
 <span class="dropdown-text">Data Science and ML Projects</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../projects/genai_projects.html">
 <span class="dropdown-text">Generative AI Projects</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-tutorials" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Tutorials</span>
    </a>
    <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="nav-menu-tutorials">    
        <li>
    <a class="dropdown-item" href="../../../topics/statistics.html">
 <span class="dropdown-text">Statistics &amp; Probability</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../topics/machine_learning.html">
 <span class="dropdown-text">Machine Learning</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../topics/de.html">
 <span class="dropdown-text">Data Engineering</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../topics/generative_ai.html">
 <span class="dropdown-text">Generative AI</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../topics/public_health.html">
 <span class="dropdown-text">Public Health</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../topics/product_sense.html">
 <span class="dropdown-text">Product Sense</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item">
    <a class="nav-link" href="../../../notes/notes.html"> 
<span class="menu-text">Notes &amp; Research</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/iamrsps"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/iamravishankar/"> <i class="bi bi-linkedin" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content column-page" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
</div>



<div class="quarto-title-meta column-page">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Ravi Shankar </p>
          </div>
  </div>
    
  
    
  </div>
  


</header>


<section id="chapter-12.-advanced-dimensionality-reduction-techniques" class="level1 text-content">
<h1>Chapter 12. Advanced Dimensionality Reduction Techniques</h1>
<p>Dimensionality reduction techniques are essential for preprocessing high-dimensional data. They help in reducing the number of features while retaining the most important information. This chapter delves into advanced aspects of Principal Component Analysis (PCA) and its variants.</p>
<section id="pca-in-depth" class="level2">
<h2 class="anchored" data-anchor-id="pca-in-depth">12.1. PCA In-Depth</h2>
<p>Principal Component Analysis (PCA) is a statistical technique used to simplify a dataset by reducing its dimensionality while preserving as much variance as possible.</p>
<section id="singular-value-decomposition-svd" class="level3">
<h3 class="anchored" data-anchor-id="singular-value-decomposition-svd">12.1.1. Singular Value Decomposition (SVD)</h3>
<p>Singular Value Decomposition (SVD) is a mathematical method used in PCA to decompose a matrix into three component matrices. SVD is the foundation of PCA and helps in understanding the structure of data.</p>
<ul>
<li><strong>Mathematical Formulation:</strong> <span class="math display">\[
\mathbf{X} = \mathbf{U} \mathbf{\Sigma} \mathbf{V}^T
\]</span>
<ul>
<li>Here, <span class="math inline">\(\mathbf{X}\)</span> is the original data matrix.</li>
<li><span class="math inline">\(\mathbf{U}\)</span> is the left singular matrix (orthonormal columns).</li>
<li><span class="math inline">\(\mathbf{\Sigma}\)</span> is the diagonal matrix of singular values.</li>
<li><span class="math inline">\(\mathbf{V}\)</span> is the right singular matrix (orthonormal columns).</li>
</ul></li>
<li><strong>Steps:</strong>
<ol type="1">
<li><strong>Center the Data:</strong> Subtract the mean of each feature to center the data around the origin.</li>
<li><strong>Compute Covariance Matrix:</strong> Calculate the covariance matrix of the centered data.</li>
<li><strong>Perform SVD:</strong> Decompose the covariance matrix using SVD to obtain eigenvalues and eigenvectors.</li>
<li><strong>Select Principal Components:</strong> Choose the top <span class="math inline">\(k\)</span> eigenvectors (principal components) corresponding to the largest eigenvalues.</li>
</ol></li>
<li><strong>Applications:</strong>
<ul>
<li><strong>Data Compression:</strong> Reduces the dimensionality of data, making it easier to store and process.</li>
<li><strong>Noise Reduction:</strong> Removes noise by discarding components with low variance.</li>
<li><strong>Visualization:</strong> Projects high-dimensional data onto lower dimensions for visualization.</li>
</ul></li>
</ul>
</section>
<section id="truncated-svd-lsa" class="level3">
<h3 class="anchored" data-anchor-id="truncated-svd-lsa">12.1.2. Truncated SVD (LSA)</h3>
<p>Truncated SVD, also known as Latent Semantic Analysis (LSA), is a variant of SVD particularly useful for text data. It approximates the original data matrix by considering only the top <span class="math inline">\(k\)</span> singular values and corresponding vectors.</p>
<ul>
<li><strong>Mathematical Formulation:</strong> <span class="math display">\[
\mathbf{X}_k = \mathbf{U}_k \mathbf{\Sigma}_k \mathbf{V}_k^T
\]</span>
<ul>
<li>Here, <span class="math inline">\(\mathbf{X}_k\)</span> is the low-rank approximation of the original data matrix <span class="math inline">\(\mathbf{X}\)</span>.</li>
<li><span class="math inline">\(\mathbf{U}_k\)</span>, <span class="math inline">\(\mathbf{\Sigma}_k\)</span>, and <span class="math inline">\(\mathbf{V}_k\)</span> are truncated versions of <span class="math inline">\(\mathbf{U}\)</span>, <span class="math inline">\(\mathbf{\Sigma}\)</span>, and <span class="math inline">\(\mathbf{V}\)</span>, respectively.</li>
</ul></li>
<li><strong>Steps:</strong>
<ol type="1">
<li><strong>Perform SVD:</strong> Decompose the original data matrix using SVD.</li>
<li><strong>Truncate Components:</strong> Retain only the top <span class="math inline">\(k\)</span> singular values and their corresponding vectors.</li>
<li><strong>Reconstruct Data:</strong> Approximate the original data matrix using the truncated components.</li>
</ol></li>
<li><strong>Applications:</strong>
<ul>
<li><strong>Text Mining:</strong> Identifies patterns in text data by reducing dimensionality.</li>
<li><strong>Document Clustering:</strong> Groups similar documents together based on underlying topics.</li>
<li><strong>Information Retrieval:</strong> Improves the efficiency of searching and retrieving relevant documents.</li>
</ul></li>
</ul>
</section>
<section id="randomized-pca" class="level3">
<h3 class="anchored" data-anchor-id="randomized-pca">12.1.3. Randomized PCA</h3>
<p>Randomized PCA is an efficient and scalable variant of PCA, which uses randomized algorithms to approximate the principal components. This method is particularly useful for large datasets.</p>
<ul>
<li><strong>Overview:</strong>
<ul>
<li>Randomized PCA approximates the SVD by projecting the data onto a lower-dimensional subspace, significantly reducing computational cost and memory usage.</li>
</ul></li>
<li><strong>Steps:</strong>
<ol type="1">
<li><strong>Generate Random Matrix:</strong> Create a random matrix <span class="math inline">\(\mathbf{\Omega}\)</span> with dimensions compatible with the input data matrix <span class="math inline">\(\mathbf{X}\)</span>.</li>
<li><strong>Project Data:</strong> Compute the projection <span class="math inline">\(\mathbf{Y} = \mathbf{X} \mathbf{\Omega}\)</span>.</li>
<li><strong>Compute SVD on Projection:</strong> Perform SVD on the projection matrix <span class="math inline">\(\mathbf{Y}\)</span>.</li>
<li><strong>Approximate Principal Components:</strong> Derive the approximate principal components from the SVD of <span class="math inline">\(\mathbf{Y}\)</span>.</li>
</ol></li>
<li><strong>Advantages:</strong>
<ul>
<li><strong>Efficiency:</strong> Faster and more memory-efficient than traditional PCA, making it suitable for large datasets.</li>
<li><strong>Scalability:</strong> Can handle high-dimensional data more effectively.</li>
</ul></li>
<li><strong>Applications:</strong>
<ul>
<li><p><strong>Large-Scale Machine Learning:</strong> Useful in scenarios where traditional PCA is computationally prohibitive.</p></li>
<li><p><strong>Real-Time Data Analysis:</strong> Enables quick approximations of principal components for streaming data.</p></li>
</ul></li>
</ul>
</section>
<section id="sparse-pca" class="level3">
<h3 class="anchored" data-anchor-id="sparse-pca">12.1.4. Sparse PCA</h3>
<p>Sparse PCA is a variant of PCA that aims to produce principal components with sparse (mostly zero) loadings. This sparsity can improve interpretability and is useful in situations where feature selection is important.</p>
<ul>
<li><strong>Mathematical Formulation:</strong> Sparse PCA introduces a sparsity constraint in the PCA optimization problem: <span class="math display">\[
\min_{\mathbf{W}, \mathbf{H}} \| \mathbf{X} - \mathbf{W} \mathbf{H}^T \|^2_F + \lambda \| \mathbf{W} \|_1
\]</span>
<ul>
<li>Here, <span class="math inline">\(\mathbf{W}\)</span> and <span class="math inline">\(\mathbf{H}\)</span> are the matrices of loadings and principal components, respectively.</li>
<li><span class="math inline">\(\lambda\)</span> is the regularization parameter controlling sparsity.</li>
<li><span class="math inline">\(\| \cdot \|_F\)</span> denotes the Frobenius norm.</li>
<li><span class="math inline">\(\| \cdot \|_1\)</span> denotes the <span class="math inline">\(\ell_1\)</span> norm to induce sparsity.</li>
</ul></li>
<li><strong>Steps:</strong>
<ol type="1">
<li><strong>Center the Data:</strong> Subtract the mean of each feature.</li>
<li><strong>Formulate Optimization Problem:</strong> Set up the PCA optimization problem with the sparsity constraint.</li>
<li><strong>Solve Optimization Problem:</strong> Use algorithms like iterative thresholding or coordinate descent to solve the optimization problem.</li>
<li><strong>Select Principal Components:</strong> Choose the components with the highest variance while ensuring sparsity.</li>
</ol></li>
<li><strong>Advantages:</strong>
<ul>
<li><strong>Interpretability:</strong> Produces more interpretable principal components by enforcing sparsity.</li>
<li><strong>Feature Selection:</strong> Helps in selecting the most relevant features, reducing model complexity.</li>
</ul></li>
<li><strong>Applications:</strong>
<ul>
<li><strong>Genomics and Bioinformatics:</strong> Identifies key genetic markers by selecting sparse components.</li>
<li><strong>Financial Data Analysis:</strong> Determines the most influential financial indicators.</li>
<li><strong>Any Domain Requiring Interpretability:</strong> Useful in fields where understanding the model is crucial.</li>
</ul></li>
</ul>
<p>By understanding these advanced PCA techniques, you can apply the appropriate method to reduce the dimensionality of your data effectively while preserving important information. These techniques can enhance your data preprocessing pipeline and improve the performance of downstream machine learning models.</p>
</section>
</section>
<section id="linear-discriminant-analysis-lda" class="level2">
<h2 class="anchored" data-anchor-id="linear-discriminant-analysis-lda">12.2. Linear Discriminant Analysis (LDA)</h2>
<p>Linear Discriminant Analysis (LDA) is a technique used for dimensionality reduction that also incorporates class labels. Unlike PCA, which is unsupervised, LDA is supervised and aims to maximize the separation between multiple classes.</p>
<section id="fishers-linear-discriminant" class="level3">
<h3 class="anchored" data-anchor-id="fishers-linear-discriminant">12.2.1. Fisher’s Linear Discriminant</h3>
<p>Fisher’s Linear Discriminant is the core concept behind LDA, focusing on finding a linear combination of features that separates two or more classes of objects.</p>
<ul>
<li><p><strong>Objective:</strong> Maximize the ratio of the between-class variance to the within-class variance, ensuring that the classes are as separable as possible.</p></li>
<li><p><strong>Mathematical Formulation:</strong></p>
<ul>
<li><strong>Within-Class Scatter Matrix (<span class="math inline">\(\mathbf{S_W}\)</span>):</strong> <span class="math display">\[
\mathbf{S_W} = \sum_{i=1}^{c} \sum_{x \in X_i} (x - \mu_i)(x - \mu_i)^T
\]</span>
<ul>
<li>Here, <span class="math inline">\(c\)</span> is the number of classes, <span class="math inline">\(X_i\)</span> is the set of samples in class <span class="math inline">\(i\)</span>, and <span class="math inline">\(\mu_i\)</span> is the mean vector of class <span class="math inline">\(i\)</span>.</li>
</ul></li>
<li><strong>Between-Class Scatter Matrix (<span class="math inline">\(\mathbf{S_B}\)</span>):</strong> <span class="math display">\[
\mathbf{S_B} = \sum_{i=1}^{c} N_i (\mu_i - \mu)(\mu_i - \mu)^T
\]</span>
<ul>
<li>Here, <span class="math inline">\(N_i\)</span> is the number of samples in class <span class="math inline">\(i\)</span>, and <span class="math inline">\(\mu\)</span> is the overall mean vector of the entire dataset.</li>
</ul></li>
<li><strong>Optimization Objective:</strong> <span class="math display">\[
\mathbf{w} = \arg \max_{\mathbf{w}} \frac{\mathbf{w}^T \mathbf{S_B} \mathbf{w}}{\mathbf{w}^T \mathbf{S_W} \mathbf{w}}
\]</span>
<ul>
<li>The solution to this optimization problem gives the linear discriminants.</li>
</ul></li>
</ul></li>
<li><p><strong>Steps:</strong></p>
<ol type="1">
<li><strong>Compute Mean Vectors:</strong> Calculate the mean vector for each class and the overall mean vector.</li>
<li><strong>Compute Scatter Matrices:</strong> Calculate the within-class scatter matrix (<span class="math inline">\(\mathbf{S_W}\)</span>) and the between-class scatter matrix (<span class="math inline">\(\mathbf{S_B}\)</span>).</li>
<li><strong>Solve the Generalized Eigenvalue Problem:</strong> Find the eigenvectors and eigenvalues of <span class="math inline">\(\mathbf{S_W}^{-1} \mathbf{S_B}\)</span>.</li>
<li><strong>Select Linear Discriminants:</strong> Choose the top eigenvectors corresponding to the largest eigenvalues.</li>
</ol></li>
<li><p><strong>Applications:</strong></p>
<ul>
<li><strong>Face Recognition:</strong> Distinguishing between different individuals based on facial features.</li>
<li><strong>Bioinformatics:</strong> Identifying genes that differentiate between different types of cancer.</li>
<li><strong>Marketing:</strong> Classifying customers into different segments based on purchasing behavior.</li>
</ul></li>
</ul>
<section id="detailed-steps-and-intuition" class="level4">
<h4 class="anchored" data-anchor-id="detailed-steps-and-intuition">Detailed Steps and Intuition:</h4>
<ol type="1">
<li><strong>Compute Mean Vectors:</strong>
<ul>
<li>Calculate the mean vector for each class <span class="math inline">\(i\)</span>: <span class="math display">\[
\mu_i = \frac{1}{N_i} \sum_{x \in X_i} x
\]</span></li>
<li>Calculate the overall mean vector <span class="math inline">\(\mu\)</span>: <span class="math display">\[
\mu = \frac{1}{N} \sum_{i=1}^{c} \sum_{x \in X_i} x
\]</span></li>
</ul></li>
<li><strong>Compute Scatter Matrices:</strong>
<ul>
<li>The within-class scatter matrix <span class="math inline">\(\mathbf{S_W}\)</span> sums the scatter within each class: <span class="math display">\[
\mathbf{S_W} = \sum_{i=1}^{c} \mathbf{S_{W,i}}, \quad \mathbf{S_{W,i}} = \sum_{x \in X_i} (x - \mu_i)(x - \mu_i)^T
\]</span></li>
<li>The between-class scatter matrix <span class="math inline">\(\mathbf{S_B}\)</span> measures the scatter between the class means: <span class="math display">\[
\mathbf{S_B} = \sum_{i=1}^{c} N_i (\mu_i - \mu)(\mu_i - \mu)^T
\]</span></li>
</ul></li>
<li><strong>Solve the Generalized Eigenvalue Problem:</strong>
<ul>
<li>Find the eigenvectors and eigenvalues of <span class="math inline">\(\mathbf{S_W}^{-1} \mathbf{S_B}\)</span>. The eigenvectors corresponding to the largest eigenvalues are the directions that maximize the ratio of between-class variance to within-class variance.</li>
</ul></li>
<li><strong>Select Linear Discriminants:</strong>
<ul>
<li>Select the top <span class="math inline">\(k\)</span> eigenvectors, where <span class="math inline">\(k\)</span> is the number of classes minus one. These eigenvectors form the transformation matrix <span class="math inline">\(\mathbf{W}\)</span>, which projects the data onto a lower-dimensional subspace.</li>
</ul></li>
</ol>
</section>
</section>
<section id="multi-class-lda" class="level3">
<h3 class="anchored" data-anchor-id="multi-class-lda">12.2.2. Multi-Class LDA</h3>
<p>Multi-Class LDA extends Fisher’s Linear Discriminant to handle multiple classes. It finds a subspace that maximizes the separation between all classes simultaneously.</p>
<ul>
<li><p><strong>Objective:</strong> Similar to Fisher’s Linear Discriminant, but adapted to handle more than two classes. The goal is to project the data onto a lower-dimensional space while maintaining maximum class separability.</p></li>
<li><p><strong>Mathematical Formulation:</strong></p>
<ul>
<li>The within-class and between-class scatter matrices are defined similarly as in Fisher’s Linear Discriminant, but the optimization involves more than two classes.</li>
<li><strong>Within-Class Scatter Matrix (<span class="math inline">\(\mathbf{S_W}\)</span>):</strong> <span class="math display">\[
\mathbf{S_W} = \sum_{i=1}^{c} \sum_{x \in X_i} (x - \mu_i)(x - \mu_i)^T
\]</span></li>
<li><strong>Between-Class Scatter Matrix (<span class="math inline">\(\mathbf{S_B}\)</span>):</strong> <span class="math display">\[
\mathbf{S_B} = \sum_{i=1}^{c} N_i (\mu_i - \mu)(\mu_i - \mu)^T
\]</span></li>
</ul></li>
<li><p><strong>Optimization Objective:</strong> <span class="math display">\[
\mathbf{W} = \arg \max_{\mathbf{W}} \frac{\det(\mathbf{W}^T \mathbf{S_B} \mathbf{W})}{\det(\mathbf{W}^T \mathbf{S_W} \mathbf{W})}
\]</span></p>
<ul>
<li>Here, <span class="math inline">\(\mathbf{W}\)</span> is the matrix of linear discriminants.</li>
</ul></li>
<li><p><strong>Steps:</strong></p>
<ol type="1">
<li><strong>Compute Mean Vectors:</strong> Calculate the mean vector for each class and the overall mean vector.</li>
<li><strong>Compute Scatter Matrices:</strong> Calculate the within-class scatter matrix (<span class="math inline">\(\mathbf{S_W}\)</span>) and the between-class scatter matrix (<span class="math inline">\(\mathbf{S_B}\)</span>).</li>
<li><strong>Solve the Generalized Eigenvalue Problem:</strong> Find the eigenvectors and eigenvalues of <span class="math inline">\(\mathbf{S_W}^{-1} \mathbf{S_B}\)</span>.</li>
<li><strong>Select Linear Discriminants:</strong> Choose the top <span class="math inline">\(k\)</span> eigenvectors corresponding to the largest eigenvalues, where <span class="math inline">\(k\)</span> is the number of classes minus one.</li>
</ol></li>
</ul>
<section id="detailed-steps-and-intuition-1" class="level4">
<h4 class="anchored" data-anchor-id="detailed-steps-and-intuition-1">Detailed Steps and Intuition:</h4>
<ol type="1">
<li><strong>Compute Mean Vectors:</strong>
<ul>
<li>Calculate the mean vector for each class <span class="math inline">\(i\)</span>: <span class="math display">\[
\mu_i = \frac{1}{N_i} \sum_{x \in X_i} x
\]</span></li>
<li>Calculate the overall mean vector <span class="math inline">\(\mu\)</span>: <span class="math display">\[
\mu = \frac{1}{N} \sum_{i=1}^{c} \sum_{x \in X_i} x
\]</span></li>
</ul></li>
<li><strong>Compute Scatter Matrices:</strong>
<ul>
<li>The within-class scatter matrix <span class="math inline">\(\mathbf{S_W}\)</span> sums the scatter within each class: <span class="math display">\[
\mathbf{S_W} = \sum_{i=1}^{c} \mathbf{S_{W,i}}, \quad \mathbf{S_{W,i}} = \sum_{x \in X_i} (x - \mu_i)(x - \mu_i)^T
\]</span></li>
<li>The between-class scatter matrix <span class="math inline">\(\mathbf{S_B}\)</span> measures the scatter between the class means: <span class="math display">\[
\mathbf{S_B} = \sum_{i=1}^{c} N_i (\mu_i - \mu)(\mu_i - \mu)^T
\]</span></li>
</ul></li>
<li><strong>Solve the Generalized Eigenvalue Problem:</strong>
<ul>
<li>Find the eigenvectors and eigenvalues of <span class="math inline">\(\mathbf{S_W}^{-1} \mathbf{S_B}\)</span>. The eigenvectors corresponding to the largest eigenvalues are the directions that maximize the ratio of between-class variance to within-class variance.</li>
</ul></li>
<li><strong>Select Linear Discriminants:</strong>
<ul>
<li>Select the top <span class="math inline">\(k\)</span> eigenvectors, where <span class="math inline">\(k\)</span> is the number of classes minus one. These eigenvectors form the transformation matrix <span class="math inline">\(\mathbf{W}\)</span>, which projects the data onto a lower-dimensional subspace.</li>
</ul></li>
</ol>
</section>
</section>
<section id="applications-and-advantages-of-lda" class="level3">
<h3 class="anchored" data-anchor-id="applications-and-advantages-of-lda">Applications and Advantages of LDA:</h3>
<ul>
<li><strong>Face Recognition:</strong> Distinguishing between different individuals based on facial features. LDA can project high-dimensional face data onto a lower-dimensional space, enhancing the classification accuracy of face recognition systems.</li>
<li><strong>Bioinformatics:</strong> Identifying genes that differentiate between different types of cancer. LDA helps in reducing the dimensionality of gene expression data while preserving class separability, improving the performance of classification algorithms.</li>
<li><strong>Marketing:</strong> Classifying customers into different segments based on purchasing behavior. By projecting customer data onto a lower-dimensional space, LDA can enhance the accuracy of customer segmentation models.</li>
</ul>
</section>
<section id="key-considerations" class="level3">
<h3 class="anchored" data-anchor-id="key-considerations">Key Considerations:</h3>
<ul>
<li><strong>Assumptions:</strong> LDA assumes that the data follows a Gaussian distribution and that each class has the same covariance matrix. If these assumptions do not hold, the performance of LDA may be compromised.</li>
<li><strong>Dimensionality Reduction:</strong> The number of linear discriminants that can be extracted is limited to the number of classes minus one. For datasets with a large number of classes, LDA may not significantly reduce dimensionality.</li>
<li><strong>Computational Complexity:</strong> LDA involves solving a generalized eigenvalue problem, which can be computationally intensive for large datasets.</li>
</ul>
<p>By understanding and applying Linear Discriminant Analysis (LDA) and its variants, you can enhance the classification performance of your machine learning models by projecting data into a lower-dimensional space that maximizes class separability.</p>
</section>
</section>
<section id="factor-analysis" class="level2">
<h2 class="anchored" data-anchor-id="factor-analysis">12.3. Factor Analysis</h2>
<p>Factor Analysis (FA) is a statistical technique used to identify underlying relationships between variables in a dataset. It assumes that observed variables are influenced by a smaller number of unobserved variables called factors. FA is widely used in psychology, social sciences, and market research.</p>
<section id="exploratory-factor-analysis-efa" class="level3">
<h3 class="anchored" data-anchor-id="exploratory-factor-analysis-efa">12.3.1. Exploratory Factor Analysis (EFA)</h3>
<p>Exploratory Factor Analysis (EFA) is used to discover the underlying structure of a relatively large set of variables without imposing any preconceived structure on the outcome. It is often used when researchers do not have a specific hypothesis about the factors.</p>
<ul>
<li><p><strong>Objective:</strong> Identify the underlying relationships between variables by grouping them into factors. EFA helps in understanding the data structure and reducing dimensionality.</p></li>
<li><p><strong>Mathematical Formulation:</strong></p>
<ul>
<li><strong>Model:</strong> <span class="math display">\[
\mathbf{X} = \mathbf{L} \mathbf{F} + \mathbf{E}
\]</span>
<ul>
<li>Here, <span class="math inline">\(\mathbf{X}\)</span> is the matrix of observed variables, <span class="math inline">\(\mathbf{L}\)</span> is the loading matrix, <span class="math inline">\(\mathbf{F}\)</span> is the matrix of common factors, and <span class="math inline">\(\mathbf{E}\)</span> is the matrix of unique factors (errors).</li>
</ul></li>
<li><strong>Factor Loadings:</strong> The coefficients in the loading matrix <span class="math inline">\(\mathbf{L}\)</span> represent the relationship between observed variables and common factors.</li>
<li><strong>Communalities:</strong> The proportion of variance in each observed variable that is accounted for by the common factors.</li>
</ul></li>
<li><p><strong>Steps:</strong></p>
<ol type="1">
<li><strong>Correlation Matrix:</strong> Calculate the correlation matrix of the observed variables. This matrix forms the basis for identifying relationships between variables.</li>
<li><strong>Extract Initial Factors:</strong> Use methods such as Principal Axis Factoring or Maximum Likelihood to extract initial factors. These methods estimate the initial factor loadings and communalities.
<ul>
<li><strong>Principal Axis Factoring:</strong> Focuses on the common variance and uses an iterative process to extract factors.</li>
<li><strong>Maximum Likelihood:</strong> Assumes a multivariate normal distribution and estimates factors that maximize the likelihood of the observed data.</li>
</ul></li>
<li><strong>Rotate Factors:</strong> Rotate the factors to achieve a simpler and more interpretable structure. Common rotation methods include:
<ul>
<li><strong>Varimax Rotation:</strong> An orthogonal rotation method that maximizes the variance of squared loadings of a factor across variables, making the structure easier to interpret.</li>
<li><strong>Promax Rotation:</strong> An oblique rotation method that allows factors to be correlated, providing a more realistic representation of the data.</li>
</ul></li>
<li><strong>Determine Number of Factors:</strong> Use criteria like the Kaiser criterion (eigenvalues greater than 1) or the Scree plot to decide the number of factors to retain.
<ul>
<li><strong>Kaiser Criterion:</strong> Retain factors with eigenvalues greater than 1.</li>
<li><strong>Scree Plot:</strong> A plot of the eigenvalues in descending order. The point where the slope of the curve levels off indicates the number of factors to retain.</li>
</ul></li>
<li><strong>Interpret Factors:</strong> Examine the factor loadings to interpret the meaning of each factor. Factor loadings close to 1 or -1 indicate a strong relationship between the variable and the factor.</li>
</ol></li>
<li><p><strong>Applications:</strong></p>
<ul>
<li><strong>Psychometrics:</strong> Understanding underlying psychological traits from survey responses. For example, EFA can reveal latent constructs like extraversion and agreeableness from personality test data.</li>
<li><strong>Market Research:</strong> Identifying latent factors that influence consumer behavior. EFA can uncover underlying dimensions such as brand loyalty, price sensitivity, and product quality perception.</li>
<li><strong>Social Sciences:</strong> Uncovering underlying dimensions in social attitudes and behaviors. EFA helps identify common themes in survey data on social issues, such as political ideology or environmental concern.</li>
</ul></li>
</ul>
</section>
<section id="confirmatory-factor-analysis-cfa" class="level3">
<h3 class="anchored" data-anchor-id="confirmatory-factor-analysis-cfa">12.3.2. Confirmatory Factor Analysis (CFA)</h3>
<p>Confirmatory Factor Analysis (CFA) is used to test whether a hypothesized factor structure fits the observed data. Unlike EFA, CFA is theory-driven and requires the researcher to specify the number of factors and the pattern of loadings a priori.</p>
<ul>
<li><p><strong>Objective:</strong> Confirm or reject predefined hypotheses about the factor structure. CFA evaluates how well the hypothesized model fits the actual data.</p></li>
<li><p><strong>Mathematical Formulation:</strong></p>
<ul>
<li><strong>Model:</strong> <span class="math display">\[
\mathbf{X} = \mathbf{L} \mathbf{F} + \mathbf{E}
\]</span>
<ul>
<li>The model structure is predefined, specifying which observed variables load onto which factors.</li>
</ul></li>
<li><strong>Goodness-of-Fit Indices:</strong> Various indices are used to assess model fit, including:
<ul>
<li><strong>Chi-Square Test:</strong> Tests the null hypothesis that the model fits the data perfectly. A non-significant Chi-Square indicates a good fit.</li>
<li><strong>Root Mean Square Error of Approximation (RMSEA):</strong> Measures the discrepancy per degree of freedom. RMSEA values less than 0.05 indicate a good fit.</li>
<li><strong>Comparative Fit Index (CFI):</strong> Compares the fit of the specified model to a baseline model. CFI values greater than 0.95 indicate a good fit.</li>
<li><strong>Tucker-Lewis Index (TLI):</strong> Also known as the Non-Normed Fit Index (NNFI). TLI values greater than 0.95 indicate a good fit.</li>
</ul></li>
</ul></li>
<li><p><strong>Steps:</strong></p>
<ol type="1">
<li><strong>Specify Model:</strong> Define the hypothesized factor structure, including the number of factors and the pattern of factor loadings. This step involves specifying which variables are associated with which factors.</li>
<li><strong>Estimate Parameters:</strong> Use methods like Maximum Likelihood Estimation (MLE) to estimate the parameters of the model, including factor loadings, variances, and covariances.</li>
<li><strong>Assess Model Fit:</strong> Evaluate the fit of the model using goodness-of-fit indices. Compare the observed covariance matrix with the covariance matrix implied by the model.</li>
<li><strong>Modify Model:</strong> If the initial model does not fit well, modify the model by adding or removing paths based on modification indices. Modification indices suggest changes that could improve the model fit.</li>
<li><strong>Interpret Factors:</strong> Interpret the factor loadings to understand the relationship between observed variables and factors. High factor loadings indicate strong relationships.</li>
</ol></li>
<li><p><strong>Applications:</strong></p>
<ul>
<li><strong>Psychometrics:</strong> Testing theoretical models of psychological constructs (e.g., intelligence, personality). CFA can validate whether a set of test items measures specific psychological traits as hypothesized.</li>
<li><strong>Educational Assessment:</strong> Validating the structure of educational assessments and tests. CFA ensures that test items align with the intended constructs, such as different mathematical skills.</li>
<li><strong>Social Sciences:</strong> Confirming theoretical models of social behavior and attitudes. For example, CFA can test whether survey items designed to measure political ideology accurately reflect the underlying dimensions.</li>
</ul></li>
</ul>
</section>
<section id="comparison-between-efa-and-cfa" class="level3">
<h3 class="anchored" data-anchor-id="comparison-between-efa-and-cfa">Comparison between EFA and CFA</h3>
<ul>
<li><strong>Purpose:</strong>
<ul>
<li><strong>EFA:</strong> Exploratory, used to identify the underlying factor structure without prior hypotheses.</li>
<li><strong>CFA:</strong> Confirmatory, used to test specific hypotheses about the factor structure.</li>
</ul></li>
<li><strong>Model Specification:</strong>
<ul>
<li><strong>EFA:</strong> The factor structure is determined from the data.</li>
<li><strong>CFA:</strong> The factor structure is specified a priori based on theory.</li>
</ul></li>
<li><strong>Rotation:</strong>
<ul>
<li><strong>EFA:</strong> Factor rotation (orthogonal or oblique) is typically performed to achieve a simpler structure.</li>
<li><strong>CFA:</strong> No rotation is needed as the model structure is predefined.</li>
</ul></li>
<li><strong>Fit Assessment:</strong>
<ul>
<li><strong>EFA:</strong> Emphasis on the interpretability of factor loadings.</li>
<li><strong>CFA:</strong> Emphasis on goodness-of-fit indices to evaluate how well the model fits the data.</li>
</ul></li>
</ul>
<p>By understanding and applying Factor Analysis (FA), including both Exploratory Factor Analysis (EFA) and Confirmatory Factor Analysis (CFA), you can uncover and validate the underlying structures in your data. These techniques are powerful tools for reducing dimensionality, understanding data relationships, and testing theoretical models.</p>
</section>
</section>
<section id="independent-component-analysis-ica" class="level2">
<h2 class="anchored" data-anchor-id="independent-component-analysis-ica">12.4. Independent Component Analysis (ICA)</h2>
<p>Independent Component Analysis (ICA) is a computational method for separating a multivariate signal into additive, independent components. ICA is used extensively in applications where the source signals are mixed together, and the goal is to recover the original signals.</p>
<section id="overview-and-objective" class="level3">
<h3 class="anchored" data-anchor-id="overview-and-objective">12.4.1. Overview and Objective</h3>
<ul>
<li><p><strong>Objective:</strong> The primary goal of ICA is to decompose a multivariate signal into independent non-Gaussian signals. ICA assumes that the observed data are linear mixtures of these independent components.</p></li>
<li><p><strong>Mathematical Formulation:</strong> <span class="math display">\[
\mathbf{X} = \mathbf{A} \mathbf{S}
\]</span></p>
<ul>
<li>Here, <span class="math inline">\(\mathbf{X}\)</span> is the observed data matrix, <span class="math inline">\(\mathbf{A}\)</span> is the mixing matrix, and <span class="math inline">\(\mathbf{S}\)</span> is the matrix of independent components.</li>
<li>The task is to find the unmixing matrix <span class="math inline">\(\mathbf{W}\)</span> such that: <span class="math display">\[
\mathbf{S} = \mathbf{W} \mathbf{X}
\]</span></li>
</ul></li>
</ul>
</section>
<section id="assumptions-of-ica" class="level3">
<h3 class="anchored" data-anchor-id="assumptions-of-ica">12.4.2. Assumptions of ICA</h3>
<ul>
<li><strong>Independence:</strong> The source signals are statistically independent.</li>
<li><strong>Non-Gaussianity:</strong> At most one of the source signals can be Gaussian; the rest must be non-Gaussian.</li>
<li><strong>Linearity:</strong> The observed signals are linear mixtures of the source signals.</li>
</ul>
</section>
<section id="algorithms-for-ica" class="level3">
<h3 class="anchored" data-anchor-id="algorithms-for-ica">12.4.3. Algorithms for ICA</h3>
<section id="fastica" class="level4">
<h4 class="anchored" data-anchor-id="fastica">FastICA</h4>
<p>FastICA is a popular algorithm for ICA that maximizes non-Gaussianity using a fixed-point iteration scheme.</p>
<ul>
<li><strong>Steps:</strong>
<ol type="1">
<li><strong>Center and Whiten Data:</strong> Remove the mean and normalize the variance of the data to produce zero-mean, unit-variance data.</li>
<li><strong>Initialize Weight Vector:</strong> Start with a random weight vector.</li>
<li><strong>Iterate to Maximize Non-Gaussianity:</strong>
<ul>
<li>Use an approximation of negentropy, such as the kurtosis or another suitable function, to measure non-Gaussianity.</li>
<li>Update the weight vector using a fixed-point iteration: <span class="math display">\[
\mathbf{w}_{\text{new}} = \mathbb{E}\left[\mathbf{X} g(\mathbf{w}^T \mathbf{X})\right] - \mathbb{E}\left[g'(\mathbf{w}^T \mathbf{X})\right] \mathbf{w}
\]</span> where ( g ) is a non-quadratic function and ( g’ ) is its derivative.</li>
<li>Normalize the weight vector: <span class="math display">\[
\mathbf{w} \leftarrow \frac{\mathbf{w}_{\text{new}}}{\|\mathbf{w}_{\text{new}}\|}
\]</span></li>
</ul></li>
<li><strong>Decorrelate Components:</strong> Ensure that the estimated components are uncorrelated by orthogonalizing the weight vectors.</li>
<li><strong>Check for Convergence:</strong> Repeat the iteration until convergence.</li>
</ol></li>
</ul>
</section>
<section id="infomax-ica" class="level4">
<h4 class="anchored" data-anchor-id="infomax-ica">Infomax ICA</h4>
<p>Infomax ICA is an algorithm based on information maximization, often used in neural network contexts.</p>
<ul>
<li><strong>Steps:</strong>
<ol type="1">
<li><strong>Initialize Weights:</strong> Start with small random weights.</li>
<li><strong>Update Weights:</strong> Use a gradient-based approach to maximize the mutual information between the observed and estimated signals.</li>
<li><strong>Convergence:</strong> Stop when the change in weights is below a threshold.</li>
</ol></li>
</ul>
</section>
</section>
<section id="applications-of-ica" class="level3">
<h3 class="anchored" data-anchor-id="applications-of-ica">12.4.4. Applications of ICA</h3>
<ul>
<li><strong>Signal Processing:</strong> Separation of mixed audio signals, such as separating individual speakers from a recording (the “cocktail party problem”).</li>
<li><strong>Neuroscience:</strong> Analyzing EEG and fMRI data to identify independent brain activity patterns.</li>
<li><strong>Financial Data:</strong> Identifying independent sources of risk and return in financial markets.</li>
<li><strong>Image Processing:</strong> Removing noise or separating superimposed images.</li>
</ul>
</section>
<section id="ica-example-cocktail-party-problem" class="level3">
<h3 class="anchored" data-anchor-id="ica-example-cocktail-party-problem">ICA Example: Cocktail Party Problem</h3>
<ul>
<li><strong>Problem Statement:</strong> Given a set of audio recordings from multiple microphones placed at different locations in a room, each recording captures a mix of several people’s voices. The goal is to separate the voices from these mixed recordings.</li>
<li><strong>Approach:</strong>
<ol type="1">
<li><strong>Record Audio:</strong> Collect audio data from multiple microphones.</li>
<li><strong>Preprocess Data:</strong> Center and whiten the audio signals.</li>
<li><strong>Apply FastICA:</strong> Use the FastICA algorithm to estimate the independent source signals.</li>
<li><strong>Evaluate Results:</strong> Listen to the separated audio tracks to ensure that the individual voices are effectively isolated.</li>
</ol></li>
</ul>
</section>
</section>
<section id="non-negative-matrix-factorization-nmf" class="level2">
<h2 class="anchored" data-anchor-id="non-negative-matrix-factorization-nmf">12.5. Non-negative Matrix Factorization (NMF)</h2>
<p>Non-negative Matrix Factorization (NMF) is a group of algorithms in multivariate analysis and linear algebra where a matrix <span class="math inline">\(\mathbf{V}\)</span> is factorized into (usually) two matrices <span class="math inline">\(\mathbf{W}\)</span> and <span class="math inline">\(\mathbf{H}\)</span>, with the property that all three matrices have no negative elements. NMF is particularly useful for parts-based, linear representations.</p>
<section id="overview-and-objective-1" class="level3">
<h3 class="anchored" data-anchor-id="overview-and-objective-1">12.5.1. Overview and Objective</h3>
<ul>
<li><strong>Objective:</strong> Decompose a non-negative data matrix into two non-negative factor matrices, capturing the underlying structure in the data.</li>
<li><strong>Mathematical Formulation:</strong> <span class="math display">\[
\mathbf{V} \approx \mathbf{W} \mathbf{H}
\]</span>
<ul>
<li>Here, <span class="math inline">\(\mathbf{V}\)</span> is the original non-negative data matrix, <span class="math inline">\(\mathbf{W}\)</span> is the basis matrix, and <span class="math inline">\(\mathbf{H}\)</span> is the coefficient matrix.</li>
</ul></li>
</ul>
</section>
<section id="algorithms-for-nmf" class="level3">
<h3 class="anchored" data-anchor-id="algorithms-for-nmf">12.5.2. Algorithms for NMF</h3>
<section id="multiplicative-update-rules" class="level4">
<h4 class="anchored" data-anchor-id="multiplicative-update-rules">Multiplicative Update Rules</h4>
<p>An iterative method where the update rules ensure non-negativity.</p>
<ul>
<li><strong>Steps:</strong>
<ol type="1">
<li><strong>Initialize <span class="math inline">\(\mathbf{W}\)</span> and <span class="math inline">\(\mathbf{H}\)</span></strong> with non-negative values.</li>
<li><strong>Update <span class="math inline">\(\mathbf{H}\)</span>:</strong> <span class="math display">\[
H_{ij} \leftarrow H_{ij} \frac{(W^T V)_{ij}}{(W^T WH)_{ij}}
\]</span></li>
<li><strong>Update <span class="math inline">\(\mathbf{W}\)</span>:</strong> <span class="math display">\[
W_{ij} \leftarrow W_{ij} \frac{(VH^T)_{ij}}{(WHH^T)_{ij}}
\]</span></li>
<li><strong>Repeat</strong> until convergence.</li>
</ol></li>
</ul>
</section>
<section id="alternating-least-squares-als" class="level4">
<h4 class="anchored" data-anchor-id="alternating-least-squares-als">Alternating Least Squares (ALS)</h4>
<p>Solves for <span class="math inline">\(\mathbf{W}\)</span> and <span class="math inline">\(\mathbf{H}\)</span> alternately by fixing one matrix and solving for the other.</p>
<ul>
<li><strong>Steps:</strong>
<ol type="1">
<li><strong>Initialize <span class="math inline">\(\mathbf{W}\)</span> and <span class="math inline">\(\mathbf{H}\)</span></strong> with non-negative values.</li>
<li><strong>Fix <span class="math inline">\(\mathbf{W}\)</span></strong> and solve for <span class="math inline">\(\mathbf{H}\)</span> using non-negative least squares.</li>
<li><strong>Fix <span class="math inline">\(\mathbf{H}\)</span></strong> and solve for <span class="math inline">\(\mathbf{W}\)</span> using non-negative least squares.</li>
<li><strong>Repeat</strong> until convergence.</li>
</ol></li>
</ul>
</section>
</section>
<section id="variants-of-nmf" class="level3">
<h3 class="anchored" data-anchor-id="variants-of-nmf">12.5.3. Variants of NMF</h3>
<section id="sparse-nmf" class="level4">
<h4 class="anchored" data-anchor-id="sparse-nmf">Sparse NMF</h4>
<p>Incorporates sparsity constraints to achieve a sparse representation.</p>
<ul>
<li><strong>Objective Function:</strong> <span class="math display">\[
\min_{\mathbf{W}, \mathbf{H}} \| \mathbf{V} - \mathbf{W} \mathbf{H} \|_F^2 + \lambda (\| \mathbf{W} \|_1 + \| \mathbf{H} \|_1)
\]</span></li>
<li><strong>Applications:</strong> Image processing, where sparse representations can enhance interpretability.</li>
</ul>
</section>
<section id="projective-nmf" class="level4">
<h4 class="anchored" data-anchor-id="projective-nmf">Projective NMF</h4>
<p>Ensures that the basis vectors form a convex hull around the data points.</p>
<ul>
<li><strong>Objective Function:</strong> <span class="math display">\[
\min_{\mathbf{W}} \| \mathbf{V} - \mathbf{W} \mathbf{W}^T \mathbf{V} \|_F^2
\]</span></li>
<li><strong>Applications:</strong> Data clustering and representation learning.</li>
</ul>
</section>
</section>
<section id="applications-of-nmf" class="level3">
<h3 class="anchored" data-anchor-id="applications-of-nmf">12.5.4. Applications of NMF</h3>
<section id="document-clustering" class="level4">
<h4 class="anchored" data-anchor-id="document-clustering">Document Clustering</h4>
<p>Grouping documents into topics based on term frequency.</p>
<ul>
<li><strong>Example:</strong> Decomposing a term-document matrix into topic vectors and document-topic distributions.
<ul>
<li><strong>Steps:</strong>
<ol type="1">
<li><strong>Create Term-Document Matrix:</strong> Represent the corpus as a matrix where rows correspond to terms and columns correspond to documents.</li>
<li><strong>Apply NMF:</strong> Factorize the term-document matrix into a basis matrix (topics) and a coefficient matrix (document-topic distribution).</li>
<li><strong>Interpret Topics:</strong> Analyze the basis matrix to understand the topics and their constituent terms.</li>
</ol></li>
</ul></li>
</ul>
</section>
<section id="image-processing" class="level4">
<h4 class="anchored" data-anchor-id="image-processing">Image Processing</h4>
<p>Parts-based representation of images.</p>
<ul>
<li><strong>Example:</strong> Decomposing images into a set of basis images and their coefficients.
<ul>
<li><strong>Steps:</strong>
<ol type="1">
<li><strong>Create Image Matrix:</strong> Represent the images as a matrix where rows correspond to pixels and columns correspond to images.</li>
<li><strong>Apply NMF:</strong> Factorize the image matrix into a basis matrix (parts) and a coefficient matrix (weights).</li>
<li><strong>Reconstruct Images:</strong> Use the basis and coefficient matrices to reconstruct the original images, emphasizing parts-based representation.</li>
</ol></li>
</ul></li>
</ul>
</section>
<section id="recommender-systems" class="level4">
<h4 class="anchored" data-anchor-id="recommender-systems">Recommender Systems</h4>
<p>Matrix completion for predicting missing entries in user-item interaction matrices.</p>
<ul>
<li><strong>Example:</strong> Predicting user preferences for items based on a partially observed interaction matrix.
<ul>
<li><strong>Steps:</strong>
<ol type="1">
<li><strong>Create User-Item Matrix:</strong> Represent the user-item interactions as a matrix where rows correspond to users and columns correspond to items.</li>
<li><strong>Apply NMF:</strong> Factorize the user-item matrix into a basis matrix (latent features of items) and a coefficient matrix (latent features of users).</li>
<li><strong>Predict Preferences:</strong> Use the factorized matrices to predict missing entries in the user-item matrix.</li>
</ol></li>
</ul></li>
</ul>
<p>By understanding and applying ICA and NMF, you can effectively separate mixed signals into their independent components and decompose data into interpretable non-negative factors, respectively. These techniques are powerful tools for advanced dimensionality reduction and data analysis.</p>
</section>
</section>
</section>
<section id="multidimensional-scaling-mds" class="level2">
<h2 class="anchored" data-anchor-id="multidimensional-scaling-mds">12.6. Multidimensional Scaling (MDS)</h2>
<p>Multidimensional Scaling (MDS) is a set of statistical techniques used for analyzing similarity or dissimilarity data. MDS aims to place each object in an N-dimensional space such that the between-object distances are preserved as well as possible. MDS is widely used in psychology, market research, and other fields where perceptual mapping is useful.</p>
<section id="classical-mds" class="level3">
<h3 class="anchored" data-anchor-id="classical-mds">12.6.1. Classical MDS</h3>
<p>Classical MDS, also known as Principal Coordinates Analysis (PCoA), is the simplest form of MDS. It starts with a matrix of distances (dissimilarities) between pairs of items and finds a set of points in a low-dimensional space whose inter-point distances approximate the original distances.</p>
<ul>
<li><p><strong>Objective:</strong> Reduce the dimensionality of the data while preserving the pairwise distances as much as possible.</p></li>
<li><p><strong>Mathematical Formulation:</strong></p>
<ol type="1">
<li><strong>Input:</strong> A distance matrix <span class="math inline">\(\mathbf{D}\)</span>, where <span class="math inline">\(d_{ij}\)</span> represents the dissimilarity between items <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span>.</li>
<li><strong>Centering Matrix <span class="math inline">\(\mathbf{J}\)</span>:</strong> <span class="math display">\[
\mathbf{J} = \mathbf{I} - \frac{1}{n} \mathbf{1}\mathbf{1}^T
\]</span>
<ul>
<li>Here, <span class="math inline">\(\mathbf{I}\)</span> is the identity matrix and <span class="math inline">\(\mathbf{1}\)</span> is a vector of ones.</li>
</ul></li>
<li><strong>Double-Centered Distance Matrix <span class="math inline">\(\mathbf{B}\)</span>:</strong> <span class="math display">\[
\mathbf{B} = -\frac{1}{2} \mathbf{J} \mathbf{D}^2 \mathbf{J}
\]</span>
<ul>
<li>Here, <span class="math inline">\(\mathbf{D}^2\)</span> is the element-wise square of the distance matrix.</li>
</ul></li>
<li><strong>Eigen Decomposition:</strong> Perform eigen decomposition of <span class="math inline">\(\mathbf{B}\)</span> to obtain the eigenvalues and eigenvectors. <span class="math display">\[
\mathbf{B} = \mathbf{V} \mathbf{\Lambda} \mathbf{V}^T
\]</span>
<ul>
<li>Here, <span class="math inline">\(\mathbf{\Lambda}\)</span> is the diagonal matrix of eigenvalues and <span class="math inline">\(\mathbf{V}\)</span> is the matrix of eigenvectors.</li>
</ul></li>
<li><strong>Coordinates:</strong> The coordinates in the reduced space are given by: <span class="math display">\[
\mathbf{X} = \mathbf{V} \mathbf{\Lambda}^{1/2}
\]</span>
<ul>
<li>Select the top <span class="math inline">\(k\)</span> eigenvalues and corresponding eigenvectors to form the <span class="math inline">\(k\)</span>-dimensional embedding.</li>
</ul></li>
</ol></li>
<li><p><strong>Steps:</strong></p>
<ol type="1">
<li><strong>Construct Distance Matrix:</strong> Compute or use a given distance matrix <span class="math inline">\(\mathbf{D}\)</span>.</li>
<li><strong>Double Center the Distance Matrix:</strong> Use the centering matrix <span class="math inline">\(\mathbf{J}\)</span> to obtain <span class="math inline">\(\mathbf{B}\)</span>.</li>
<li><strong>Perform Eigen Decomposition:</strong> Obtain the eigenvalues and eigenvectors of <span class="math inline">\(\mathbf{B}\)</span>.</li>
<li><strong>Compute Coordinates:</strong> Calculate the coordinates using the top <span class="math inline">\(k\)</span> eigenvalues and eigenvectors.</li>
</ol></li>
<li><p><strong>Applications:</strong></p>
<ul>
<li><strong>Perceptual Mapping:</strong> Visualizing consumer perceptions of products. For example, plotting brands on a 2D map based on consumer ratings to identify how they are perceived relative to each other.</li>
<li><strong>Genomics:</strong> Visualizing genetic similarities between species or individuals. For example, mapping genetic distance between different species to study evolutionary relationships.</li>
<li><strong>Psychometrics:</strong> Understanding relationships between psychological traits. For example, mapping responses to a psychological survey to identify clusters of similar traits.</li>
</ul></li>
</ul>
</section>
<section id="metric-mds" class="level3">
<h3 class="anchored" data-anchor-id="metric-mds">12.6.2. Metric MDS</h3>
<p>Metric MDS is a generalization of Classical MDS that aims to preserve the distances between points in a low-dimensional space, assuming the dissimilarities are metric (i.e., they satisfy the properties of a distance metric).</p>
<ul>
<li><p><strong>Objective:</strong> Find a configuration of points in a low-dimensional space that best preserves the metric properties of the original dissimilarities.</p></li>
<li><p><strong>Mathematical Formulation:</strong></p>
<ul>
<li><strong>Stress Function:</strong> The quality of the embedding is evaluated using a stress function, which measures the discrepancy between the original distances and the distances in the reduced space. <span class="math display">\[
\text{Stress} = \sqrt{\frac{\sum_{i&lt;j} (d_{ij} - \delta_{ij})^2}{\sum_{i&lt;j} \delta_{ij}^2}}
\]</span>
<ul>
<li>Here, <span class="math inline">\(d_{ij}\)</span> are the distances in the reduced space and <span class="math inline">\(\delta_{ij}\)</span> are the original dissimilarities.</li>
</ul></li>
<li><strong>Optimization:</strong> Minimize the stress function with respect to the coordinates of the points in the reduced space.</li>
</ul></li>
<li><p><strong>Steps:</strong></p>
<ol type="1">
<li><strong>Initialize Coordinates:</strong> Start with an initial configuration of points, often obtained from Classical MDS or a random configuration.</li>
<li><strong>Compute Distances:</strong> Calculate the distances <span class="math inline">\(d_{ij}\)</span> between points in the current configuration.</li>
<li><strong>Minimize Stress:</strong> Use iterative optimization techniques (e.g., gradient descent) to adjust the coordinates and minimize the stress function.</li>
<li><strong>Convergence:</strong> Stop when the change in stress is below a threshold or after a fixed number of iterations.</li>
</ol></li>
<li><p><strong>Applications:</strong></p>
<ul>
<li><strong>Geography:</strong> Visualizing spatial relationships between geographic locations. For example, plotting cities on a map based on travel times between them.</li>
<li><strong>Sociology:</strong> Analyzing social networks and relationships. For example, mapping individuals in a social network based on their interactions.</li>
<li><strong>Bioinformatics:</strong> Comparing molecular structures or genetic sequences. For example, mapping proteins based on structural similarity.</li>
</ul></li>
</ul>
</section>
<section id="non-metric-mds" class="level3">
<h3 class="anchored" data-anchor-id="non-metric-mds">12.6.3. Non-metric MDS</h3>
<p>Non-metric MDS is a variant of MDS that only preserves the rank order of the dissimilarities. It is useful when the dissimilarities are ordinal rather than interval or ratio scaled.</p>
<ul>
<li><p><strong>Objective:</strong> Find a configuration of points in a low-dimensional space such that the rank order of the distances matches the rank order of the original dissimilarities as closely as possible.</p></li>
<li><p><strong>Mathematical Formulation:</strong></p>
<ul>
<li><strong>Stress Function:</strong> The stress function is adapted to focus on the rank order of distances. <span class="math display">\[
\text{Stress} = \sqrt{\frac{\sum_{i&lt;j} (\hat{d}_{ij} - f(\delta_{ij}))^2}{\sum_{i&lt;j} \delta_{ij}^2}}
\]</span>
<ul>
<li>Here, <span class="math inline">\(\hat{d}_{ij}\)</span> are the fitted distances, <span class="math inline">\(\delta_{ij}\)</span> are the original dissimilarities, and <span class="math inline">\(f\)</span> is a monotonic transformation function.</li>
</ul></li>
<li><strong>Optimization:</strong> Minimize the stress function with respect to the coordinates and the transformation function <span class="math inline">\(f\)</span>.</li>
</ul></li>
<li><p><strong>Steps:</strong></p>
<ol type="1">
<li><strong>Initialize Coordinates:</strong> Start with an initial configuration of points.</li>
<li><strong>Compute Distances:</strong> Calculate the distances <span class="math inline">\(d_{ij}\)</span> between points in the current configuration.</li>
<li><strong>Monotonic Transformation:</strong> Apply a monotonic transformation to the original dissimilarities to best match the distances.</li>
<li><strong>Minimize Stress:</strong> Use iterative optimization techniques to adjust the coordinates and minimize the stress function.</li>
<li><strong>Convergence:</strong> Stop when the change in stress is below a threshold or after a fixed number of iterations.</li>
</ol></li>
<li><p><strong>Applications:</strong></p>
<ul>
<li><strong>Psychometrics:</strong> Understanding subjective judgments and preferences. For example, mapping individuals based on their preferences for different products.</li>
<li><strong>Marketing:</strong> Analyzing consumer preference data and brand positioning. For example, plotting brands on a perceptual map based on consumer preferences.</li>
<li><strong>Ecology:</strong> Studying ecological dissimilarities and species distributions. For example, mapping species based on ecological dissimilarity data.</li>
</ul></li>
</ul>
</section>
<section id="example-of-mds-application" class="level3">
<h3 class="anchored" data-anchor-id="example-of-mds-application">Example of MDS Application</h3>
<section id="perceptual-mapping-in-marketing" class="level4">
<h4 class="anchored" data-anchor-id="perceptual-mapping-in-marketing">Perceptual Mapping in Marketing</h4>
<ul>
<li><strong>Problem Statement:</strong> A company wants to understand how consumers perceive different brands of soft drinks.</li>
<li><strong>Approach:</strong>
<ol type="1">
<li><strong>Collect Data:</strong> Conduct a survey where consumers rate the similarity between different pairs of brands.</li>
<li><strong>Compute Dissimilarity Matrix:</strong> Calculate the dissimilarity matrix based on the survey responses.</li>
<li><strong>Apply MDS:</strong> Use Classical MDS to create a 2D map of the brands.</li>
<li><strong>Interpret Map:</strong> Analyze the positions of the brands on the map to understand consumer perceptions. Brands that are close together are perceived as similar, while brands that are far apart are perceived as different.</li>
</ol></li>
</ul>
</section>
<section id="genetic-similarity-in-genomics" class="level4">
<h4 class="anchored" data-anchor-id="genetic-similarity-in-genomics">Genetic Similarity in Genomics</h4>
<ul>
<li><strong>Problem Statement:</strong> Researchers want to visualize the genetic similarities between different species of plants.</li>
<li><strong>Approach:</strong>
<ol type="1">
<li><strong>Collect Data:</strong> Obtain genetic sequence data for the species.</li>
<li><strong>Compute Dissimilarity Matrix:</strong> Calculate a genetic distance matrix based on sequence similarity.</li>
<li><strong>Apply MDS:</strong> Use Metric MDS to create a 3D map of the species.</li>
<li><strong>Interpret Map:</strong> Examine the positions of the species on the map to understand genetic relationships. Species that are close together have high genetic similarity, while species that are far apart have low genetic similarity.</li>
</ol></li>
</ul>
<p>By understanding and applying Multidimensional Scaling (MDS) techniques, including Classical MDS, Metric MDS, and Non-metric MDS, you can effectively reduce the dimensionality of your data while preserving the essential relationships between data points. These techniques are powerful tools for visualizing and interpreting complex datasets.</p>
</section>
</section>
</section>
<section id="isomap" class="level2">
<h2 class="anchored" data-anchor-id="isomap">12.7. Isomap</h2>
<p>Isomap (Isometric Mapping) is a nonlinear dimensionality reduction technique that extends MDS by incorporating geodesic distances between points on a manifold. It is particularly useful for data that lies on a curved manifold in high-dimensional space.</p>
<section id="overview-and-objective-2" class="level3">
<h3 class="anchored" data-anchor-id="overview-and-objective-2">12.7.1. Overview and Objective</h3>
<ul>
<li><p><strong>Objective:</strong> Reduce the dimensionality of data while preserving the geodesic distances between all points. Isomap seeks to unfold the manifold and embed it in a lower-dimensional space.</p></li>
<li><p><strong>Mathematical Formulation:</strong></p>
<ol type="1">
<li><strong>Input:</strong> High-dimensional data matrix <span class="math inline">\(\mathbf{X}\)</span>.</li>
<li><strong>Neighborhood Graph Construction:</strong> Construct a neighborhood graph by connecting each point to its <span class="math inline">\(k\)</span> nearest neighbors based on Euclidean distance.</li>
<li><strong>Compute Geodesic Distances:</strong> Calculate the shortest paths (geodesic distances) between all pairs of points using the neighborhood graph. This can be done using algorithms like Floyd-Warshall or Dijkstra’s algorithm.</li>
<li><strong>Classical MDS:</strong> Apply Classical MDS to the matrix of geodesic distances to find the low-dimensional embedding.</li>
</ol></li>
</ul>
</section>
<section id="steps-of-isomap" class="level3">
<h3 class="anchored" data-anchor-id="steps-of-isomap">12.7.2. Steps of Isomap</h3>
<ol type="1">
<li><strong>Construct Neighborhood Graph:</strong>
<ul>
<li>Identify <span class="math inline">\(k\)</span> nearest neighbors for each data point.</li>
<li>Construct a graph where edges represent the Euclidean distances between neighbors.</li>
<li>Example: For a 3D dataset, determine the 5 nearest neighbors for each point to form the graph.</li>
</ul></li>
<li><strong>Compute Geodesic Distances:</strong>
<ul>
<li>Calculate the shortest paths between all pairs of points in the graph to approximate the geodesic distances on the manifold.</li>
<li>Example: Use Dijkstra’s algorithm to find the shortest paths in the neighborhood graph.</li>
</ul></li>
<li><strong>Perform Classical MDS:</strong>
<ul>
<li>Apply Classical MDS to the geodesic distance matrix to find the coordinates in the lower-dimensional space.</li>
<li>Example: Use eigen decomposition on the centered distance matrix to obtain the low-dimensional embedding.</li>
</ul></li>
</ol>
</section>
<section id="applications-of-isomap" class="level3">
<h3 class="anchored" data-anchor-id="applications-of-isomap">12.7.3. Applications of Isomap</h3>
<ul>
<li><strong>Manifold Learning:</strong> Understanding the intrinsic geometry of high-dimensional data.
<ul>
<li>Example: Unfolding a Swiss Roll dataset to reveal its 2D structure.</li>
</ul></li>
<li><strong>Image Processing:</strong> Reducing the dimensionality of images while preserving important structural information.
<ul>
<li>Example: Embedding high-resolution images of faces into a 2D space for visualization.</li>
</ul></li>
<li><strong>Sensor Networks:</strong> Mapping the physical layout of sensors based on distance measurements.
<ul>
<li>Example: Determining the layout of a network of sensors distributed in a geographic area.</li>
</ul></li>
</ul>
</section>
<section id="example-of-isomap-application" class="level3">
<h3 class="anchored" data-anchor-id="example-of-isomap-application">Example of Isomap Application</h3>
<section id="manifold-learning-for-handwritten-digits" class="level4">
<h4 class="anchored" data-anchor-id="manifold-learning-for-handwritten-digits">Manifold Learning for Handwritten Digits</h4>
<ul>
<li><strong>Problem Statement:</strong> Visualize the manifold structure of handwritten digit images.</li>
<li><strong>Approach:</strong>
<ol type="1">
<li><strong>Collect Data:</strong> Use a dataset of handwritten digit images (e.g., MNIST).</li>
<li><strong>Construct Neighborhood Graph:</strong> Identify <span class="math inline">\(k\)</span> nearest neighbors for each image based on pixel intensity distances.</li>
<li><strong>Compute Geodesic Distances:</strong> Calculate the geodesic distances between all pairs of images using the neighborhood graph.</li>
<li><strong>Apply Isomap:</strong> Perform Classical MDS on the geodesic distance matrix to obtain a 2D embedding.</li>
<li><strong>Visualize Results:</strong> Plot the 2D embedding to visualize the manifold structure of the digit images.</li>
</ol></li>
</ul>
</section>
</section>
</section>
<section id="locally-linear-embedding-lle" class="level2">
<h2 class="anchored" data-anchor-id="locally-linear-embedding-lle">12.8. Locally Linear Embedding (LLE)</h2>
<p>Locally Linear Embedding (LLE) is a nonlinear dimensionality reduction technique that aims to preserve the local structure of the data. It is particularly effective for data that lies on a nonlinear manifold.</p>
<section id="overview-and-objective-3" class="level3">
<h3 class="anchored" data-anchor-id="overview-and-objective-3">12.8.1. Overview and Objective</h3>
<ul>
<li><p><strong>Objective:</strong> Reduce the dimensionality of data by preserving the local neighborhoods of each point. LLE seeks to map high-dimensional data to a lower-dimensional space while maintaining the local relationships between points.</p></li>
<li><p><strong>Mathematical Formulation:</strong></p>
<ol type="1">
<li><strong>Input:</strong> High-dimensional data matrix <span class="math inline">\(\mathbf{X}\)</span>.</li>
<li><strong>Construct Neighborhood Graph:</strong> Identify <span class="math inline">\(k\)</span> nearest neighbors for each data point based on Euclidean distance.</li>
<li><strong>Compute Weights:</strong> For each point, compute the weights that best reconstruct the point from its neighbors using linear combinations.
<ul>
<li><strong>Weight Calculation:</strong> <span class="math display">\[
\min_{\mathbf{W}} \sum_{i=1}^N \left| \mathbf{x}_i - \sum_{j \in \mathcal{N}(i)} w_{ij} \mathbf{x}_j \right|^2
\]</span>
<ul>
<li>Here, <span class="math inline">\(\mathcal{N}(i)\)</span> denotes the set of <span class="math inline">\(k\)</span> nearest neighbors of point <span class="math inline">\(i\)</span>, and <span class="math inline">\(w_{ij}\)</span> are the weights.</li>
</ul></li>
</ul></li>
<li><strong>Embedding in Low-Dimensional Space:</strong> Find the low-dimensional embedding <span class="math inline">\(\mathbf{Y}\)</span> that best preserves these weights.
<ul>
<li><strong>Embedding Calculation:</strong> <span class="math display">\[
\min_{\mathbf{Y}} \sum_{i=1}^N \left| \mathbf{y}_i - \sum_{j \in \mathcal{N}(i)} w_{ij} \mathbf{y}_j \right|^2
\]</span></li>
</ul></li>
</ol></li>
</ul>
</section>
<section id="steps-of-lle" class="level3">
<h3 class="anchored" data-anchor-id="steps-of-lle">12.8.2. Steps of LLE</h3>
<ol type="1">
<li><strong>Construct Neighborhood Graph:</strong>
<ul>
<li>Identify <span class="math inline">\(k\)</span> nearest neighbors for each data point based on Euclidean distance.</li>
<li>Example: For a dataset of 3D points, find the 10 nearest neighbors for each point.</li>
</ul></li>
<li><strong>Compute Reconstruction Weights:</strong>
<ul>
<li>For each data point, solve the optimization problem to find weights that minimize the reconstruction error.</li>
<li>Ensure the weights sum to one: <span class="math display">\[
\sum_{j \in \mathcal{N}(i)} w_{ij} = 1
\]</span></li>
<li>Example: For a data point, calculate the weights that best represent it as a linear combination of its 10 nearest neighbors.</li>
</ul></li>
<li><strong>Compute Low-Dimensional Embedding:</strong>
<ul>
<li>Solve the eigenvalue problem to find the low-dimensional coordinates <span class="math inline">\(\mathbf{Y}\)</span> that preserve the reconstruction weights.</li>
<li>Example: Find the 2D coordinates of the data points that best maintain the local relationships defined by the weights.</li>
</ul></li>
</ol>
</section>
<section id="variants-of-lle" class="level3">
<h3 class="anchored" data-anchor-id="variants-of-lle">12.8.3. Variants of LLE</h3>
<section id="modified-lle" class="level4">
<h4 class="anchored" data-anchor-id="modified-lle">Modified LLE</h4>
<p>Modified LLE aims to improve the robustness of the original LLE algorithm by using a different weight calculation method that includes a regularization term to handle cases where the local neighborhood is insufficient to reconstruct the data point accurately.</p>
<ul>
<li><strong>Objective Function:</strong> <span class="math display">\[
\min_{\mathbf{W}} \sum_{i=1}^N \left| \mathbf{x}_i - \sum_{j \in \mathcal{N}(i)} w_{ij} \mathbf{x}_j \right|^2 + \lambda \sum_{i,j} w_{ij}^2
\]</span>
<ul>
<li>Here, <span class="math inline">\(\lambda\)</span> is the regularization parameter that controls the trade-off between reconstruction error and weight regularization.</li>
</ul></li>
<li><strong>Applications:</strong>
<ul>
<li>More robust dimensionality reduction in noisy datasets.</li>
<li>Improved stability and accuracy in the presence of outliers.</li>
</ul></li>
</ul>
</section>
<section id="hessian-lle" class="level4">
<h4 class="anchored" data-anchor-id="hessian-lle">Hessian LLE</h4>
<p>Hessian LLE focuses on capturing the curvature of the manifold by incorporating second-order information into the weight calculation process. It uses the Hessian matrix to better understand the local geometry of the data.</p>
<ul>
<li><strong>Objective Function:</strong> <span class="math display">\[
\min_{\mathbf{W}} \sum_{i=1}^N \left| \mathbf{x}_i - \sum_{j \in \mathcal{N}(i)} w_{ij} \mathbf{x}_j \right|^2
\]</span>
<ul>
<li>Additionally, the Hessian matrix is used to refine the weight calculations and better capture the manifold’s curvature.</li>
</ul></li>
<li><strong>Applications:</strong>
<ul>
<li>Enhanced dimensionality reduction for data with complex geometric structures.</li>
<li>Improved representation of manifolds with varying curvature.</li>
</ul></li>
</ul>
</section>
</section>
<section id="applications-of-lle" class="level3">
<h3 class="anchored" data-anchor-id="applications-of-lle">12.8.4. Applications of LLE</h3>
<ul>
<li><strong>Manifold Learning:</strong> Understanding the intrinsic geometry of high-dimensional data.
<ul>
<li>Example: Unfolding a complex 3D surface into a 2D plane for visualization.</li>
</ul></li>
<li><strong>Image Processing:</strong> Reducing the dimensionality of images while preserving local structural information.
<ul>
<li>Example: Embedding high-dimensional facial images into a lower-dimensional space for recognition tasks.</li>
</ul></li>
<li><strong>Bioinformatics:</strong> Visualizing gene expression data to identify patterns and clusters.
<ul>
<li>Example: Mapping gene expression profiles into a 2D space to observe clustering of similar gene expression patterns.</li>
</ul></li>
</ul>
</section>
<section id="example-of-lle-application" class="level3">
<h3 class="anchored" data-anchor-id="example-of-lle-application">Example of LLE Application</h3>
<section id="visualizing-swiss-roll-dataset" class="level4">
<h4 class="anchored" data-anchor-id="visualizing-swiss-roll-dataset">Visualizing Swiss Roll Dataset</h4>
<ul>
<li><strong>Problem Statement:</strong> Visualize the manifold structure of the Swiss Roll dataset.</li>
<li><strong>Approach:</strong>
<ol type="1">
<li><strong>Generate Data:</strong> Create a Swiss Roll dataset, which is a 3D dataset that lies on a 2D manifold.</li>
<li><strong>Construct Neighborhood Graph:</strong> Identify <span class="math inline">\(k\)</span> nearest neighbors for each data point based on Euclidean distance.</li>
<li><strong>Compute Reconstruction Weights:</strong> Calculate the weights that best reconstruct each point from its neighbors.</li>
<li><strong>Apply LLE:</strong> Perform LLE to obtain a 2D embedding.</li>
<li><strong>Visualize Results:</strong> Plot the 2D embedding to visualize the manifold structure of the Swiss Roll dataset.</li>
</ol></li>
</ul>
</section>
</section>
<section id="example-of-isomap-and-lle-comparison" class="level3">
<h3 class="anchored" data-anchor-id="example-of-isomap-and-lle-comparison">Example of Isomap and LLE Comparison</h3>
<section id="unfolding-a-swiss-roll" class="level4">
<h4 class="anchored" data-anchor-id="unfolding-a-swiss-roll">Unfolding a Swiss Roll</h4>
<ul>
<li><strong>Isomap Approach:</strong>
<ol type="1">
<li><strong>Generate Swiss Roll Data:</strong> Create a dataset of points arranged in a Swiss roll in 3D space.</li>
<li><strong>Construct Neighborhood Graph:</strong> Connect each point to its 10 nearest neighbors.</li>
<li><strong>Compute Geodesic Distances:</strong> Use shortest path algorithms to estimate the geodesic distances between points.</li>
<li><strong>Apply Classical MDS:</strong> Use MDS on the geodesic distance matrix to unfold the Swiss roll into a 2D plane.</li>
<li><strong>Visualize Results:</strong> Plot the 2D coordinates to show the unrolled Swiss roll.</li>
</ol></li>
<li><strong>LLE Approach:</strong>
<ol type="1">
<li><strong>Generate Swiss Roll Data:</strong> Create the same dataset of points in a Swiss roll.</li>
<li><strong>Construct Neighborhood Graph:</strong> Identify 10 nearest neighbors for each point.</li>
<li><strong>Compute Reconstruction Weights:</strong> Calculate weights to best reconstruct each point from its neighbors.</li>
<li><strong>Apply LLE:</strong> Perform LLE to obtain a 2D embedding.</li>
<li><strong>Visualize Results:</strong> Plot the 2D embedding to show the local relationships preserved by LLE.</li>
</ol></li>
</ul>
<p>By understanding and applying Isomap and Locally Linear Embedding (LLE), you can effectively reduce the dimensionality of your data while preserving the essential relationships between data points. These techniques are powerful tools for visualizing and interpreting complex datasets, especially when the data lies on a nonlinear manifold.</p>
</section>
</section>
</section>
<section id="autoencoders-for-dimensionality-reduction" class="level2">
<h2 class="anchored" data-anchor-id="autoencoders-for-dimensionality-reduction">12.9. Autoencoders for Dimensionality Reduction</h2>
<p>Autoencoders are a type of artificial neural network used for learning efficient codings of input data in an unsupervised manner. They are primarily used for dimensionality reduction, feature learning, and data denoising.</p>
<section id="undercomplete-autoencoders" class="level3">
<h3 class="anchored" data-anchor-id="undercomplete-autoencoders">12.9.1. Undercomplete Autoencoders</h3>
<p>Undercomplete autoencoders aim to learn a compressed representation of the input data. The bottleneck layer has fewer neurons than the input and output layers, forcing the autoencoder to learn the most important features of the data.</p>
<ul>
<li><p><strong>Objective:</strong> Reduce dimensionality by learning a lower-dimensional representation that captures the most significant features of the input data.</p></li>
<li><p><strong>Architecture:</strong></p>
<ul>
<li><strong>Encoder:</strong> Maps the input data <span class="math inline">\(\mathbf{X}\)</span> to a latent space representation <span class="math inline">\(\mathbf{Z}\)</span>. <span class="math display">\[
\mathbf{Z} = f(\mathbf{W}_e \mathbf{X} + \mathbf{b}_e)
\]</span>
<ul>
<li>Here, <span class="math inline">\(\mathbf{W}_e\)</span> and <span class="math inline">\(\mathbf{b}_e\)</span> are the weights and biases of the encoder.</li>
</ul></li>
<li><strong>Decoder:</strong> Reconstructs the input data from the latent representation. <span class="math display">\[
\mathbf{\hat{X}} = g(\mathbf{W}_d \mathbf{Z} + \mathbf{b}_d)
\]</span>
<ul>
<li>Here, <span class="math inline">\(\mathbf{W}_d\)</span> and <span class="math inline">\(\mathbf{b}_d\)</span> are the weights and biases of the decoder.</li>
</ul></li>
</ul></li>
<li><p><strong>Loss Function:</strong> Measures the difference between the input data and its reconstruction. <span class="math display">\[
\mathcal{L}(\mathbf{X}, \mathbf{\hat{X}}) = \|\mathbf{X} - \mathbf{\hat{X}}\|^2
\]</span></p>
<ul>
<li>This is typically the mean squared error (MSE) between the input and reconstructed output.</li>
</ul></li>
<li><p><strong>Applications:</strong></p>
<ul>
<li><strong>Dimensionality Reduction:</strong> Reducing the number of features while retaining essential information.</li>
<li><strong>Feature Learning:</strong> Learning useful representations of data for other machine learning tasks.</li>
<li><strong>Anomaly Detection:</strong> Identifying anomalies by measuring reconstruction error.</li>
</ul></li>
</ul>
</section>
<section id="denoising-autoencoders" class="level3">
<h3 class="anchored" data-anchor-id="denoising-autoencoders">12.9.2. Denoising Autoencoders</h3>
<p>Denoising autoencoders (DAEs) are trained to reconstruct the original input from a corrupted version of it. This process helps the autoencoder learn more robust features.</p>
<ul>
<li><p><strong>Objective:</strong> Improve the robustness of learned features by training the autoencoder to remove noise from the input data.</p></li>
<li><p><strong>Architecture:</strong></p>
<ul>
<li>Similar to undercomplete autoencoders but trained on corrupted input data.</li>
</ul></li>
<li><p><strong>Training Process:</strong></p>
<ol type="1">
<li><strong>Corrupt Input:</strong> Add noise to the input data <span class="math inline">\(\mathbf{X}\)</span> to create a corrupted version <span class="math inline">\(\mathbf{\tilde{X}}\)</span>. <span class="math display">\[
\mathbf{\tilde{X}} = \mathbf{X} + \mathbf{N}
\]</span>
<ul>
<li>Here, <span class="math inline">\(\mathbf{N}\)</span> represents the noise added to the input data.</li>
</ul></li>
<li><strong>Reconstruction:</strong> Train the autoencoder to reconstruct the original input <span class="math inline">\(\mathbf{X}\)</span> from the corrupted input <span class="math inline">\(\mathbf{\tilde{X}}\)</span>. <span class="math display">\[
\mathbf{\hat{X}} = g(f(\mathbf{\tilde{X}}))
\]</span></li>
</ol></li>
<li><p><strong>Loss Function:</strong> Measures the difference between the original input and the reconstructed output. <span class="math display">\[
\mathcal{L}(\mathbf{X}, \mathbf{\hat{X}}) = \|\mathbf{X} - \mathbf{\hat{X}}\|^2
\]</span></p></li>
<li><p><strong>Applications:</strong></p>
<ul>
<li><strong>Data Denoising:</strong> Removing noise from images, signals, and other data types.</li>
<li><strong>Robust Feature Learning:</strong> Learning features that are invariant to noise and other perturbations.</li>
</ul></li>
</ul>
</section>
<section id="variational-autoencoders" class="level3">
<h3 class="anchored" data-anchor-id="variational-autoencoders">12.9.3. Variational Autoencoders</h3>
<p>Variational Autoencoders (VAEs) are a type of generative model that combines autoencoders with probabilistic inference. They learn to generate new data samples similar to the training data by learning a latent space representation.</p>
<ul>
<li><p><strong>Objective:</strong> Learn a probabilistic model of the data by combining autoencoding with variational inference.</p></li>
<li><p><strong>Architecture:</strong></p>
<ul>
<li><strong>Encoder:</strong> Maps the input data <span class="math inline">\(\mathbf{X}\)</span> to a distribution over the latent space. <span class="math display">\[
q(\mathbf{z}|\mathbf{x}) = \mathcal{N}(\mathbf{z}; \mu(\mathbf{x}), \sigma^2(\mathbf{x}))
\]</span>
<ul>
<li>Here, <span class="math inline">\(\mu(\mathbf{x})\)</span> and <span class="math inline">\(\sigma^2(\mathbf{x})\)</span> are the mean and variance of the Gaussian distribution in the latent space.</li>
</ul></li>
<li><strong>Decoder:</strong> Reconstructs the input data from samples drawn from the latent space distribution. <span class="math display">\[
\mathbf{\hat{X}} = g(\mathbf{z})
\]</span>
<ul>
<li>Here, <span class="math inline">\(\mathbf{z}\)</span> is a sample from the latent distribution.</li>
</ul></li>
</ul></li>
<li><p><strong>Loss Function:</strong> Combines reconstruction loss with a regularization term (KL divergence) to ensure the latent space distribution is close to a prior distribution (typically a standard normal distribution). <span class="math display">\[
\mathcal{L}(\mathbf{X}, \mathbf{\hat{X}}, \mathbf{z}) = \mathbb{E}_{q(\mathbf{z}|\mathbf{x})}[\|\mathbf{X} - \mathbf{\hat{X}}\|^2] + \text{KL}(q(\mathbf{z}|\mathbf{x}) \| p(\mathbf{z}))
\]</span></p>
<ul>
<li>Here, <span class="math inline">\(\text{KL}(q(\mathbf{z}|\mathbf{x}) \| p(\mathbf{z}))\)</span> is the Kullback-Leibler divergence between the approximate posterior and the prior distribution.</li>
</ul></li>
<li><p><strong>Applications:</strong></p>
<ul>
<li><strong>Generative Modeling:</strong> Generating new data samples similar to the training data.</li>
<li><strong>Data Imputation:</strong> Filling in missing data by sampling from the learned distribution.</li>
<li><strong>Anomaly Detection:</strong> Identifying anomalies based on the probability of reconstruction.</li>
</ul></li>
</ul>
</section>
<section id="examples-of-autoencoder-applications" class="level3">
<h3 class="anchored" data-anchor-id="examples-of-autoencoder-applications">Examples of Autoencoder Applications</h3>
<section id="undercomplete-autoencoder-for-dimensionality-reduction" class="level4">
<h4 class="anchored" data-anchor-id="undercomplete-autoencoder-for-dimensionality-reduction">Undercomplete Autoencoder for Dimensionality Reduction</h4>
<ul>
<li><strong>Problem Statement:</strong> Reduce the dimensionality of a dataset of handwritten digit images (e.g., MNIST) for visualization and classification tasks.</li>
<li><strong>Approach:</strong>
<ol type="1">
<li><strong>Collect Data:</strong> Use the MNIST dataset of handwritten digit images.</li>
<li><strong>Design Autoencoder:</strong> Create an undercomplete autoencoder with a bottleneck layer smaller than the input layer.</li>
<li><strong>Train Autoencoder:</strong> Train the autoencoder to minimize the reconstruction error on the MNIST dataset.</li>
<li><strong>Extract Features:</strong> Use the encoder part of the trained autoencoder to transform the input images into lower-dimensional representations.</li>
<li><strong>Visualize Results:</strong> Plot the low-dimensional representations to visualize the structure of the data.</li>
</ol></li>
</ul>
</section>
<section id="denoising-autoencoder-for-image-denoising" class="level4">
<h4 class="anchored" data-anchor-id="denoising-autoencoder-for-image-denoising">Denoising Autoencoder for Image Denoising</h4>
<ul>
<li><strong>Problem Statement:</strong> Remove noise from images of handwritten digits.</li>
<li><strong>Approach:</strong>
<ol type="1">
<li><strong>Collect Data:</strong> Use the MNIST dataset and add random noise to the images.</li>
<li><strong>Design Autoencoder:</strong> Create a denoising autoencoder with an architecture similar to an undercomplete autoencoder.</li>
<li><strong>Corrupt Input:</strong> Add noise to the input images to create corrupted versions.</li>
<li><strong>Train Autoencoder:</strong> Train the autoencoder to reconstruct the original images from the corrupted versions.</li>
<li><strong>Evaluate Results:</strong> Test the trained autoencoder on noisy images and compare the denoised outputs to the original images.</li>
</ol></li>
</ul>
</section>
<section id="variational-autoencoder-for-generative-modeling" class="level4">
<h4 class="anchored" data-anchor-id="variational-autoencoder-for-generative-modeling">Variational Autoencoder for Generative Modeling</h4>
<ul>
<li><strong>Problem Statement:</strong> Generate new handwritten digit images similar to those in the MNIST dataset.</li>
<li><strong>Approach:</strong>
<ol type="1">
<li><strong>Collect Data:</strong> Use the MNIST dataset of handwritten digit images.</li>
<li><strong>Design VAE:</strong> Create a variational autoencoder with an encoder that outputs mean and variance parameters for a Gaussian distribution in the latent space.</li>
<li><strong>Train VAE:</strong> Train the VAE to minimize the combined reconstruction and KL divergence loss on the MNIST dataset.</li>
<li><strong>Generate Samples:</strong> Sample from the learned latent space distribution and use the decoder to generate new digit images.</li>
<li><strong>Visualize Results:</strong> Plot the generated digit images to evaluate the quality and diversity of the samples.</li>
</ol></li>
</ul>
<p>By understanding and applying autoencoders, including undercomplete autoencoders, denoising autoencoders, and variational autoencoders, you can effectively reduce the dimensionality of your data, learn robust features, and generate new data samples. These techniques are powerful tools for advanced data analysis and machine learning applications.</p>
</section>
</section>
</section>
<section id="random-projection" class="level2">
<h2 class="anchored" data-anchor-id="random-projection">12.10. Random Projection</h2>
<p>Random projection is a simple and computationally efficient technique for dimensionality reduction. It is based on the Johnson-Lindenstrauss lemma, which states that points in a high-dimensional space can be projected into a lower-dimensional space such that the distances between the points are nearly preserved.</p>
<section id="overview-and-objective-4" class="level3">
<h3 class="anchored" data-anchor-id="overview-and-objective-4">12.10.1. Overview and Objective</h3>
<ul>
<li><strong>Objective:</strong> Reduce the dimensionality of high-dimensional data while approximately preserving the pairwise distances between data points.</li>
<li><strong>Key Idea:</strong> Use a random matrix to project the high-dimensional data into a lower-dimensional space.</li>
</ul>
</section>
<section id="mathematical-formulation" class="level3">
<h3 class="anchored" data-anchor-id="mathematical-formulation">12.10.2. Mathematical Formulation</h3>
<ul>
<li><strong>Input:</strong> High-dimensional data matrix <span class="math inline">\(\mathbf{X} \in \mathbb{R}^{n \times d}\)</span>, where <span class="math inline">\(n\)</span> is the number of samples and <span class="math inline">\(d\)</span> is the number of features.</li>
<li><strong>Random Projection Matrix:</strong> A random matrix <span class="math inline">\(\mathbf{R} \in \mathbb{R}^{d \times k}\)</span>, where <span class="math inline">\(k \ll d\)</span>, is used to project the data into a <span class="math inline">\(k\)</span>-dimensional space.</li>
<li><strong>Projection:</strong> The projected data matrix <span class="math inline">\(\mathbf{X}' \in \mathbb{R}^{n \times k}\)</span> is obtained by multiplying the original data matrix <span class="math inline">\(\mathbf{X}\)</span> with the random matrix <span class="math inline">\(\mathbf{R}\)</span>: <span class="math display">\[
\mathbf{X}' = \mathbf{X} \mathbf{R}
\]</span></li>
</ul>
</section>
<section id="properties-of-random-projection" class="level3">
<h3 class="anchored" data-anchor-id="properties-of-random-projection">12.10.3. Properties of Random Projection</h3>
<ul>
<li><strong>Approximate Distance Preservation:</strong> The distances between points in the original high-dimensional space are approximately preserved in the lower-dimensional space.</li>
<li><strong>Simplicity:</strong> The technique is computationally efficient and easy to implement.</li>
<li><strong>Random Matrices:</strong> Common choices for the random matrix <span class="math inline">\(\mathbf{R}\)</span> include Gaussian random matrices and sparse random matrices.</li>
</ul>
</section>
<section id="steps-of-random-projection" class="level3">
<h3 class="anchored" data-anchor-id="steps-of-random-projection">12.10.4. Steps of Random Projection</h3>
<ol type="1">
<li><p><strong>Generate Random Matrix:</strong> Generate a random matrix <span class="math inline">\(\mathbf{R}\)</span> with dimensions <span class="math inline">\(d \times k\)</span>.</p>
<ul>
<li>For Gaussian random projection, each element of <span class="math inline">\(\mathbf{R}\)</span> is drawn from a Gaussian distribution with mean 0 and variance <span class="math inline">\(\frac{1}{k}\)</span>.</li>
<li>For sparse random projection, <span class="math inline">\(\mathbf{R}\)</span> can be a sparse matrix with elements drawn from a specific distribution (e.g., elements being 0 with high probability and <span class="math inline">\(\pm 1\)</span> with lower probability).</li>
</ul></li>
<li><p><strong>Project Data:</strong> Multiply the original data matrix <span class="math inline">\(\mathbf{X}\)</span> by the random matrix <span class="math inline">\(\mathbf{R}\)</span> to obtain the projected data matrix <span class="math inline">\(\mathbf{X}'\)</span>.</p></li>
<li><p><strong>Verify Distance Preservation:</strong> Check that the pairwise distances between points in the projected space are approximately preserved.</p></li>
</ol>
</section>
<section id="applications-of-random-projection" class="level3">
<h3 class="anchored" data-anchor-id="applications-of-random-projection">12.10.5. Applications of Random Projection</h3>
<ul>
<li><strong>Text Mining:</strong> Dimensionality reduction for high-dimensional text data (e.g., word embeddings).
<ul>
<li>Example: Projecting TF-IDF vectors into a lower-dimensional space for clustering or classification tasks.</li>
</ul></li>
<li><strong>Image Processing:</strong> Reducing the dimensionality of high-resolution images.
<ul>
<li>Example: Compressing image data for faster processing and storage.</li>
</ul></li>
<li><strong>Genomics:</strong> Analyzing high-dimensional genetic data.
<ul>
<li>Example: Projecting gene expression profiles into a lower-dimensional space for visualization and analysis.</li>
</ul></li>
</ul>
</section>
<section id="example-of-random-projection-application" class="level3">
<h3 class="anchored" data-anchor-id="example-of-random-projection-application">Example of Random Projection Application</h3>
<section id="dimensionality-reduction-for-text-data" class="level4">
<h4 class="anchored" data-anchor-id="dimensionality-reduction-for-text-data">Dimensionality Reduction for Text Data</h4>
<ul>
<li><strong>Problem Statement:</strong> Reduce the dimensionality of TF-IDF vectors for a large collection of documents.</li>
<li><strong>Approach:</strong>
<ol type="1">
<li><strong>Collect Data:</strong> Use a corpus of documents and compute TF-IDF vectors for each document.</li>
<li><strong>Generate Random Matrix:</strong> Create a random projection matrix with appropriate dimensions.</li>
<li><strong>Project Data:</strong> Multiply the TF-IDF matrix by the random projection matrix to obtain lower-dimensional representations.</li>
<li><strong>Evaluate Results:</strong> Compare the distances between documents in the original and projected spaces to ensure approximate distance preservation.</li>
</ol></li>
</ul>
</section>
</section>
</section>
<section id="feature-agglomeration" class="level2">
<h2 class="anchored" data-anchor-id="feature-agglomeration">12.11. Feature Agglomeration</h2>
<p>Feature agglomeration is a hierarchical clustering technique applied to feature space. It groups similar features together to form clusters, which are then used to create new features. This technique is particularly useful for reducing the dimensionality of data with many correlated features.</p>
<section id="overview-and-objective-5" class="level3">
<h3 class="anchored" data-anchor-id="overview-and-objective-5">12.11.1. Overview and Objective</h3>
<ul>
<li><strong>Objective:</strong> Reduce the dimensionality of data by merging similar features into clusters and creating new aggregated features.</li>
<li><strong>Key Idea:</strong> Apply hierarchical clustering to group similar features and use the clusters to create new features.</li>
</ul>
</section>
<section id="mathematical-formulation-1" class="level3">
<h3 class="anchored" data-anchor-id="mathematical-formulation-1">12.11.2. Mathematical Formulation</h3>
<ul>
<li><strong>Input:</strong> High-dimensional data matrix <span class="math inline">\(\mathbf{X} \in \mathbb{R}^{n \times d}\)</span>, where <span class="math inline">\(n\)</span> is the number of samples and <span class="math inline">\(d\)</span> is the number of features.</li>
<li><strong>Distance Metric:</strong> A metric to measure the similarity between features. Common choices include Euclidean distance and correlation distance.</li>
<li><strong>Linkage Criteria:</strong> A criterion to determine how clusters are formed during the hierarchical clustering process. Common choices include single linkage, complete linkage, average linkage, and Ward’s method.</li>
</ul>
</section>
<section id="steps-of-feature-agglomeration" class="level3">
<h3 class="anchored" data-anchor-id="steps-of-feature-agglomeration">12.11.3. Steps of Feature Agglomeration</h3>
<ol type="1">
<li><p><strong>Compute Distance Matrix:</strong> Calculate the pairwise distances between features using the chosen distance metric.</p></li>
<li><p><strong>Hierarchical Clustering:</strong> Apply hierarchical clustering to the distance matrix using the chosen linkage criterion to form a dendrogram of features.</p></li>
<li><p><strong>Determine Number of Clusters:</strong> Decide the number of clusters to form by cutting the dendrogram at an appropriate level.</p></li>
<li><p><strong>Aggregate Features:</strong> For each cluster, aggregate the features to form a new feature. Common aggregation methods include taking the mean or median of the features in each cluster.</p></li>
<li><p><strong>Create New Feature Matrix:</strong> Construct a new feature matrix with the aggregated features.</p></li>
</ol>
</section>
<section id="applications-of-feature-agglomeration" class="level3">
<h3 class="anchored" data-anchor-id="applications-of-feature-agglomeration">12.11.4. Applications of Feature Agglomeration</h3>
<ul>
<li><strong>Genomics:</strong> Reducing the dimensionality of genetic data by clustering similar genes.
<ul>
<li>Example: Grouping genes with similar expression patterns to create aggregate gene expression profiles.</li>
</ul></li>
<li><strong>Finance:</strong> Simplifying financial datasets by clustering correlated financial indicators.
<ul>
<li>Example: Aggregating similar financial metrics (e.g., different stock indices) into composite indicators.</li>
</ul></li>
<li><strong>Image Processing:</strong> Reducing the number of features in high-dimensional image data.
<ul>
<li>Example: Clustering similar pixel intensity features to create aggregated image features.</li>
</ul></li>
</ul>
</section>
<section id="example-of-feature-agglomeration-application" class="level3">
<h3 class="anchored" data-anchor-id="example-of-feature-agglomeration-application">Example of Feature Agglomeration Application</h3>
<section id="dimensionality-reduction-for-genetic-data" class="level4">
<h4 class="anchored" data-anchor-id="dimensionality-reduction-for-genetic-data">Dimensionality Reduction for Genetic Data</h4>
<ul>
<li><strong>Problem Statement:</strong> Reduce the dimensionality of gene expression profiles while preserving important patterns.</li>
<li><strong>Approach:</strong>
<ol type="1">
<li><strong>Collect Data:</strong> Use a dataset of gene expression profiles from various samples.</li>
<li><strong>Compute Distance Matrix:</strong> Calculate the pairwise distances between gene expression profiles using correlation distance.</li>
<li><strong>Hierarchical Clustering:</strong> Apply hierarchical clustering to group similar genes based on their expression patterns.</li>
<li><strong>Determine Number of Clusters:</strong> Choose the number of clusters by examining the dendrogram.</li>
<li><strong>Aggregate Features:</strong> For each cluster of genes, compute the mean expression profile to create a new aggregated feature.</li>
<li><strong>Create New Feature Matrix:</strong> Construct a new feature matrix with the aggregated gene expression profiles.</li>
</ol></li>
</ul>
<p>By understanding and applying Random Projection and Feature Agglomeration, you can effectively reduce the dimensionality of your data while preserving the essential relationships between features and data points. These techniques are powerful tools for simplifying complex datasets and improving the efficiency of machine learning algorithms.</p>
</section>
</section>
</section>
</section>



</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>